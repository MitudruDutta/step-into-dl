{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-markdown",
      "metadata": {},
      "source": [
        "# ü§ñ BERT: Bidirectional Encoder Representations from Transformers\n",
        "\n",
        "This notebook explores **BERT**, one of the most influential NLP models. BERT uses the **encoder-only** architecture and is trained with **Masked Language Modeling (MLM)**.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "| Concept | Description |\n",
        "|:--------|:------------|\n",
        "| **Architecture** | Encoder-only Transformer (12 layers in base) |\n",
        "| **Attention** | Bidirectional - sees entire context |\n",
        "| **Training** | Masked Language Modeling (MLM) + Next Sentence Prediction |\n",
        "| **Output** | Contextual embeddings for each token |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports-markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "We'll use the ü§ó Hugging Face `transformers` library, which provides pre-trained BERT models and tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenizer-intro",
      "metadata": {},
      "source": [
        "## 2. Loading the BERT Tokenizer\n",
        "\n",
        "BERT uses **WordPiece tokenization**, which breaks words into subword units. This allows BERT to:\n",
        "- Handle out-of-vocabulary words\n",
        "- Keep vocabulary size manageable (~30,000 tokens)\n",
        "- Capture morphological patterns\n",
        "\n",
        "The `bert-base-uncased` model:\n",
        "- Uses lowercase text (uncased)\n",
        "- Has 12 transformer layers\n",
        "- 768-dimensional embeddings\n",
        "- 110M parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-tokenizer",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tokenize-single",
      "metadata": {},
      "source": [
        "## 3. Tokenizing a Single Sentence\n",
        "\n",
        "Let's tokenize a simple sentence and examine the output:\n",
        "\n",
        "```\n",
        "Tokenizer Output:\n",
        "‚îú‚îÄ‚îÄ input_ids:      Token IDs (integers representing each token)\n",
        "‚îú‚îÄ‚îÄ token_type_ids: Segment IDs (0 for first sentence, 1 for second)\n",
        "‚îî‚îÄ‚îÄ attention_mask: 1 for real tokens, 0 for padding\n",
        "```\n",
        "\n",
        "**Special Tokens:**\n",
        "- `[CLS]` (ID: 101): Added at the start, used for classification\n",
        "- `[SEP]` (ID: 102): Added at the end, separates sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tokenize-single-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = tokenizer('I am a nerd', return_tensors='pt')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decode-explain",
      "metadata": {},
      "source": [
        "### Understanding the Token IDs\n",
        "\n",
        "Let's decode the token IDs to see what tokens they represent:\n",
        "\n",
        "```\n",
        "Token ID  ‚Üí  Token\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "101       ‚Üí  [CLS]\n",
        "1045      ‚Üí  i\n",
        "2572      ‚Üí  am\n",
        "1037      ‚Üí  a\n",
        "11265     ‚Üí  ner\n",
        "4103      ‚Üí  ##d\n",
        "102       ‚Üí  [SEP]\n",
        "```\n",
        "\n",
        "Notice that \"nerd\" is split into \"ner\" + \"##d\" (the `##` prefix indicates a subword continuation)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "batch-tokenize",
      "metadata": {},
      "source": [
        "## 4. Batch Tokenization with Padding\n",
        "\n",
        "When processing multiple sentences of different lengths, we need **padding** to create uniform tensor shapes.\n",
        "\n",
        "```\n",
        "Before Padding:           After Padding:\n",
        "\"I am a nerd\"    (4 words)     \"I am a nerd\"     [PAD] [PAD]\n",
        "\"reading books\" (5 words)      \"reading books all day long\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "batch-tokenize-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = tokenizer(['I am a nerd','reading books all day long'], padding=True, return_tensors='pt')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-intro",
      "metadata": {},
      "source": [
        "## 5. Loading the BERT Model\n",
        "\n",
        "Now we'll load the pre-trained BERT model and pass our tokenized inputs through it.\n",
        "\n",
        "```\n",
        "BERT Model Architecture:\n",
        "\n",
        "    Input IDs ‚Üí Embedding Layer ‚Üí 12√ó Transformer Encoder Layers ‚Üí Output\n",
        "                      ‚Üì\n",
        "            + Positional Encoding\n",
        "            + Token Type Embedding\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-model",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "output = model(**tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "output-explain",
      "metadata": {},
      "source": [
        "## 6. Understanding BERT's Output\n",
        "\n",
        "BERT returns two main outputs:\n",
        "\n",
        "### `last_hidden_state`\n",
        "Contextual embeddings for **every token** in the input.\n",
        "- Shape: `(batch_size, sequence_length, hidden_size)`\n",
        "- Each token has a 768-dimensional vector that depends on its context\n",
        "\n",
        "### `pooler_output`\n",
        "A single vector representing the **entire sentence**.\n",
        "- Shape: `(batch_size, hidden_size)`\n",
        "- Derived from the `[CLS]` token's embedding\n",
        "- Often used for classification tasks\n",
        "\n",
        "```\n",
        "Input:  [CLS] I am a ner ##d [SEP]\n",
        "          ‚Üì   ‚Üì  ‚Üì ‚Üì  ‚Üì   ‚Üì   ‚Üì\n",
        "Output:  h‚ÇÄ  h‚ÇÅ h‚ÇÇ h‚ÇÉ h‚ÇÑ  h‚ÇÖ  h‚ÇÜ   ‚Üê last_hidden_state\n",
        "          ‚Üì\n",
        "        pooler_output (from [CLS])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "view-hidden",
      "metadata": {},
      "outputs": [],
      "source": [
        "output['last_hidden_state']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shape-explain",
      "metadata": {},
      "source": [
        "### Checking the Output Shapes\n",
        "\n",
        "For our batch of 2 sentences with 7 tokens each:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hidden-shape",
      "metadata": {},
      "outputs": [],
      "source": [
        "output['last_hidden_state'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shape-breakdown",
      "metadata": {},
      "source": [
        "**Shape breakdown:** `[2, 7, 768]`\n",
        "- `2` = batch size (2 sentences)\n",
        "- `7` = sequence length (7 tokens per sentence)\n",
        "- `768` = hidden dimension (BERT-base embedding size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pooler-shape",
      "metadata": {},
      "outputs": [],
      "source": [
        "output['pooler_output'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pooler-explain",
      "metadata": {},
      "source": [
        "**Shape breakdown:** `[2, 768]`\n",
        "- `2` = batch size (one pooled output per sentence)\n",
        "- `768` = hidden dimension\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Key Takeaways\n",
        "\n",
        "| Concept | Description |\n",
        "|:--------|:------------|\n",
        "| **WordPiece Tokenization** | Splits words into subwords (e.g., \"nerd\" ‚Üí \"ner\" + \"##d\") |\n",
        "| **Special Tokens** | `[CLS]` for classification, `[SEP]` for separation |\n",
        "| **last_hidden_state** | Contextual embeddings for each token |\n",
        "| **pooler_output** | Single embedding for the whole sentence |\n",
        "| **Bidirectional** | Each token sees the full context (left AND right) |\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "- See `spam_classification.ipynb` for using BERT for text classification\n",
        "- See `GPT2.ipynb` for a decoder-only model comparison"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
