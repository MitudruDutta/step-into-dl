{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ğŸš€ GPT-2: Generative Pre-trained Transformer 2\n",
    "\n",
    "This notebook explores **GPT-2**, a powerful text generation model from OpenAI. GPT-2 uses the **decoder-only** architecture and is trained with **Causal Language Modeling (CLM)**.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|:--------|:------------|\n",
    "| **Architecture** | Decoder-only Transformer (12 layers in small) |\n",
    "| **Attention** | Causal (unidirectional) - only sees past tokens |\n",
    "| **Training** | Causal Language Modeling (next token prediction) |\n",
    "| **Output** | Probability distribution over vocabulary for next token |\n",
    "\n",
    "## GPT-2 vs BERT Comparison\n",
    "\n",
    "```text\n",
    "BERT (Encoder):                GPT-2 (Decoder):\n",
    "â”œâ”€ Bidirectional               â”œâ”€ Unidirectional (left-to-right)\n",
    "â”œâ”€ Masked Language Modeling    â”œâ”€ Causal Language Modeling\n",
    "â”œâ”€ Good for understanding      â”œâ”€ Good for generation\n",
    "â””â”€ Classification tasks        â””â”€ Text completion tasks\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-section",
   "metadata": {},
   "source": [
    "## 1. Quick Text Generation with Pipeline\n",
    "\n",
    "The easiest way to use GPT-2 is through Hugging Face's `pipeline` API. This handles tokenization, model inference, and decoding automatically.\n",
    "\n",
    "**Parameters:**\n",
    "- `max_length`: Maximum number of tokens to generate\n",
    "- `num_return_sequences`: How many different completions to generate\n",
    "- `set_seed`: For reproducible outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pipeline-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"I am a nerd and,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-section",
   "metadata": {},
   "source": [
    "## 2. Understanding GPT-2 Tokenization\n",
    "\n",
    "GPT-2 uses **Byte-Pair Encoding (BPE)** tokenization, which is different from BERT's WordPiece:\n",
    "\n",
    "| Feature | BERT (WordPiece) | GPT-2 (BPE) |\n",
    "|:--------|:-----------------|:------------|\n",
    "| Subword prefix | `##` for continuation | `Ä ` for space before |\n",
    "| Special tokens | `[CLS]`, `[SEP]` | None required |\n",
    "| Vocabulary size | ~30,000 | ~50,257 |\n",
    "\n",
    "The `Ä ` symbol represents a space before the token (it's the byte representation of a space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "prompt = \"Criminal Thrillers are my favourite genre of\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokens-explain",
   "metadata": {},
   "source": [
    "### Visualizing the Tokens\n",
    "\n",
    "Let's see how the text is split into tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-tokens",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokens-breakdown",
   "metadata": {},
   "source": [
    "**Token Breakdown:**\n",
    "\n",
    "```text\n",
    "Token         Meaning\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "'C'           Start of \"Criminal\"\n",
    "'riminal'     Continuation (no space)\n",
    "'Ä Thr'        Space + \"Thr\"\n",
    "'ill'         Continuation\n",
    "'ers'         Continuation  \n",
    "'Ä are'        Space + \"are\"\n",
    "'Ä my'         Space + \"my\"\n",
    "'Ä favourite'  Space + \"favourite\"\n",
    "'Ä genre'      Space + \"genre\"\n",
    "'Ä of'         Space + \"of\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lm-head-section",
   "metadata": {},
   "source": [
    "## 3. GPT-2 for Language Modeling\n",
    "\n",
    "For text generation, we use `GPT2LMHeadModel` which adds a **language modeling head** on top of the base model.\n",
    "\n",
    "```text\n",
    "GPT2LMHeadModel Architecture:\n",
    "\n",
    "    Input Tokens\n",
    "         â†“\n",
    "    Token Embeddings + Position Embeddings\n",
    "         â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  12Ã— Transformer Decoder â”‚\n",
    "    â”‚  (Masked Self-Attention) â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“\n",
    "    Linear Layer (hidden â†’ vocab_size)\n",
    "         â†“\n",
    "    Logits (50,257 values per position)\n",
    "         â†“\n",
    "    Softmax â†’ Probabilities\n",
    "         â†“\n",
    "    Next Token Prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-lm-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logits-explain",
   "metadata": {},
   "source": [
    "## 4. Understanding the Logits\n",
    "\n",
    "The model outputs **logits** - raw scores for each token in the vocabulary at each position.\n",
    "\n",
    "**Key insight:** For next-token prediction, we only care about the logits at the **last position**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logits-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shape-explain",
   "metadata": {},
   "source": [
    "**Shape breakdown:** `[1, 10, 50257]`\n",
    "- `1` = batch size\n",
    "- `10` = sequence length (10 input tokens)\n",
    "- `50257` = vocabulary size (probability for each possible next token)\n",
    "\n",
    "```text\n",
    "Position:  0   1   2   3   4   5   6   7   8   9\n",
    "Token:    \"C\" \"rim\" \"Thr\" \"ill\" \"ers\" \"are\" \"my\" \"fav\" \"genre\" \"of\"\n",
    "                                                              â†“\n",
    "                                                         We want this!\n",
    "                                                         (predict what comes after \"of\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "last-logits",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_logits = logits[:, -1, :]\n",
    "last_token_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-section",
   "metadata": {},
   "source": [
    "## 5. Predicting the Next Token\n",
    "\n",
    "To get the predicted next token, we take the **argmax** of the logits (the token with the highest score).\n",
    "\n",
    "This is called **greedy decoding** - always picking the most likely token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "argmax-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_id = torch.argmax(last_token_logits).item()\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decode-section",
   "metadata": {},
   "source": [
    "### Decoding the Token ID\n",
    "\n",
    "Let's convert this token ID back to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decode-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(next_token_id, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "result-explain",
   "metadata": {},
   "source": [
    "**Result:** The model predicts **\" horror\"** as the next word!\n",
    "\n",
    "Full sentence: *\"Criminal Thrillers are my favourite genre of **horror**\"*\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ The Autoregressive Generation Process\n",
    "\n",
    "To generate more text, we repeat this process:\n",
    "\n",
    "```text\n",
    "Step 1: \"Criminal Thrillers are my favourite genre of\" â†’ predict \"horror\"\n",
    "Step 2: \"Criminal Thrillers are my favourite genre of horror\" â†’ predict \"movies\"\n",
    "Step 3: \"Criminal Thrillers are my favourite genre of horror movies\" â†’ predict \".\"\n",
    "...\n",
    "```\n",
    "\n",
    "This is what the `pipeline` does automatically!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Key Takeaways\n",
    "\n",
    "| Concept | Description |\n",
    "|:--------|:------------|\n",
    "| **BPE Tokenization** | Uses `Ä ` prefix for tokens with leading space |\n",
    "| **Causal Attention** | Each position only attends to previous positions |\n",
    "| **Logits** | Raw scores for each vocabulary token |\n",
    "| **Greedy Decoding** | Pick the token with highest probability |\n",
    "| **Autoregressive** | Generate one token at a time, using previous as context |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "- See `BERT.ipynb` for encoder-only model comparison\n",
    "- See `spam_classification.ipynb` for a practical classification task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
