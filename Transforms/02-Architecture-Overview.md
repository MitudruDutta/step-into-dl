# ğŸ—ï¸ Architecture Overview: Encoder and Decoder

The Transformer architecture consists of two main components: the **Encoder** and the **Decoder**. Understanding how these work is crucial for modern NLP.

---

## The Original Transformer Architecture

```text
                    THE TRANSFORMER

         ENCODER                    DECODER
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Multi-Head      â”‚       â”‚ Masked Multi-   â”‚
    â”‚ Self-Attention  â”‚       â”‚ Head Attention  â”‚
    â”‚       â†“         â”‚       â”‚       â†“         â”‚
    â”‚ Add & Norm      â”‚       â”‚ Add & Norm      â”‚
    â”‚       â†“         â”‚       â”‚       â†“         â”‚
    â”‚ Feed Forward    â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚ Cross-Attention â”‚
    â”‚       â†“         â”‚Contextâ”‚       â†“         â”‚
    â”‚ Add & Norm      â”‚       â”‚ Add & Norm      â”‚
    â”‚                 â”‚       â”‚       â†“         â”‚
    â”‚     Ã— N         â”‚       â”‚ Feed Forward    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚       â†“         â”‚
             â”‚                â”‚ Add & Norm      â”‚
         Input Tokens         â”‚     Ã— N         â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                  Output Tokens
```

---

## The Encoder: Understanding Context

The encoder creates **contextual representations** of the input. It reads the entire input at once and understands how each token relates to every other.

### Key Properties

| Property                | Description                                               |
| :---------------------- | :-------------------------------------------------------- |
| **Bidirectional**       | Each token attends to all other tokens (left and right)   |
| **Parallel Processing** | All tokens processed simultaneously                       |
| **Contextual Output**   | Same word gets different representations based on context |

### Contextualization Example

```text
"bank" in different contexts:

Sentence 1: "I deposited money at the bank"
  "bank" attends to â†’ "deposited", "money"
  Result: bank â†’ [financial-context embedding]

Sentence 2: "The river bank was muddy"
  "bank" attends to â†’ "river", "muddy"
  Result: bank â†’ [geographical-context embedding]

Same word "bank" â†’ DIFFERENT embeddings!
```

---

## The Decoder: Generating Output

The decoder generates output tokens one at a time, using previous outputs and encoder representations.

### Key Properties

| Property            | Description                   |
| :------------------ | :---------------------------- |
| **Autoregressive**  | Generates one token at a time |
| **Causal Masking**  | Cannot see future tokens      |
| **Cross-Attention** | References encoder output     |

### Autoregressive Generation

```text
Translation: "Hello world" â†’ "Bonjour le monde"

Step 1: <START>              â†’ "Bonjour"
Step 2: <START> Bonjour      â†’ "le"
Step 3: <START> Bonjour le   â†’ "monde"
Step 4: <START> Bonjour le monde â†’ <END>
```

---

## Architecture Variants

### Encoder-Only (BERT, RoBERTa)

```text
Best for: Understanding & classification

    [CLS] Tokenâ‚ Tokenâ‚‚ ... TokenN
              â†“
        Encoder Stack
              â†“
    Classification / Token Labels

Use cases: Sentiment analysis, NER, Q&A
```

### Decoder-Only (GPT, LLaMA)

```text
Best for: Text generation

    "The capital of France is"
              â†“
        Decoder Stack
              â†“
    Predict next: "Paris"

Use cases: Chatbots, completion, creative writing
```

### Encoder-Decoder (T5, BART)

```text
Best for: Sequence-to-sequence

    "Translate: Hello world"
              â†“
          ENCODER
              â†“
         Context
              â†“
          DECODER
              â†“
    "Bonjour le monde"

Use cases: Translation, summarization
```

### Comparison

| Architecture        | Attention     | Best For      | Examples      |
| :------------------ | :------------ | :------------ | :------------ |
| **Encoder-Only**    | Bidirectional | Understanding | BERT, RoBERTa |
| **Decoder-Only**    | Causal        | Generation    | GPT, LLaMA    |
| **Encoder-Decoder** | Both          | Seq2Seq       | T5, BART      |

---

## Layer Components

### Multi-Head Attention

```text
Input X
   â”œâ”€â”€â–º Head 1 â†’ Attentionâ‚
   â”œâ”€â”€â–º Head 2 â†’ Attentionâ‚‚
   â””â”€â”€â–º Head 8 â†’ Attentionâ‚ˆ
              â†“
        Concatenate â†’ Linear â†’ Output
```

### Feed-Forward Network

```text
x â†’ Linear (768â†’3072) â†’ ReLU â†’ Linear (3072â†’768) â†’ output
```

### Residual + LayerNorm

```text
output = LayerNorm(x + Sublayer(x))
```

---

## Model Dimensions

| Model     | Layers | d_model | Heads | Parameters |
| :-------- | :----- | :------ | :---- | :--------- |
| BERT-Base | 12     | 768     | 12    | 110M       |
| GPT-2     | 12     | 768     | 12    | 117M       |
| GPT-3     | 96     | 12288   | 96    | 175B       |

---

## Summary

| Component    | Purpose                    | Key Feature              |
| :----------- | :------------------------- | :----------------------- |
| **Encoder**  | Contextual representations | Bidirectional            |
| **Decoder**  | Generate sequence          | Causal + cross-attention |
| **FFN**      | Non-linearity              | Position-wise            |
| **Residual** | Deep networks              | Gradient flow            |

---

## What's Next?

â¡ï¸ **Next:** [The Attention Mechanism](03-Attention-Mechanism.md)

---

_The encoder-decoder architecture provides the foundation. The real magic lies in the attention mechanism._
