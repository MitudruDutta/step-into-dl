# ğŸ›ï¸ CNN Architectures

## Evolution of CNN Architectures

CNN architectures have evolved dramatically since the 1990s. Each breakthrough introduced new concepts that pushed the boundaries of what's possible in computer vision.

```
Timeline:
1998: LeNet-5      â†’ First successful CNN
2012: AlexNet      â†’ Deep learning revolution
2014: VGGNet       â†’ Deeper is better
2014: GoogLeNet    â†’ Inception modules
2015: ResNet       â†’ Skip connections
2017: DenseNet     â†’ Dense connections
2019: EfficientNet â†’ Compound scaling
```

---

## LeNet-5 (1998)

The **pioneer** of CNNs, designed by Yann LeCun for handwritten digit recognition.

### Architecture

```
Input: 32Ã—32Ã—1 (grayscale)
       â†“
Conv1: 6 filters, 5Ã—5 â†’ 28Ã—28Ã—6
       â†“
Pool1: 2Ã—2, stride 2 â†’ 14Ã—14Ã—6
       â†“
Conv2: 16 filters, 5Ã—5 â†’ 10Ã—10Ã—16
       â†“
Pool2: 2Ã—2, stride 2 â†’ 5Ã—5Ã—16
       â†“
Flatten â†’ 400
       â†“
FC1: 120 neurons
       â†“
FC2: 84 neurons
       â†“
Output: 10 classes
```

### Key Contributions
- Demonstrated that CNNs can learn useful features
- Introduced the Conv â†’ Pool â†’ Conv â†’ Pool pattern
- Used tanh activation (ReLU wasn't popular yet)

### Limitations
- Small by modern standards (~60K parameters)
- Only works on small grayscale images
- Shallow architecture limits feature complexity

---

## AlexNet (2012)

The architecture that **sparked the deep learning revolution** by winning ImageNet 2012 with a huge margin.

### Architecture

```
Input: 227Ã—227Ã—3 (RGB)
       â†“
Conv1: 96 filters, 11Ã—11, stride 4 â†’ 55Ã—55Ã—96
       â†“
MaxPool: 3Ã—3, stride 2 â†’ 27Ã—27Ã—96
       â†“
Conv2: 256 filters, 5Ã—5, padding 2 â†’ 27Ã—27Ã—256
       â†“
MaxPool: 3Ã—3, stride 2 â†’ 13Ã—13Ã—256
       â†“
Conv3: 384 filters, 3Ã—3, padding 1 â†’ 13Ã—13Ã—384
       â†“
Conv4: 384 filters, 3Ã—3, padding 1 â†’ 13Ã—13Ã—384
       â†“
Conv5: 256 filters, 3Ã—3, padding 1 â†’ 13Ã—13Ã—256
       â†“
MaxPool: 3Ã—3, stride 2 â†’ 6Ã—6Ã—256
       â†“
Flatten â†’ 9216
       â†“
FC1: 4096 â†’ FC2: 4096 â†’ Output: 1000
```

### Key Contributions
- **ReLU activation**: Faster training than tanh/sigmoid
- **Dropout**: Regularization to prevent overfitting
- **GPU training**: Split across 2 GPUs
- **Data augmentation**: Image translations, reflections
- **Local Response Normalization** (later replaced by BatchNorm)

### Parameters
~60 million parameters (mostly in FC layers)

---

## VGGNet (2014)

Showed that **depth matters** â€” using very small (3Ã—3) filters consistently.

### VGG-16 Architecture

```
Input: 224Ã—224Ã—3
       â†“
[Conv3-64] Ã— 2 â†’ MaxPool â†’ 112Ã—112Ã—64
       â†“
[Conv3-128] Ã— 2 â†’ MaxPool â†’ 56Ã—56Ã—128
       â†“
[Conv3-256] Ã— 3 â†’ MaxPool â†’ 28Ã—28Ã—256
       â†“
[Conv3-512] Ã— 3 â†’ MaxPool â†’ 14Ã—14Ã—512
       â†“
[Conv3-512] Ã— 3 â†’ MaxPool â†’ 7Ã—7Ã—512
       â†“
Flatten â†’ FC-4096 â†’ FC-4096 â†’ Output-1000
```

### Key Contributions
- **3Ã—3 filters only**: Two 3Ã—3 convs = one 5Ã—5 receptive field, fewer parameters
- **Uniform architecture**: Easy to understand and implement
- **Deeper networks**: 16-19 layers (vs AlexNet's 8)

### Why 3Ã—3 Filters?
```
5Ã—5 conv: 25 parameters
Two 3Ã—3 convs: 9 + 9 = 18 parameters

Same receptive field, fewer parameters, more non-linearity!
```

### Limitations
- 138 million parameters (very large)
- Slow to train
- FC layers are parameter-heavy

---

## GoogLeNet / Inception (2014)

Introduced the **Inception module** â€” parallel convolutions at multiple scales.

### Inception Module

```
         Input
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
    â”‚      â”‚      â”‚      â”‚
   1Ã—1    1Ã—1    1Ã—1   3Ã—3
  conv   conv   conv  MaxPool
    â”‚      â”‚      â”‚      â”‚
    â”‚     3Ã—3    5Ã—5    1Ã—1
    â”‚    conv   conv   conv
    â”‚      â”‚      â”‚      â”‚
    â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
           â”‚
      Concatenate
           â”‚
        Output
```

### Key Contributions
- **Multi-scale processing**: Capture features at different scales simultaneously
- **1Ã—1 convolutions**: Reduce dimensions before expensive 3Ã—3 and 5Ã—5 convs
- **No FC layers**: Global Average Pooling instead
- **Auxiliary classifiers**: Help gradient flow in deep networks

### Parameters
Only ~5 million parameters (vs VGG's 138M)!

---

## ResNet (2015)

Solved the **degradation problem** with skip connections, enabling extremely deep networks.


### The Degradation Problem

```
Observation: Deeper networks should be at least as good as shallow ones
Reality: After ~20 layers, adding more layers DECREASED accuracy

Why? Not overfitting â€” training error also increased!
The problem: Optimization difficulty, not capacity
```

### Residual Block

```
        Input (x)
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚
        Conv 3Ã—3             â”‚
           â”‚                 â”‚
        BatchNorm            â”‚
           â”‚                 â”‚
          ReLU               â”‚
           â”‚                 â”‚
        Conv 3Ã—3             â”‚
           â”‚                 â”‚
        BatchNorm            â”‚
           â”‚                 â”‚
           +â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (Skip Connection)
           â”‚
          ReLU
           â”‚
        Output: F(x) + x
```

### Why Skip Connections Work

```
Without skip: Network must learn H(x) directly
With skip: Network learns F(x) = H(x) - x (the residual)

If identity mapping is optimal, F(x) = 0 is easier to learn than H(x) = x
```

### ResNet Variants

| Model | Layers | Parameters | Top-1 Accuracy |
|-------|--------|------------|----------------|
| ResNet-18 | 18 | 11.7M | 69.8% |
| ResNet-34 | 34 | 21.8M | 73.3% |
| ResNet-50 | 50 | 25.6M | 76.1% |
| ResNet-101 | 101 | 44.5M | 77.4% |
| ResNet-152 | 152 | 60.2M | 78.3% |

### Bottleneck Block (ResNet-50+)

```
Input (256 channels)
       â”‚
    1Ã—1 Conv (64) â† Reduce dimensions
       â”‚
    3Ã—3 Conv (64) â† Process
       â”‚
    1Ã—1 Conv (256) â† Restore dimensions
       â”‚
       + â† Skip connection
       â”‚
    Output
```

---

## DenseNet (2017)

Takes skip connections further â€” **every layer connects to every other layer**.

### Dense Block

```
Layer 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                     â”‚
Layer 2 â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                     â”‚
Layer 3 â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                     â”‚
Layer 4 â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Contributions
- **Feature reuse**: All previous features available to each layer
- **Gradient flow**: Direct paths from loss to early layers
- **Compact models**: Fewer parameters than ResNet for same accuracy
- **Growth rate**: Each layer adds k feature maps (typically k=32)

---

## EfficientNet (2019)

Introduced **compound scaling** â€” systematically scale depth, width, and resolution together.

### Scaling Dimensions

```
Width (w): Number of channels
Depth (d): Number of layers
Resolution (r): Input image size

Compound scaling:
  depth = Î±^Ï†
  width = Î²^Ï†
  resolution = Î³^Ï†

  where Î± Ã— Î²Â² Ã— Î³Â² â‰ˆ 2 (to double FLOPS)
```

### EfficientNet Family

| Model | Resolution | Parameters | Top-1 Accuracy |
|-------|------------|------------|----------------|
| B0 | 224 | 5.3M | 77.1% |
| B1 | 240 | 7.8M | 79.1% |
| B2 | 260 | 9.2M | 80.1% |
| B3 | 300 | 12M | 81.6% |
| B4 | 380 | 19M | 82.9% |
| B5 | 456 | 30M | 83.6% |
| B6 | 528 | 43M | 84.0% |
| B7 | 600 | 66M | 84.3% |

### Key Innovation
- **MBConv blocks**: Mobile inverted bottleneck with squeeze-and-excitation
- **Neural Architecture Search**: Found optimal base architecture
- **Compound scaling**: Balanced scaling outperforms single-dimension scaling

---

## Architecture Comparison

| Architecture | Year | Depth | Parameters | Key Innovation |
|--------------|------|-------|------------|----------------|
| LeNet-5 | 1998 | 5 | 60K | First CNN |
| AlexNet | 2012 | 8 | 60M | ReLU, Dropout, GPU |
| VGG-16 | 2014 | 16 | 138M | 3Ã—3 filters only |
| GoogLeNet | 2014 | 22 | 5M | Inception modules |
| ResNet-50 | 2015 | 50 | 25M | Skip connections |
| DenseNet-121 | 2017 | 121 | 8M | Dense connections |
| EfficientNet-B0 | 2019 | - | 5M | Compound scaling |

---

## Choosing an Architecture

### For Learning/Prototyping
- **VGG-16**: Simple, easy to understand
- **ResNet-18/34**: Good balance of simplicity and performance

### For Production (Accuracy Focus)
- **ResNet-50/101**: Reliable, well-understood
- **EfficientNet-B4/B5**: Best accuracy/efficiency trade-off

### For Mobile/Edge Deployment
- **MobileNet**: Designed for efficiency
- **EfficientNet-B0**: Good accuracy with few parameters

### For Transfer Learning
- **ResNet-50**: Most pre-trained weights available
- **EfficientNet**: Better features for fine-tuning

---

## Summary

The evolution of CNN architectures shows key principles:

1. **Deeper is better** (with proper techniques)
2. **Skip connections** enable very deep networks
3. **Small filters (3Ã—3)** are efficient and effective
4. **Global Average Pooling** replaces FC layers
5. **Compound scaling** balances depth, width, resolution

---

*Next: [06-Data-Augmentation.md](06-Data-Augmentation.md) â€” Expand your training data*
