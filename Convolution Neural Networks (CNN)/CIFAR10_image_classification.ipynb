{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb8aff9",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# üñºÔ∏è CIFAR-10 Image Classification\n",
    "\n",
    "### Building Convolutional Neural Networks from Scratch & Transfer Learning\n",
    "\n",
    "\n",
    "*A hands-on tutorial for deep learning image classification*\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d2858",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports\n",
    "\n",
    "We import the essential libraries:\n",
    "- **PyTorch** (`torch`, `torch.nn`, `torch.optim`) - Deep learning framework\n",
    "- **torchvision** - Datasets, models, and image transformations\n",
    "- **matplotlib** - Visualization\n",
    "- **numpy** - Numerical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76293638",
   "metadata": {},
   "source": [
    "## 2. Device Configuration\n",
    "\n",
    "Deep learning benefits greatly from GPU acceleration. We check if CUDA (NVIDIA GPU) is available:\n",
    "\n",
    "| Device | Training Speed | Memory |\n",
    "|--------|---------------|--------|\n",
    "| CPU | Slow (baseline) | System RAM |\n",
    "| CUDA (GPU) | 10-100x faster | GPU VRAM |\n",
    "\n",
    "> **Tip**: If you have an NVIDIA GPU, make sure PyTorch CUDA is properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a0802",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4833fc5",
   "metadata": {},
   "source": [
    "## 3. Data Transforms (Preprocessing)\n",
    "\n",
    "Before feeding images to our model, we apply transformations:\n",
    "\n",
    "### Basic Transform Pipeline:\n",
    "1. **ToTensor()** - Converts PIL Image to PyTorch tensor and scales pixels from [0, 255] to [0.0, 1.0]\n",
    "2. **Normalize()** - Normalizes each channel to have mean=0.5 and std=0.5\n",
    "\n",
    "### Why Normalize?\n",
    "Normalization helps neural networks train faster and more stably:\n",
    "- Centers data around 0\n",
    "- Ensures all features have similar scale\n",
    "- Prevents gradients from exploding or vanishing\n",
    "\n",
    "The formula is: `normalized = (pixel - mean) / std`\n",
    "\n",
    "With mean=0.5 and std=0.5:\n",
    "- Input range [0, 1] ‚Üí Output range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cdcfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic transform for both training and testing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor [0, 1]\n",
    "    transforms.Normalize(\n",
    "        mean=(0.5, 0.5, 0.5),  # Mean for each RGB channel\n",
    "        std=(0.5, 0.5, 0.5)    # Std for each RGB channel\n",
    "    )  # Output range: [-1, 1]\n",
    "])\n",
    "\n",
    "print(\"Transform pipeline created!\")\n",
    "print(\"Input: PIL Image [0-255] ‚Üí Output: Tensor [-1, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a603d",
   "metadata": {},
   "source": [
    "## 4. Loading the CIFAR-10 Dataset\n",
    "\n",
    "PyTorch's `torchvision.datasets` provides easy access to CIFAR-10:\n",
    "\n",
    "- **train=True**: Load 50,000 training images\n",
    "- **train=False**: Load 10,000 test images  \n",
    "- **download=True**: Download if not present locally\n",
    "- **transform**: Apply our preprocessing pipeline\n",
    "\n",
    "The data is stored in the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05407e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Test samples: {len(test_dataset):,}\")\n",
    "print(f\"\\nClasses: {train_dataset.classes}\")\n",
    "print(f\"Number of classes: {len(train_dataset.classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649375bf",
   "metadata": {},
   "source": [
    "## 5. Creating Data Loaders\n",
    "\n",
    "**DataLoaders** are essential for efficient training:\n",
    "\n",
    "| Parameter | Purpose |\n",
    "|-----------|--------|\n",
    "| `batch_size` | Number of samples per gradient update |\n",
    "| `shuffle` | Randomize order each epoch (training only) |\n",
    "| `num_workers` | Parallel data loading processes |\n",
    "\n",
    "### Batch Size Considerations:\n",
    "- **Larger batch**: More stable gradients, faster training, more memory\n",
    "- **Smaller batch**: More noise (can help escape local minima), less memory\n",
    "- **Common values**: 32, 64, 128, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# Training loader with shuffling\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True  # Randomize order each epoch\n",
    ")\n",
    "\n",
    "# Test loader without shuffling\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False  # Keep order consistent for evaluation\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b81fe",
   "metadata": {},
   "source": [
    "### Understanding Batch Dimensions\n",
    "\n",
    "Let's examine the shape of our data:\n",
    "- **Images**: `[batch_size, channels, height, width]` = `[100, 3, 32, 32]`\n",
    "- **Labels**: `[batch_size]` = `[100]`\n",
    "\n",
    "CIFAR-10 images are RGB (3 channels) with 32x32 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch to examine\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"  - Batch size: {images.shape[0]}\")\n",
    "print(f\"  - Channels (RGB): {images.shape[1]}\")\n",
    "print(f\"  - Height: {images.shape[2]}\")\n",
    "print(f\"  - Width: {images.shape[3]}\")\n",
    "print(f\"\\nLabel batch shape: {labels.shape}\")\n",
    "print(f\"Pixel value range: [{images.min():.2f}, {images.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d25fc3",
   "metadata": {},
   "source": [
    "## 6. Visualizing Sample Images\n",
    "\n",
    "Let's visualize some training images to understand our data. We need to:\n",
    "1. **Unnormalize**: Reverse the normalization (multiply by std, add mean)\n",
    "2. **Rearrange dimensions**: From `[C, H, W]` to `[H, W, C]` for matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b9fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, title=None):\n",
    "    \"\"\"Display a normalized image tensor.\"\"\"\n",
    "    # Unnormalize: reverse the normalization\n",
    "    img = img / 2 + 0.5  # [-1, 1] -> [0, 1]\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    # Rearrange from [C, H, W] to [H, W, C]\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Create a grid of images\n",
    "img_grid = torchvision.utils.make_grid(images[:8], nrow=8)\n",
    "imshow(img_grid)\n",
    "\n",
    "# Show corresponding labels\n",
    "class_names = train_dataset.classes\n",
    "print(\"Labels:\", [class_names[label] for label in labels[:8].tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e026945a",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Custom CNN Architecture\n",
    "\n",
    "## 7. Building a CNN from Scratch\n",
    "\n",
    "Our CNN architecture follows the classic pattern:\n",
    "\n",
    "### Feature Extraction (Convolutional Layers)\n",
    "```\n",
    "Input: (3, 32, 32) - RGB image\n",
    "    ‚Üì\n",
    "Conv1: 3‚Üí32 filters, 3x3, padding=1 ‚Üí (32, 32, 32)\n",
    "ReLU + MaxPool 2x2                  ‚Üí (32, 16, 16)\n",
    "    ‚Üì\n",
    "Conv2: 32‚Üí64 filters, 3x3, padding=1 ‚Üí (64, 16, 16)\n",
    "ReLU + MaxPool 2x2                   ‚Üí (64, 8, 8)\n",
    "    ‚Üì\n",
    "Conv3: 64‚Üí64 filters, 3x3, padding=1 ‚Üí (64, 8, 8)\n",
    "ReLU + MaxPool 2x2                   ‚Üí (64, 4, 4)\n",
    "```\n",
    "\n",
    "### Classification (Fully Connected Layers)\n",
    "```\n",
    "Flatten: 64 √ó 4 √ó 4 = 1024 features\n",
    "    ‚Üì\n",
    "FC1: 1024 ‚Üí 64 + ReLU\n",
    "    ‚Üì\n",
    "FC2: 64 ‚Üí 10 (output classes)\n",
    "```\n",
    "\n",
    "### Key Concepts:\n",
    "- **Conv2d**: Applies learnable filters to detect features\n",
    "- **ReLU**: Non-linear activation, enables learning complex patterns\n",
    "- **MaxPool2d**: Reduces spatial dimensions, provides translation invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322456e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: 3 -> 32 channels\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),   # (32, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # (32, 16, 16)\n",
    "            \n",
    "            # Block 2: 32 -> 64 channels\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # (64, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # (64, 8, 8)\n",
    "            \n",
    "            # Block 3: 64 -> 64 channels\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # (64, 8, 8)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # (64, 4, 4)\n",
    "        )\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                    # 64 * 4 * 4 = 1024\n",
    "            nn.Linear(64 * 4 * 4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create model and move to device\n",
    "model = CNN(num_classes=10).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbbd7b",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "\n",
    "Let's verify our model by passing a sample batch and counting parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "sample_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "sample_output = model(sample_input)\n",
    "print(f\"Input shape:  {sample_input.shape}\")\n",
    "print(f\"Output shape: {sample_output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf44d37",
   "metadata": {},
   "source": [
    "## 8. Loss Function and Optimizer\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "The standard loss function for multi-class classification:\n",
    "$$\\mathcal{L} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Where:\n",
    "- $C$ = number of classes (10)\n",
    "- $y_i$ = true label (one-hot encoded)\n",
    "- $\\hat{y}_i$ = predicted probability for class $i$\n",
    "\n",
    "### Adam Optimizer\n",
    "Adam (Adaptive Moment Estimation) combines:\n",
    "- **Momentum**: Accelerates convergence\n",
    "- **RMSprop**: Adapts learning rate per parameter\n",
    "\n",
    "**Learning rate** controls step size during optimization. Common values: 0.001, 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer with learning rate\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Loss function: CrossEntropyLoss\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7c758",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "The training process repeats these steps for each batch:\n",
    "\n",
    "1. **Forward pass**: Compute predictions `model(images)`\n",
    "2. **Compute loss**: Measure error `criterion(outputs, labels)`\n",
    "3. **Backward pass**: Compute gradients `loss.backward()`\n",
    "4. **Update weights**: Apply gradients `optimizer.step()`\n",
    "5. **Zero gradients**: Clear for next batch `optimizer.zero_grad()`\n",
    "\n",
    "### Training Visualization\n",
    "We track loss over time to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    \"\"\"Train the model and return loss history.\"\"\"\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients from previous step\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e068a89",
   "metadata": {},
   "source": [
    "## 10. Evaluation Function\n",
    "\n",
    "To evaluate model performance:\n",
    "- Use `model.eval()` to disable dropout and batch normalization updates\n",
    "- Use `torch.no_grad()` to disable gradient computation (saves memory)\n",
    "- Calculate accuracy as percentage of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model accuracy on test set.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class with highest score\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9a6df",
   "metadata": {},
   "source": [
    "## 11. Training the Custom CNN\n",
    "\n",
    "Now let's train our model for 5 epochs and visualize the training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698328d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training Custom CNN...\")\n",
    "print(\"=\" * 40)\n",
    "loss_history = train_model(model, train_loader, criterion, optimizer, num_epochs=5)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "accuracy = evaluate_model(model, test_loader)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edccb956",
   "metadata": {},
   "source": [
    "### Training Loss Visualization\n",
    "\n",
    "A decreasing loss curve indicates the model is learning. If loss plateaus or increases, consider:\n",
    "- Adjusting learning rate\n",
    "- Adding regularization\n",
    "- Training longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4137f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(loss_history) + 1), loss_history, 'b-o', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(1, len(loss_history) + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f0208",
   "metadata": {},
   "source": [
    "## 12. Per-Class Accuracy\n",
    "\n",
    "Let's see how well our model performs on each class. Some classes might be harder to distinguish than others (e.g., cat vs dog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_accuracy(model, test_loader, class_names):\n",
    "    \"\"\"Calculate accuracy for each class.\"\"\"\n",
    "    model.eval()\n",
    "    class_correct = {name: 0 for name in class_names}\n",
    "    class_total = {name: 0 for name in class_names}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            for label, pred in zip(labels, predicted):\n",
    "                class_name = class_names[label]\n",
    "                class_total[class_name] += 1\n",
    "                if label == pred:\n",
    "                    class_correct[class_name] += 1\n",
    "    \n",
    "    print(\"Per-Class Accuracy:\")\n",
    "    print(\"-\" * 30)\n",
    "    for name in class_names:\n",
    "        acc = 100 * class_correct[name] / class_total[name]\n",
    "        print(f\"{name:<12}: {acc:>6.2f}%\")\n",
    "\n",
    "per_class_accuracy(model, test_loader, train_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae0055",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Transfer Learning with Pre-trained Models\n",
    "\n",
    "## 13. What is Transfer Learning?\n",
    "\n",
    "**Transfer Learning** uses knowledge from models trained on large datasets (like ImageNet) and applies it to new tasks.\n",
    "\n",
    "### Why Transfer Learning Works:\n",
    "- **Early layers** learn generic features (edges, textures, colors)\n",
    "- **Later layers** learn task-specific features\n",
    "- Generic features transfer well to new tasks!\n",
    "\n",
    "### Our Approach:\n",
    "1. Load a pre-trained ResNet-18 (trained on ImageNet)\n",
    "2. Replace the final classification layer (1000 ‚Üí 10 classes)\n",
    "3. Fine-tune on CIFAR-10\n",
    "\n",
    "### Important: Image Size\n",
    "ResNet was designed for 224x224 images, but CIFAR-10 has 32x32 images. We need to resize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca55fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform with resizing for transfer learning\n",
    "transform_resnet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet expects 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet statistics\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Create new datasets with resized images\n",
    "train_dataset_resnet = datasets.CIFAR10(\n",
    "    root=\"data\", train=True, download=True, transform=transform_resnet\n",
    ")\n",
    "test_dataset_resnet = datasets.CIFAR10(\n",
    "    root=\"data\", train=False, download=True, transform=transform_resnet\n",
    ")\n",
    "\n",
    "# Create data loaders (smaller batch due to larger images)\n",
    "train_loader_resnet = DataLoader(train_dataset_resnet, batch_size=32, shuffle=True)\n",
    "test_loader_resnet = DataLoader(test_dataset_resnet, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Datasets created with 224x224 images for ResNet\")\n",
    "print(f\"Training batches: {len(train_loader_resnet)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0eb598",
   "metadata": {},
   "source": [
    "## 14. Loading Pre-trained ResNet-18\n",
    "\n",
    "ResNet-18 architecture:\n",
    "- **18 layers** deep\n",
    "- **~11 million parameters**\n",
    "- Uses **skip connections** (residual connections) to enable training deeper networks\n",
    "- Final layer: `fc` (fully connected) with 1000 output classes\n",
    "\n",
    "We replace `fc` to output 10 classes for CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5cbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet-18\n",
    "model_resnet = models.resnet18(weights='DEFAULT')\n",
    "\n",
    "# Check original final layer\n",
    "print(f\"Original final layer: {model_resnet.fc}\")\n",
    "print(f\"Input features: {model_resnet.fc.in_features}\")\n",
    "print(f\"Output classes: {model_resnet.fc.out_features}\")\n",
    "\n",
    "# Replace final layer for 10 classes\n",
    "num_classes = 10\n",
    "model_resnet.fc = nn.Linear(model_resnet.fc.in_features, num_classes)\n",
    "\n",
    "print(f\"\\nModified final layer: {model_resnet.fc}\")\n",
    "\n",
    "# Move to device\n",
    "model_resnet = model_resnet.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model_resnet.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45c4f0",
   "metadata": {},
   "source": [
    "## 15. Training ResNet-18\n",
    "\n",
    "Since most weights are pre-trained, we often use a smaller learning rate for fine-tuning to avoid destroying the learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion_resnet = nn.CrossEntropyLoss()\n",
    "optimizer_resnet = optim.Adam(model_resnet.parameters(), lr=0.0001)  # Lower LR for fine-tuning\n",
    "\n",
    "# Train the model\n",
    "print(\"Training ResNet-18 with Transfer Learning...\")\n",
    "print(\"=\" * 40)\n",
    "loss_history_resnet = train_model(\n",
    "    model_resnet, train_loader_resnet, criterion_resnet, optimizer_resnet, num_epochs=5\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "accuracy_resnet = evaluate_model(model_resnet, test_loader_resnet)\n",
    "print(f\"ResNet-18 Test Accuracy: {accuracy_resnet:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23cdc43",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Model Comparison\n",
    "\n",
    "## 16. Comparing Results\n",
    "\n",
    "Let's compare our custom CNN with the transfer learning approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe921bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n{'Model':<25} {'Accuracy':>15}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    custom_acc = evaluate_model(model, test_loader)\n",
    "    print(f\"{'Custom CNN (32x32)':<25} {custom_acc:>14.2f}%\")\n",
    "except:\n",
    "    print(f\"{'Custom CNN':<25} {'Not trained':>15}\")\n",
    "\n",
    "try:\n",
    "    resnet_acc = evaluate_model(model_resnet, test_loader_resnet)\n",
    "    print(f\"{'ResNet-18 (224x224)':<25} {resnet_acc:>14.2f}%\")\n",
    "except:\n",
    "    print(f\"{'ResNet-18':<25} {'Not trained':>15}\")\n",
    "\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67060136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history, 'b-o', label='Custom CNN')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Custom CNN Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history_resnet, 'r-o', label='ResNet-18')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('ResNet-18 Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f4458",
   "metadata": {},
   "source": [
    "## 17. Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Custom CNN**\n",
    "   - Fast training on small 32x32 images\n",
    "   - Good for understanding CNN fundamentals\n",
    "   - Limited by architecture design and data size\n",
    "\n",
    "2. **Transfer Learning (ResNet-18)**\n",
    "   - Leverages pre-trained features from ImageNet\n",
    "   - Often achieves better accuracy with less training\n",
    "   - Requires resizing images (more computation)\n",
    "\n",
    "### Tips for Better Results:\n",
    "\n",
    "| Technique | Impact |\n",
    "|-----------|--------|\n",
    "| Data Augmentation | +2-5% accuracy |\n",
    "| Learning Rate Scheduling | +1-3% accuracy |\n",
    "| More Training Epochs | +2-5% accuracy |\n",
    "| Larger Models (ResNet-50) | +3-7% accuracy |\n",
    "| Batch Normalization | Faster convergence |\n",
    "| Dropout | Reduces overfitting |\n",
    "\n",
    "### Next Steps:\n",
    "- üîß Add data augmentation (random crops, flips, color jitter)\n",
    "- üìä Implement learning rate scheduling\n",
    "- ‚è∞ Train for more epochs (20-50)\n",
    "- üß™ Try other architectures (VGG, EfficientNet)\n",
    "- üìà Add validation set for hyperparameter tuning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
