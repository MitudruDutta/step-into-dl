{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b4a77d",
   "metadata": {},
   "source": [
    "# Caltech-101 Image Classification with CNNs\n",
    "\n",
    "This notebook demonstrates image classification on the **Caltech-101 dataset** using Convolutional Neural Networks (CNNs). We'll explore:\n",
    "\n",
    "1. **Custom CNN Architecture** - Building a CNN from scratch\n",
    "2. **Transfer Learning with ResNet-18** - Using pre-trained weights\n",
    "3. **Transfer Learning with EfficientNet-B0** - A more efficient architecture\n",
    "\n",
    "## About Caltech-101\n",
    "The Caltech-101 dataset contains images from 101 object categories (plus a background category). Each category has about 40-800 images, making it a challenging multi-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eafe117",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We import the necessary libraries:\n",
    "- **PyTorch** for deep learning\n",
    "- **torchvision** for datasets, models, and transforms\n",
    "- **matplotlib** for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2369b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1780b",
   "metadata": {},
   "source": [
    "## 2. Device Configuration\n",
    "\n",
    "We check if a GPU is available. Training on GPU is significantly faster than CPU for deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c554f31d",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Augmentation\n",
    "\n",
    "Data augmentation is crucial for improving model generalization. Our transform pipeline includes:\n",
    "\n",
    "- **Resize**: Standardize all images to 128x128 pixels\n",
    "- **RandomHorizontalFlip**: Randomly flip images horizontally (50% chance)\n",
    "- **RGB Conversion**: Ensure all images have 3 channels (some Caltech-101 images are grayscale)\n",
    "- **RandomRotation**: Rotate images by up to ¬±10 degrees\n",
    "- **ToTensor**: Convert PIL images to PyTorch tensors\n",
    "- **Normalize**: Normalize pixel values to [-1, 1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285da482",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Lambda(lambda x: x.convert('RGB')),  # Handle grayscale images\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d827f9b",
   "metadata": {},
   "source": [
    "## 4. Loading the Dataset\n",
    "\n",
    "We download and load the Caltech-101 dataset. The dataset will be automatically downloaded if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Caltech101(root=\"data\", download=True, transform=transform)\n",
    "print(f\"Total samples in dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98333ae7",
   "metadata": {},
   "source": [
    "### Exploring the Categories\n",
    "\n",
    "Let's see all 101 categories in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed262d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of categories: {len(dataset.categories)}\")\n",
    "print(f\"Categories: {dataset.categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4625bec",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split\n",
    "\n",
    "We split the dataset into:\n",
    "- **80% Training set** - Used to train the model\n",
    "- **20% Test set** - Used to evaluate model performance\n",
    "\n",
    "This split helps us assess how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de4fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e3a84",
   "metadata": {},
   "source": [
    "## 6. Creating Data Loaders\n",
    "\n",
    "DataLoaders handle batching and shuffling of data during training:\n",
    "- **batch_size=32**: Process 32 images at a time\n",
    "- **shuffle=True**: Randomize order each epoch to prevent learning order-dependent patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e194f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6302e",
   "metadata": {},
   "source": [
    "### Inspecting a Batch\n",
    "\n",
    "Let's examine the shape of our data batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2aa97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch image shape: {images.shape}\")  # [batch_size, channels, height, width]\n",
    "    print(f\"Batch labels shape: {labels.shape}\")  # [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2823548",
   "metadata": {},
   "source": [
    "## 7. Visualizing Sample Images\n",
    "\n",
    "Let's visualize some training images to understand our data better. The `imshow` function reverses the normalization to display images correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf59654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    \"\"\"Display a normalized image tensor.\"\"\"\n",
    "    img = img / 2 + 0.5  # Unnormalize: reverse the normalization\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display first 8 images from the batch\n",
    "imshow(torchvision.utils.make_grid(images[:8]))\n",
    "\n",
    "# Show corresponding labels\n",
    "print(\"Labels:\", [dataset.categories[i] for i in labels[:8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebd72e",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Custom CNN Architecture\n",
    "\n",
    "## 8. Building a CNN from Scratch\n",
    "\n",
    "Our custom CNN architecture consists of:\n",
    "\n",
    "### Convolutional Layers (Feature Extraction)\n",
    "1. **Conv Layer 1**: 3 ‚Üí 32 channels, 3x3 kernel, same padding\n",
    "   - Output: (32, 128, 128)\n",
    "   - MaxPool: (32, 64, 64)\n",
    "\n",
    "2. **Conv Layer 2**: 32 ‚Üí 64 channels, 3x3 kernel, same padding\n",
    "   - Output: (64, 64, 64)\n",
    "   - MaxPool: (64, 32, 32)\n",
    "\n",
    "### Fully Connected Layers (Classification)\n",
    "- Flatten: 64 √ó 32 √ó 32 = 65,536 features\n",
    "- FC1: 65,536 ‚Üí 256\n",
    "- FC2: 256 ‚Üí 128\n",
    "- FC3: 128 ‚Üí num_classes (101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935355f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            # First Convolutional Block\n",
    "            nn.Conv2d(3, 32, kernel_size=(3,3), padding=\"same\"),  # output: (32, 128, 128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(stride=(2,2), kernel_size=(2,2)),        # output: (32, 64, 64)\n",
    "\n",
    "            # Second Convolutional Block\n",
    "            nn.Conv2d(32, 64, kernel_size=(3,3), padding=\"same\"), # output: (64, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(stride=(2,2), kernel_size=(2,2)),        # output: (64, 32, 32)\n",
    "\n",
    "            # Classifier\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 32 * 32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b1d93f",
   "metadata": {},
   "source": [
    "## 9. Training Function\n",
    "\n",
    "The training loop performs the following steps for each epoch:\n",
    "1. **Forward pass**: Compute predictions\n",
    "2. **Loss calculation**: Measure prediction error using CrossEntropyLoss\n",
    "3. **Backward pass**: Compute gradients\n",
    "4. **Optimization**: Update weights using Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_loader, test_loader, num_epochs=5):\n",
    "    \"\"\"Train the model for specified number of epochs.\"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()        # Clear previous gradients\n",
    "            outputs = model(images)      # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()              # Backward pass\n",
    "            optimizer.step()             # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529576a",
   "metadata": {},
   "source": [
    "## 10. Evaluation Function\n",
    "\n",
    "The test function evaluates model performance on unseen data:\n",
    "- Uses `torch.no_grad()` to disable gradient computation (saves memory)\n",
    "- Calculates accuracy as the percentage of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    \"\"\"Evaluate model accuracy on test set.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72325fa8",
   "metadata": {},
   "source": [
    "## 11. Training the Custom CNN\n",
    "\n",
    "Now let's train our custom CNN:\n",
    "- **Loss Function**: CrossEntropyLoss (standard for multi-class classification)\n",
    "- **Optimizer**: Adam with learning rate 0.001\n",
    "- **Epochs**: 5 (increase for better results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "num_classes = len(dataset.categories)\n",
    "model = CNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Training Custom CNN for {num_classes} classes...\")\n",
    "train_model(model, optimizer, criterion, train_loader, test_loader, num_epochs=5)\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3815c",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Transfer Learning with ResNet-18\n",
    "\n",
    "## 12. What is Transfer Learning?\n",
    "\n",
    "**Transfer Learning** leverages knowledge from pre-trained models:\n",
    "- ResNet-18 was trained on ImageNet (1.2M images, 1000 classes)\n",
    "- The convolutional layers have learned general features (edges, textures, shapes)\n",
    "- We only need to replace the final classification layer for our 101 classes\n",
    "\n",
    "### Benefits:\n",
    "- ‚ö° **Faster training** - Most weights are already optimized\n",
    "- üìà **Better performance** - Leverages learned feature representations\n",
    "- üìâ **Less data needed** - Pre-trained features generalize well\n",
    "\n",
    "### ResNet-18 Architecture Overview\n",
    "ResNet (Residual Network) introduced **skip connections** that allow gradients to flow directly through the network, enabling training of very deep networks. ResNet-18 has:\n",
    "- 18 layers deep\n",
    "- ~11 million parameters\n",
    "- Pre-trained on ImageNet (1000 classes)\n",
    "\n",
    "We'll replace the final fully connected layer (`fc`) to output 101 classes instead of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead929733093792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet-18\n",
    "model_resnet = models.resnet18(weights='DEFAULT')\n",
    "\n",
    "# Examine the original final layer\n",
    "print(f\"Original FC layer: {model_resnet.fc}\")\n",
    "print(f\"Input features: {model_resnet.fc.in_features}\")\n",
    "print(f\"Original output classes: {model_resnet.fc.out_features}\")\n",
    "\n",
    "# Replace the final layer for our 101 classes\n",
    "model_resnet.fc = nn.Linear(model_resnet.fc.in_features, num_classes)\n",
    "print(f\"\\nModified FC layer: {model_resnet.fc}\")\n",
    "\n",
    "# Move model to device\n",
    "model_resnet = model_resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4bdf0",
   "metadata": {},
   "source": [
    "## 13. Training ResNet-18\n",
    "\n",
    "Now we'll train the modified ResNet-18. Since the convolutional layers are already pre-trained, training is faster and typically achieves better accuracy than training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feadee78a82d06c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:10:02.303497884Z",
     "start_time": "2025-12-25T16:09:02.436166659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 3.3185\n",
      "Epoch [2/5], Loss: 2.4057\n",
      "Epoch [3/5], Loss: 1.9091\n",
      "Epoch [4/5], Loss: 1.5524\n",
      "Epoch [5/5], Loss: 1.2774\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer for ResNet-18\n",
    "criterion_resnet = nn.CrossEntropyLoss()\n",
    "optimizer_resnet = optim.Adam(model_resnet.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training ResNet-18 with Transfer Learning...\")\n",
    "train_model(model_resnet, optimizer_resnet, criterion_resnet, train_loader, test_loader, num_epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating ResNet-18...\")\n",
    "resnet_accuracy = test_model(model_resnet, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf7449",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Transfer Learning with EfficientNet-B0\n",
    "\n",
    "## 14. What is EfficientNet?\n",
    "\n",
    "**EfficientNet** is a family of models that achieve state-of-the-art accuracy while being much more efficient than previous models.\n",
    "\n",
    "### Key Innovations:\n",
    "- **Compound Scaling**: Balances network depth, width, and resolution\n",
    "- **Neural Architecture Search (NAS)**: Architecture was found automatically\n",
    "- **Mobile Inverted Bottleneck (MBConv)**: Efficient building blocks\n",
    "\n",
    "### EfficientNet-B0 Specifications:\n",
    "- ~5.3 million parameters (half of ResNet-18!)\n",
    "- Better accuracy with fewer parameters\n",
    "- Uses Squeeze-and-Excitation blocks for channel attention\n",
    "\n",
    "### Why EfficientNet?\n",
    "| Model | Parameters | Top-1 Accuracy (ImageNet) |\n",
    "|-------|-----------|---------------------------|\n",
    "| ResNet-18 | 11.7M | 69.8% |\n",
    "| EfficientNet-B0 | 5.3M | 77.1% |\n",
    "\n",
    "EfficientNet achieves **better accuracy with fewer parameters**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb788f51a0e763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T16:15:10.383534813Z",
     "start_time": "2025-12-25T16:13:38.862567216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /home/btwitsvoid/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:04<00:00, 4.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.4256\n",
      "Epoch [2/5], Loss: 0.4710\n",
      "Epoch [3/5], Loss: 0.2967\n",
      "Epoch [4/5], Loss: 0.2371\n",
      "Epoch [5/5], Loss: 0.2237\n",
      "Test Accuracy: 86.98%\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained EfficientNet-B0\n",
    "model_efficient = models.efficientnet_b0(weights='DEFAULT')\n",
    "\n",
    "# Examine the classifier structure\n",
    "print(f\"Original classifier: {model_efficient.classifier}\")\n",
    "\n",
    "# EfficientNet uses a Sequential classifier with Dropout + Linear\n",
    "# We replace only the final Linear layer\n",
    "model_efficient.classifier[1] = nn.Linear(model_efficient.classifier[1].in_features, num_classes)\n",
    "print(f\"\\nModified classifier: {model_efficient.classifier}\")\n",
    "\n",
    "# Move to device\n",
    "model_efficient = model_efficient.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion_efficient = nn.CrossEntropyLoss()\n",
    "optimizer_efficient = optim.Adam(model_efficient.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining EfficientNet-B0 with Transfer Learning...\")\n",
    "train_model(model_efficient, optimizer_efficient, criterion_efficient, train_loader, test_loader, num_epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating EfficientNet-B0...\")\n",
    "efficient_accuracy = test_model(model_efficient, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2183692",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Model Comparison and Summary\n",
    "\n",
    "## 15. Comparing All Three Models\n",
    "\n",
    "Let's summarize the performance of our three approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a923ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n{'Model':<25} {'Test Accuracy':>15}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Note: Run all training cells above first to get actual accuracies\n",
    "# The variables below will contain the accuracies after training\n",
    "try:\n",
    "    print(f\"{'Custom CNN':<25} {test_model(model, test_loader):>14.2f}%\")\n",
    "except:\n",
    "    print(f\"{'Custom CNN':<25} {'Not trained':>15}\")\n",
    "    \n",
    "try:\n",
    "    print(f\"{'ResNet-18':<25} {test_model(model_resnet, test_loader):>14.2f}%\")\n",
    "except:\n",
    "    print(f\"{'ResNet-18':<25} {'Not trained':>15}\")\n",
    "    \n",
    "try:\n",
    "    print(f\"{'EfficientNet-B0':<25} {test_model(model_efficient, test_loader):>14.2f}%\")\n",
    "except:\n",
    "    print(f\"{'EfficientNet-B0':<25} {'Not trained':>15}\")\n",
    "\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c2ff4",
   "metadata": {},
   "source": [
    "## 16. Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Custom CNN** \n",
    "   - Good for learning CNN fundamentals\n",
    "   - Requires more training time and data to achieve good results\n",
    "   - Full control over architecture design\n",
    "\n",
    "2. **Transfer Learning Benefits**\n",
    "   - Pre-trained models provide excellent starting weights\n",
    "   - Significantly faster training convergence\n",
    "   - Better accuracy, especially with limited data\n",
    "\n",
    "3. **Model Selection**\n",
    "   - **ResNet-18**: Good balance of speed and accuracy, well-understood architecture\n",
    "   - **EfficientNet-B0**: Best accuracy-to-parameter ratio, modern architecture\n",
    "\n",
    "### Next Steps to Improve:\n",
    "- üîß **Fine-tune learning rate** using learning rate schedulers\n",
    "- üìä **Add more data augmentation** (color jitter, random crops)\n",
    "- ‚è∞ **Train for more epochs** (10-20 epochs)\n",
    "- üßä **Freeze early layers** to speed up training\n",
    "- üìà **Use larger models** (ResNet-50, EfficientNet-B3)\n",
    "\n",
    "### When to Use Each Approach:\n",
    "| Scenario | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| Learning CNNs | Custom CNN |\n",
    "| Limited data | Transfer Learning |\n",
    "| Production deployment | EfficientNet (efficiency) |\n",
    "| Quick prototyping | ResNet (simplicity) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
