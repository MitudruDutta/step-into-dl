# ğŸ” Kernels and Filters

## What are Kernels and Filters?

A **kernel** (also called a **filter**) is a small matrix of learnable weights that slides across the input image to detect specific features. The terms "kernel" and "filter" are often used interchangeably, though technically a filter can contain multiple kernels (one per input channel).

---

## How Filters Detect Features

### The Convolution Process

When a filter slides across an image, it performs element-wise multiplication followed by summation at each position:

```
Image Patch        Filter           Result
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10 20 30â”‚      â”‚ -1  0  1â”‚
â”‚ 40 50 60â”‚  Ã—   â”‚ -2  0  2â”‚  =  Sum of products
â”‚ 70 80 90â”‚      â”‚ -1  0  1â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

= (10Ã—-1) + (20Ã—0) + (30Ã—1) + (40Ã—-2) + (50Ã—0) + (60Ã—2) + (70Ã—-1) + (80Ã—0) + (90Ã—1)
= -10 + 0 + 30 - 80 + 0 + 120 - 70 + 0 + 90
= 80
```

The result is high when the image patch matches the pattern encoded in the filter.

---

## Classic Hand-Crafted Filters

Before deep learning, computer vision relied on hand-designed filters. Understanding these helps build intuition for what CNNs learn.

### Edge Detection Filters

**Sobel Filter (Horizontal Edges)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -1  -2  -1 â”‚
â”‚  0   0   0 â”‚
â”‚  1   2   1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Detects horizontal edges (transitions from dark to light vertically)
```

**Sobel Filter (Vertical Edges)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -1   0   1 â”‚
â”‚ -2   0   2 â”‚
â”‚ -1   0   1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Detects vertical edges (transitions from dark to light horizontally)
```

### Sharpening Filter
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  0  -1   0 â”‚
â”‚ -1   5  -1 â”‚
â”‚  0  -1   0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Enhances edges and details
```

### Blur Filter (Box Blur)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1/9  1/9  1/9    â”‚
â”‚ 1/9  1/9  1/9    â”‚
â”‚ 1/9  1/9  1/9    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Averages neighboring pixels, smoothing the image
```

### Gaussian Blur
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1/16  2/16  1/16     â”‚
â”‚ 2/16  4/16  2/16     â”‚
â”‚ 1/16  2/16  1/16     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Weighted average, more natural smoothing
```

---

## Learned Filters in CNNs

Unlike hand-crafted filters, CNN filters are **learned from data** through backpropagation. The network discovers which patterns are useful for the task.

### What CNNs Learn

**Layer 1 Filters** (typically look like Gabor filters):
```
Common patterns learned:
- Horizontal edges at various angles
- Vertical edges at various angles  
- Color blobs (for RGB images)
- Gradient detectors
```

**Deeper Layer Filters**:
- Combinations of earlier features
- Texture patterns
- Object parts
- Abstract concepts

### Visualization of Learned Filters

```
Layer 1 (Edge Detectors):
â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”
â”‚ / â”‚ â”‚ \ â”‚ â”‚ â€” â”‚ â”‚ | â”‚
â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜

Layer 2 (Corners, Textures):
â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”
â”‚ â”Œ â”‚ â”‚ â” â”‚ â”‚ # â”‚
â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜

Layer 3+ (Complex Patterns):
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚ ğŸ‘ï¸  â”‚ â”‚ ğŸ”µ  â”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
```

---

## Filter Dimensions

### For 2D Images

```
Single Channel Input (Grayscale):
  Filter shape: (kernel_height, kernel_width)
  Example: 3Ã—3 filter

Multi-Channel Input (RGB):
  Filter shape: (kernel_height, kernel_width, input_channels)
  Example: 3Ã—3Ã—3 filter for RGB image
  
  The filter has separate weights for each channel,
  but produces a single output value per position.
```

### Multiple Filters

```
Input: H Ã— W Ã— C_in
Filters: K filters, each of size (k Ã— k Ã— C_in)
Output: H' Ã— W' Ã— K

Example:
  Input: 32Ã—32Ã—3 (RGB image)
  32 filters of size 3Ã—3Ã—3
  Output: 30Ã—30Ã—32 (32 feature maps)
```

---

## Common Kernel Sizes

| Size | Use Case | Receptive Field |
|------|----------|-----------------|
| **1Ã—1** | Channel mixing, dimensionality reduction | 1 pixel |
| **3Ã—3** | Most common, good balance | 3Ã—3 pixels |
| **5Ã—5** | Larger features, less common now | 5Ã—5 pixels |
| **7Ã—7** | Often in first layer only | 7Ã—7 pixels |

### Why 3Ã—3 is Preferred

Two 3Ã—3 convolutions have the same receptive field as one 5Ã—5, but with fewer parameters:

```
5Ã—5 convolution: 5 Ã— 5 = 25 parameters
Two 3Ã—3 convolutions: 3 Ã— 3 + 3 Ã— 3 = 18 parameters

Same receptive field, fewer parameters, more non-linearity!
```

---

## 1Ã—1 Convolutions

Despite their small size, 1Ã—1 convolutions are powerful:

### Purpose

1. **Dimensionality Reduction**: Reduce number of channels
2. **Dimensionality Expansion**: Increase number of channels
3. **Add Non-linearity**: When followed by activation
4. **Cross-Channel Interaction**: Mix information across channels

### Example

```
Input: 56Ã—56Ã—256
1Ã—1 Conv with 64 filters
Output: 56Ã—56Ã—64

Reduced channels from 256 to 64 without changing spatial dimensions
```

---

## Depthwise Separable Convolutions

A more efficient alternative to standard convolutions, used in MobileNet and EfficientNet.

### Standard Convolution

```
Input: H Ã— W Ã— C_in
Filter: k Ã— k Ã— C_in Ã— C_out
Operations: H Ã— W Ã— k Ã— k Ã— C_in Ã— C_out
```

### Depthwise Separable Convolution

**Step 1: Depthwise Convolution**
```
Apply one kÃ—k filter per input channel
Operations: H Ã— W Ã— k Ã— k Ã— C_in
```

**Step 2: Pointwise Convolution (1Ã—1)**
```
Mix channels with 1Ã—1 convolutions
Operations: H Ã— W Ã— C_in Ã— C_out
```

### Efficiency Comparison

```
Standard: kÂ² Ã— C_in Ã— C_out multiplications per pixel
Separable: kÂ² Ã— C_in + C_in Ã— C_out multiplications per pixel

For k=3, C_in=256, C_out=256:
  Standard: 9 Ã— 256 Ã— 256 = 589,824
  Separable: 9 Ã— 256 + 256 Ã— 256 = 67,840
  
  ~8.7Ã— fewer operations!
```

---

## Filter Initialization

How filters are initialized affects training:

### Common Initialization Methods

| Method | Description | When to Use |
|--------|-------------|-------------|
| **Xavier/Glorot** | Scaled by fan_in + fan_out | Sigmoid, Tanh |
| **He/Kaiming** | Scaled by fan_in | ReLU, Leaky ReLU |
| **Random Normal** | Small random values | General purpose |

### Why Initialization Matters

- **Too small**: Gradients vanish, slow learning
- **Too large**: Gradients explode, unstable training
- **Just right**: Maintains variance through layers

---

## Visualizing What Filters Learn

### Techniques

1. **Direct Visualization**: Plot filter weights as images (works for first layer)
2. **Activation Maximization**: Generate input that maximally activates a filter
3. **Gradient-based Methods**: See which input regions affect the filter most
4. **Feature Map Visualization**: Show output of each filter for a given input

### Interpreting First Layer Filters

```
For a network trained on natural images:

Filter 1: Responds to horizontal edges
Filter 2: Responds to vertical edges
Filter 3: Responds to diagonal edges (45Â°)
Filter 4: Responds to diagonal edges (135Â°)
Filter 5: Responds to red color
Filter 6: Responds to green color
...
```

---

## Key Takeaways

1. **Filters are learnable**: CNNs discover useful patterns automatically
2. **Small filters are efficient**: 3Ã—3 is the sweet spot
3. **Hierarchical features**: Early layers detect simple patterns, deep layers detect complex ones
4. **1Ã—1 convolutions**: Powerful for channel manipulation
5. **Depthwise separable**: Trade-off between accuracy and efficiency

---

*Next: [03-Padding-and-Strides.md](03-Padding-and-Strides.md) â€” Control output dimensions*
