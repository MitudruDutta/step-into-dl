{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 锔 L2 Regularization (Weight Decay)\n",
    "\n",
    "This notebook demonstrates **L2 Regularization** (also known as Weight Decay)a technique that penalizes large weights to prevent overfitting. We'll compare models trained with and without L2 regularization on the Sonar dataset.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How L2 regularization works mathematically\n",
    "2. Using `weight_decay` in PyTorch optimizers\n",
    "3. The effect of L2 on training dynamics\n",
    "4. Choosing the right regularization strength\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding L2 Regularization\n",
    "\n",
    "### The Math Behind L2\n",
    "\n",
    "L2 regularization adds a penalty term to the loss function:\n",
    "\n",
    "```\n",
    "Total Loss = Original Loss + 位  危(w虏)\n",
    "\n",
    "Where:\n",
    "- Original Loss: CrossEntropy, MSE, etc.\n",
    "- 位 (lambda): Regularization strength (weight_decay in PyTorch)\n",
    "- 危(w虏): Sum of squared weights\n",
    "```\n",
    "\n",
    "### Why It Works\n",
    "\n",
    "By penalizing large weights:\n",
    "- Model can't rely too heavily on any single feature\n",
    "- Weights are distributed more evenly\n",
    "- Simpler, more generalizable patterns are learned\n",
    "\n",
    "### Effect on Weights\n",
    "\n",
    "```\n",
    "Without L2: weights = [2.5, -3.1, 0.8, 4.2, -1.9]  (some very large)\n",
    "With L2:    weights = [1.2, -1.5, 0.4, 2.0, -0.9]  (smaller, more uniform)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the Sonar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sonar.all-data\", header=None)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df[60].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numeric\n",
    "df[60] = df[60].map({'M': 0, 'R': 1})\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(60, axis=1)\n",
    "y = df[60]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"Simple neural network for binary classification.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(60, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=40):\n",
    "    \"\"\"\n",
    "    Train model and track metrics.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, val_losses, val_accuracies\n",
    "    \"\"\"\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_pred, y_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracy = accuracy_score(y_true, y_pred)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_losses[-1]:.4f} | \"\n",
    "                  f\"Val Loss: {val_losses[-1]:.4f} | \"\n",
    "                  f\"Val Acc: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training Without Regularization\n",
    "\n",
    "First, let's train a model **without any regularization** to establish a baseline and observe overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model WITHOUT regularization\n",
    "model_without_reg = NeuralNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Note: weight_decay=0 means NO L2 regularization\n",
    "optimizer = optim.Adam(model_without_reg.parameters(), lr=0.001, weight_decay=0)\n",
    "\n",
    "print(\"Training WITHOUT L2 Regularization (weight_decay=0)\")\n",
    "print(\"=\" * 60)\n",
    "train_losses_no_reg, val_losses_no_reg, val_accuracies_no_reg = train_model(\n",
    "    model_without_reg, train_loader, test_loader, criterion, optimizer, epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training without regularization\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs = range(1, 41)\n",
    "\n",
    "plt.plot(epochs, train_losses_no_reg, 'b-', label=\"Train Loss\", linewidth=2)\n",
    "plt.plot(epochs, val_losses_no_reg, 'r-', label=\"Validation Loss\", linewidth=2)\n",
    "plt.plot(epochs, val_accuracies_no_reg, 'g--', label=\"Validation Accuracy\", linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss / Accuracy', fontsize=12)\n",
    "plt.title('Training WITHOUT L2 Regularization', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Results\n",
    "\n",
    "Look for signs of **overfitting**:\n",
    "- Training loss keeps decreasing toward zero\n",
    "- Validation loss may increase or plateau\n",
    "- Growing gap between training and validation loss\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training With L2 Regularization\n",
    "\n",
    "Now let's add L2 regularization using the `weight_decay` parameter in the optimizer.\n",
    "\n",
    "### Using weight_decay in PyTorch\n",
    "\n",
    "```python\n",
    "# L2 regularization is built into the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.04)\n",
    "```\n",
    "\n",
    "The `weight_decay` parameter adds L2 penalty automatically during optimization.\n",
    "\n",
    "### Choosing weight_decay Value\n",
    "\n",
    "| Value | Effect |\n",
    "|-------|--------|\n",
    "| 0 | No regularization |\n",
    "| 1e-5 to 1e-4 | Light regularization |\n",
    "| 1e-4 to 1e-3 | Moderate regularization |\n",
    "| 1e-3 to 1e-2 | Strong regularization |\n",
    "| > 0.01 | Very strong (may underfit) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model WITH L2 regularization\n",
    "model_with_l2 = NeuralNet()\n",
    "\n",
    "# weight_decay=0.04 adds L2 regularization\n",
    "optimizer_l2 = optim.Adam(model_with_l2.parameters(), lr=0.001, weight_decay=0.04)\n",
    "\n",
    "print(\"\\nTraining WITH L2 Regularization (weight_decay=0.04)\")\n",
    "print(\"=\" * 60)\n",
    "train_losses_with_l2, val_losses_with_l2, val_accuracies_with_l2 = train_model(\n",
    "    model_with_l2, train_loader, test_loader, criterion, optimizer_l2, epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training with L2 regularization\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs = range(1, 41)\n",
    "\n",
    "plt.plot(epochs, train_losses_with_l2, 'b-', label=\"Train Loss\", linewidth=2)\n",
    "plt.plot(epochs, val_losses_with_l2, 'r-', label=\"Validation Loss\", linewidth=2)\n",
    "plt.plot(epochs, val_accuracies_with_l2, 'g--', label=\"Validation Accuracy\", linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss / Accuracy', fontsize=12)\n",
    "plt.title('Training WITH L2 Regularization (weight_decay=0.04)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Comparing Both Models\n",
    "\n",
    "Let's compare the training dynamics and final performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "epochs = range(1, 41)\n",
    "\n",
    "# Training Loss Comparison\n",
    "axes[0].plot(epochs, train_losses_no_reg, 'b-', label=\"No Regularization\", linewidth=2)\n",
    "axes[0].plot(epochs, train_losses_with_l2, 'r-', label=\"With L2 (位=0.04)\", linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Comparison', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss Comparison\n",
    "axes[1].plot(epochs, val_losses_no_reg, 'b-', label=\"No Regularization\", linewidth=2)\n",
    "axes[1].plot(epochs, val_losses_with_l2, 'r-', label=\"With L2 (位=0.04)\", linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Validation Loss', fontsize=12)\n",
    "axes[1].set_title('Validation Loss Comparison', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Accuracy Comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs, val_accuracies_no_reg, 'b-o', label=\"No Regularization\", \n",
    "         linewidth=2, markersize=4)\n",
    "plt.plot(epochs, val_accuracies_with_l2, 'r-o', label=\"With L2 (位=0.04)\", \n",
    "         linewidth=2, markersize=4)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "plt.title('Validation Accuracy: L2 Regularization Effect', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Examining Weight Magnitudes\n",
    "\n",
    "Let's compare the weight magnitudes of both models to see L2's effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_stats(model):\n",
    "    \"\"\"Get statistics about model weights.\"\"\"\n",
    "    all_weights = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            all_weights.extend(param.detach().cpu().numpy().flatten())\n",
    "    \n",
    "    weights = np.array(all_weights)\n",
    "    return {\n",
    "        'mean': np.mean(np.abs(weights)),\n",
    "        'max': np.max(np.abs(weights)),\n",
    "        'std': np.std(weights),\n",
    "        'weights': weights\n",
    "    }\n",
    "\n",
    "stats_no_reg = get_weight_stats(model_without_reg)\n",
    "stats_with_l2 = get_weight_stats(model_with_l2)\n",
    "\n",
    "print(\"Weight Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nWithout L2 Regularization:\")\n",
    "print(f\"  Mean |weight|: {stats_no_reg['mean']:.4f}\")\n",
    "print(f\"  Max |weight|:  {stats_no_reg['max']:.4f}\")\n",
    "print(f\"  Std:           {stats_no_reg['std']:.4f}\")\n",
    "\n",
    "print(f\"\\nWith L2 Regularization:\")\n",
    "print(f\"  Mean |weight|: {stats_with_l2['mean']:.4f}\")\n",
    "print(f\"  Max |weight|:  {stats_with_l2['max']:.4f}\")\n",
    "print(f\"  Std:           {stats_with_l2['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(stats_no_reg['weights'], bins=50, alpha=0.7, color='blue')\n",
    "axes[0].set_xlabel('Weight Value', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Weight Distribution (No Regularization)', fontsize=14)\n",
    "axes[0].axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "axes[1].hist(stats_with_l2['weights'], bins=50, alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Weight Value', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Weight Distribution (With L2)', fontsize=14)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Notice how L2 regularization produces smaller, more concentrated weights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nWithout L2 Regularization:\")\n",
    "print(f\"  Best Val Accuracy: {max(val_accuracies_no_reg):.4f}\")\n",
    "print(f\"  Final Val Accuracy: {val_accuracies_no_reg[-1]:.4f}\")\n",
    "print(f\"  Final Train Loss: {train_losses_no_reg[-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {val_losses_no_reg[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nWith L2 Regularization (位=0.04):\")\n",
    "print(f\"  Best Val Accuracy: {max(val_accuracies_with_l2):.4f}\")\n",
    "print(f\"  Final Val Accuracy: {val_accuracies_with_l2[-1]:.4f}\")\n",
    "print(f\"  Final Train Loss: {train_losses_with_l2[-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {val_losses_with_l2[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Key Takeaways\n",
    "\n",
    "### What L2 Regularization Does\n",
    "\n",
    "| Effect | Description |\n",
    "|--------|-------------|\n",
    "| **Shrinks weights** | Pushes weights toward zero |\n",
    "| **Prevents overfitting** | Model can't memorize training data |\n",
    "| **Smoother decision boundaries** | Less complex model |\n",
    "| **Better generalization** | Works better on unseen data |\n",
    "\n",
    "### PyTorch Implementation\n",
    "\n",
    "```python\n",
    "# Method 1: Built into optimizer (recommended)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Method 2: Use AdamW for proper weight decay with Adam\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "```\n",
    "\n",
    "### Choosing weight_decay\n",
    "\n",
    "1. **Start with 1e-4** as a default\n",
    "2. **Increase if overfitting** persists\n",
    "3. **Decrease if underfitting** (training loss too high)\n",
    "4. **Use AdamW** instead of Adam for better results\n",
    "\n",
    "### When to Use L2 Regularization\n",
    "\n",
    "- Almost always! It's a safe default\n",
    "- Especially with small datasets\n",
    "- When model has many parameters\n",
    "- Combine with dropout for best results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
