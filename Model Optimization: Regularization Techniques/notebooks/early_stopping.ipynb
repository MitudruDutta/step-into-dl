{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â±ï¸ Early Stopping: Knowing When to Stop Training\n",
    "\n",
    "This notebook demonstrates **Early Stopping**â€”a simple yet effective regularization technique that stops training when the model starts to overfit. We'll compare training with and without early stopping on the Sonar dataset.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Why training too long leads to overfitting\n",
    "2. How early stopping works\n",
    "3. Implementing early stopping with patience\n",
    "4. Saving and restoring the best model weights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Sonar Dataset\n",
    "\n",
    "The **Sonar dataset** is ideal for demonstrating early stopping because:\n",
    "- Small size (208 samples) makes overfitting likely\n",
    "- High dimensionality (60 features) increases model complexity\n",
    "- Binary classification task is easy to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sonar.all-data\", header=None)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df[60].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numeric\n",
    "df[60] = df[60].map({'M': 0, 'R': 1})\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(60, axis=1)\n",
    "y = df[60]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model Definition\n",
    "\n",
    "We'll use a simple feedforward network for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Simple neural network for binary classification.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(60, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Without Early Stopping\n",
    "\n",
    "First, let's train for a fixed number of epochs (20) and observe what happens.\n",
    "\n",
    "### The Overfitting Problem\n",
    "\n",
    "```\n",
    "Epoch 1-5:   Model learns useful patterns (both losses decrease)\n",
    "Epoch 6-10:  Model reaches optimal point (validation loss minimum)\n",
    "Epoch 11-20: Model overfits (train loss â†“, validation loss â†‘)\n",
    "```\n",
    "\n",
    "Without early stopping, we train past the optimal point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    \"\"\"Standard training without early stopping.\"\"\"\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_pred, y_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracy = accuracy_score(y_true, y_pred)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f} | \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f} | \"\n",
    "              f\"Val Acc: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train without early stopping\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training WITHOUT Early Stopping (20 epochs)\")\n",
    "print(\"=\" * 60)\n",
    "train_losses, val_losses, val_accuracies = train_model(\n",
    "    model, train_loader, test_loader, criterion, optimizer, epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs_range = range(1, 21)\n",
    "\n",
    "plt.plot(epochs_range, val_accuracies, 'g-o', label=\"Validation Accuracy\", linewidth=2)\n",
    "plt.axhline(y=max(val_accuracies), color='r', linestyle='--', \n",
    "            label=f\"Best Accuracy: {max(val_accuracies):.4f}\")\n",
    "\n",
    "# Mark the best epoch\n",
    "best_epoch = val_accuracies.index(max(val_accuracies)) + 1\n",
    "plt.axvline(x=best_epoch, color='orange', linestyle='--', \n",
    "            label=f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "plt.title('Training Without Early Stopping', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation accuracy was at epoch {best_epoch}\")\n",
    "print(f\"But we trained until epoch 20!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Training With Early Stopping\n",
    "\n",
    "Now let's implement early stopping to automatically stop at the optimal point.\n",
    "\n",
    "### How Early Stopping Works\n",
    "\n",
    "```\n",
    "1. Track validation metric (accuracy or loss)\n",
    "2. If metric improves:\n",
    "   - Save model weights (checkpoint)\n",
    "   - Reset patience counter\n",
    "3. If metric doesn't improve:\n",
    "   - Increment patience counter\n",
    "4. If patience exceeded:\n",
    "   - Stop training\n",
    "   - Restore best weights\n",
    "```\n",
    "\n",
    "### Key Parameter: Patience\n",
    "\n",
    "**Patience** = number of epochs to wait for improvement before stopping\n",
    "\n",
    "- `patience=3`: Stop quickly, may miss better solutions\n",
    "- `patience=10`: More exploration, longer training\n",
    "- `patience=5-7`: Good balance for most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_early_stop(model, train_loader, val_loader, criterion, optimizer, \n",
    "                           epochs=20, patience=3):\n",
    "    \"\"\"\n",
    "    Training with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        patience: Number of epochs to wait for improvement\n",
    "    \"\"\"\n",
    "    best_accuracy = 0\n",
    "    counter = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        y_pred, y_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracy = accuracy_score(y_true, y_pred)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f} | \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f} | \"\n",
    "              f\"Val Acc: {val_accuracy:.4f}\", end=\"\")\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\" â† Best model saved!\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f\" (patience: {counter}/{patience})\")\n",
    "            if counter >= patience:\n",
    "                print(f\"\\nðŸ›‘ Early Stopping triggered at epoch {epoch+1}!\")\n",
    "                print(f\"   Best accuracy was {best_accuracy:.4f} at epoch {best_epoch}\")\n",
    "                break\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with early stopping\n",
    "model_es = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_es.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training WITH Early Stopping (patience=3)\")\n",
    "print(\"=\" * 60)\n",
    "train_losses_es, val_losses_es, val_accuracies_es, best_epoch = train_model_early_stop(\n",
    "    model_es, train_loader, test_loader, criterion, optimizer, \n",
    "    epochs=20, patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize early stopping\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs_range = range(1, len(val_accuracies_es) + 1)\n",
    "\n",
    "plt.plot(epochs_range, val_accuracies_es, 'g-o', label=\"Validation Accuracy\", linewidth=2)\n",
    "plt.axvline(x=best_epoch, color='red', linestyle='--', linewidth=2,\n",
    "            label=f\"Best Epoch: {best_epoch}\")\n",
    "plt.axvline(x=len(val_accuracies_es), color='orange', linestyle='--', linewidth=2,\n",
    "            label=f\"Stopped at Epoch: {len(val_accuracies_es)}\")\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "plt.title('Training WITH Early Stopping', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Restoring the Best Model\n",
    "\n",
    "Early stopping saved the best model weights. Let's load them and verify the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights\n",
    "model_es.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "print(\"âœ“ Loaded best model weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the restored model\n",
    "model_es.eval()\n",
    "y_pred, y_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_es(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "final_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nFinal Test Accuracy (Best Model): {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nWithout Early Stopping:\")\n",
    "print(f\"  Epochs trained: 20\")\n",
    "print(f\"  Best accuracy: {max(val_accuracies):.4f} (at epoch {val_accuracies.index(max(val_accuracies))+1})\")\n",
    "print(f\"  Final accuracy: {val_accuracies[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nWith Early Stopping (patience=3):\")\n",
    "print(f\"  Epochs trained: {len(val_accuracies_es)}\")\n",
    "print(f\"  Best accuracy: {max(val_accuracies_es):.4f} (at epoch {best_epoch})\")\n",
    "print(f\"  Final accuracy (restored): {final_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Early stopping saved {20 - len(val_accuracies_es)} epochs of training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways\n",
    "\n",
    "### Benefits of Early Stopping\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Prevents overfitting** | Stops before model memorizes training data |\n",
    "| **Saves computation** | No wasted epochs after optimal point |\n",
    "| **Simple to implement** | Just track validation metric |\n",
    "| **No hyperparameters to tune** | Only patience (easy to set) |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always save checkpoints** of the best model\n",
    "2. **Use patience of 5-10** for most cases\n",
    "3. **Monitor validation loss OR accuracy** (not training metrics)\n",
    "4. **Restore best weights** after stopping\n",
    "5. **Combine with other regularization** (dropout, L2) for best results\n",
    "\n",
    "### When to Use Early Stopping\n",
    "\n",
    "- Almost always! It's a safe, effective technique\n",
    "- Especially important for small datasets\n",
    "- When training time is a concern\n",
    "- When you're not sure how many epochs to train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
