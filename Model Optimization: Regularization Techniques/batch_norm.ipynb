{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Batch Normalization: Stabilizing Deep Network Training\n",
    "\n",
    "This notebook demonstrates the impact of **Batch Normalization** on neural network training. We'll train two identical networks on MNISTâ€”one with BatchNorm and one withoutâ€”and compare their performance.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How Batch Normalization affects training dynamics\n",
    "2. Implementing BatchNorm in PyTorch\n",
    "3. Comparing convergence speed with and without BatchNorm\n",
    "4. When and why to use Batch Normalization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We import the necessary libraries for building and training neural networks with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Device Configuration\n",
    "\n",
    "We check for GPU availability to accelerate training. CUDA-enabled GPUs significantly speed up neural network computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the MNIST Dataset\n",
    "\n",
    "**MNIST** is a classic dataset of handwritten digits (0-9), containing:\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "- 28Ã—28 grayscale images\n",
    "\n",
    "We normalize the pixel values to have mean=0.5 and std=0.5, which helps with training stability.\n",
    "\n",
    "For faster experimentation, we use a subset of the data (5,000 training, 1,000 test samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images to [-1, 1] range\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load full datasets\n",
    "full_train_dataset = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "full_test_dataset = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Create smaller subsets for faster training\n",
    "train_subset = Subset(full_train_dataset, torch.arange(5000))\n",
    "test_subset = Subset(full_test_dataset, torch.arange(1000))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=60, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=60, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_subset)}\")\n",
    "print(f\"Test samples: {len(test_subset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model Without Batch Normalization\n",
    "\n",
    "First, let's define a simple feedforward neural network **without** Batch Normalization.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input (28Ã—28 = 784) â†’ Hidden (128) â†’ ReLU â†’ Hidden (64) â†’ ReLU â†’ Output (10)\n",
    "```\n",
    "\n",
    "This is a standard MLP with two hidden layers. Without BatchNorm, the network must learn to handle varying input distributions at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    \"\"\"Simple MLP without Batch Normalization.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 128),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Function\n",
    "\n",
    "We define a reusable training function that:\n",
    "1. Trains the model for a specified number of epochs\n",
    "2. Evaluates on the test set after each epoch\n",
    "3. Returns validation accuracies for comparison\n",
    "\n",
    "### Training Loop Steps\n",
    "\n",
    "1. **Forward pass**: Compute predictions\n",
    "2. **Compute loss**: Compare predictions to true labels\n",
    "3. **Backward pass**: Compute gradients\n",
    "4. **Update weights**: Apply gradients via optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, optimizer, criterion, epochs=5):\n",
    "    \"\"\"\n",
    "    Train a model and track validation accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network to train\n",
    "        train_loader: DataLoader for training data\n",
    "        test_loader: DataLoader for test/validation data\n",
    "        optimizer: Optimization algorithm\n",
    "        criterion: Loss function\n",
    "        epochs: Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        List of validation accuracies per epoch\n",
    "    \"\"\"\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for batch, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()              # Clear gradients\n",
    "            outputs = model(images)            # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()                    # Backpropagate\n",
    "            optimizer.step()                   # Update weights\n",
    "            \n",
    "            if batch % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch: {batch}, Train Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        y_pred, y_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_true, y_pred)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Epoch {epoch+1} - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model Without BatchNorm\n",
    "\n",
    "Let's train our baseline model without Batch Normalization for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model, loss function, and optimizer\n",
    "model = MNISTClassifier().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training WITHOUT Batch Normalization\")\n",
    "print(\"=\" * 50)\n",
    "val_accuracies_no_bn = train_model(model, train_loader, test_loader, optimizer, criterion, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValidation Accuracies (No BatchNorm):\")\n",
    "print(val_accuracies_no_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Model With Batch Normalization\n",
    "\n",
    "Now let's create the same architecture but **with Batch Normalization** layers.\n",
    "\n",
    "### What BatchNorm Does\n",
    "\n",
    "For each mini-batch, BatchNorm:\n",
    "1. Computes the mean and variance of activations\n",
    "2. Normalizes to zero mean and unit variance\n",
    "3. Applies learnable scale (Î³) and shift (Î²) parameters\n",
    "\n",
    "```\n",
    "BatchNorm(x) = Î³ Ã— (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²\n",
    "```\n",
    "\n",
    "### Architecture with BatchNorm\n",
    "\n",
    "```\n",
    "Input â†’ Linear(128) â†’ BatchNorm â†’ ReLU â†’ Linear(64) â†’ BatchNorm â†’ ReLU â†’ Output(10)\n",
    "```\n",
    "\n",
    "We place `BatchNorm1d` after each linear layer, before the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifierWithBN(nn.Module):\n",
    "    \"\"\"MLP with Batch Normalization after each hidden layer.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.BatchNorm1d(128),    # BatchNorm after first linear layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),     # BatchNorm after second linear layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)       # No BatchNorm before output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model With BatchNorm\n",
    "\n",
    "Now let's train the BatchNorm model with the same hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with BatchNorm\n",
    "model_with_bn = MNISTClassifierWithBN().to(device)\n",
    "criterion_with_bn = nn.CrossEntropyLoss()\n",
    "optimizer_with_bn = optim.Adam(model_with_bn.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training WITH Batch Normalization\")\n",
    "print(\"=\" * 50)\n",
    "val_accuracies_with_bn = train_model(\n",
    "    model_with_bn, train_loader, test_loader, \n",
    "    optimizer_with_bn, criterion_with_bn, epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValidation Accuracies (With BatchNorm):\")\n",
    "print(val_accuracies_with_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Comparing Results\n",
    "\n",
    "Let's visualize the training progress of both models to see the impact of Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "epochs = range(1, len(val_accuracies_no_bn) + 1)\n",
    "\n",
    "plt.plot(epochs, val_accuracies_no_bn, 'b-o', label=\"Without BatchNorm\", linewidth=2)\n",
    "plt.plot(epochs, val_accuracies_with_bn, 'r-o', label=\"With BatchNorm\", linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "plt.title('Batch Normalization: Impact on Training', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analysis\n",
    "\n",
    "### Expected Observations\n",
    "\n",
    "| Aspect | Without BatchNorm | With BatchNorm |\n",
    "|--------|-------------------|----------------|\n",
    "| **Early epochs** | Slower improvement | Faster improvement |\n",
    "| **Final accuracy** | Good | Often slightly better |\n",
    "| **Training stability** | May fluctuate | More stable |\n",
    "\n",
    "### Why BatchNorm Helps\n",
    "\n",
    "1. **Reduces internal covariate shift**: Each layer receives inputs with consistent statistics\n",
    "2. **Allows higher learning rates**: Normalized activations are more stable\n",
    "3. **Acts as regularization**: Batch statistics add noise, reducing overfitting\n",
    "4. **Reduces sensitivity to initialization**: Less dependent on careful weight initialization\n",
    "\n",
    "### When to Use BatchNorm\n",
    "\n",
    "- Deep networks (many layers)\n",
    "- When training is unstable\n",
    "- When you want to use higher learning rates\n",
    "- CNNs (use `BatchNorm2d`)\n",
    "\n",
    "### When BatchNorm May Not Help\n",
    "\n",
    "- Very small batch sizes (noisy statistics)\n",
    "- RNNs/Transformers (use LayerNorm instead)\n",
    "- When batch statistics don't make sense (e.g., online learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Key Takeaways\n",
    "\n",
    "1. **Batch Normalization normalizes layer inputs** using batch statistics\n",
    "2. **Faster convergence**: Models with BatchNorm often train faster\n",
    "3. **More stable training**: Reduces sensitivity to hyperparameters\n",
    "4. **Implicit regularization**: Adds noise through batch statistics\n",
    "5. **Remember `model.eval()`**: BatchNorm behaves differently at inference time\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different learning rates with and without BatchNorm\n",
    "- Experiment with `BatchNorm2d` for CNNs\n",
    "- Compare with LayerNorm for sequence models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
