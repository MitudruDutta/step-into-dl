{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≤ Dropout Regularization: Preventing Overfitting\n",
    "\n",
    "This notebook demonstrates **Dropout Regularization**‚Äîa powerful technique that randomly deactivates neurons during training to prevent overfitting. We'll train two models on the Sonar dataset and compare their performance.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How dropout prevents overfitting\n",
    "2. Implementing dropout in PyTorch\n",
    "3. Comparing models with and without dropout\n",
    "4. Interpreting training vs validation loss curves\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Sonar Dataset\n",
    "\n",
    "The **Sonar dataset** is a classic binary classification problem:\n",
    "- **Task**: Distinguish between sonar signals bounced off a metal cylinder (Mine) vs a rock\n",
    "- **Features**: 60 numerical attributes (sonar frequencies)\n",
    "- **Samples**: 208 total\n",
    "- **Classes**: 'M' (Mine) and 'R' (Rock)\n",
    "\n",
    "This is a challenging dataset because:\n",
    "- Small sample size (easy to overfit)\n",
    "- High-dimensional features (60 features for 208 samples)\n",
    "\n",
    "**Perfect for demonstrating dropout's effectiveness!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sonar.all-data\", header=None)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df[60].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "We need to:\n",
    "1. Convert class labels ('M', 'R') to numeric (0, 1)\n",
    "2. Split into features (X) and target (y)\n",
    "3. Create train/test split\n",
    "4. Convert to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels: M (Mine) = 0, R (Rock) = 1\n",
    "df[60] = df[60].map({'M': 0, 'R': 1})\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(60, axis=1)\n",
    "y = df[60]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (75% train, 25% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=1\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model Without Dropout\n",
    "\n",
    "First, let's create a neural network **without any dropout**. This model is prone to overfitting, especially on small datasets like Sonar.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input (60) ‚Üí Hidden (128) ‚Üí ReLU ‚Üí Hidden (64) ‚Üí ReLU ‚Üí Output (2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Neural network WITHOUT dropout - prone to overfitting.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(60, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)  # Output: 2 classes (Mine, Rock)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Function\n",
    "\n",
    "We define a training function that tracks:\n",
    "- **Training loss**: How well the model fits training data\n",
    "- **Validation loss**: How well the model generalizes\n",
    "- **Validation accuracy**: Classification performance\n",
    "\n",
    "**Key insight**: When training loss decreases but validation loss increases, the model is overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    \"\"\"\n",
    "    Train model and track metrics.\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, val_losses, val_accuracies\n",
    "    \"\"\"\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()  # Enable dropout\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Disable dropout\n",
    "        val_loss = 0.0\n",
    "        y_pred, y_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracy = accuracy_score(y_true, y_pred)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f} | \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f} | \"\n",
    "              f\"Val Acc: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model Without Dropout\n",
    "\n",
    "Let's train the model without dropout and observe the overfitting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model without dropout\n",
    "model_without_dropout = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_without_dropout.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training WITHOUT Dropout\")\n",
    "print(\"=\" * 60)\n",
    "train_losses_no_dropout, val_losses_no_dropout, val_accuracies_no_dropout = train_model(\n",
    "    model_without_dropout, train_loader, test_loader, criterion, optimizer, epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs = range(1, 21)\n",
    "\n",
    "plt.plot(epochs, train_losses_no_dropout, 'b-', label=\"Train Loss\", linewidth=2)\n",
    "plt.plot(epochs, val_losses_no_dropout, 'r-', label=\"Validation Loss\", linewidth=2)\n",
    "plt.plot(epochs, val_accuracies_no_dropout, 'g--', label=\"Validation Accuracy\", linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss / Accuracy', fontsize=12)\n",
    "plt.title('Training WITHOUT Dropout (Overfitting Expected)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Results\n",
    "\n",
    "Look for signs of **overfitting**:\n",
    "- Training loss keeps decreasing\n",
    "- Validation loss starts increasing (or plateaus while train loss drops)\n",
    "- Gap between training and validation loss grows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model With Dropout\n",
    "\n",
    "Now let's add **Dropout layers** to prevent overfitting.\n",
    "\n",
    "### How Dropout Works\n",
    "\n",
    "During training, dropout randomly \"drops\" (sets to zero) a fraction of neurons:\n",
    "\n",
    "```\n",
    "Training:   [‚óè][‚óã][‚óè][‚óè][‚óã][‚óè]  (‚óã = dropped)\n",
    "Inference:  [‚óè][‚óè][‚óè][‚óè][‚óè][‚óè]  (all active)\n",
    "```\n",
    "\n",
    "This forces the network to:\n",
    "- Not rely on any single neuron\n",
    "- Learn redundant representations\n",
    "- Generalize better\n",
    "\n",
    "### Architecture with Dropout\n",
    "\n",
    "```\n",
    "Input (60) ‚Üí Hidden (128) ‚Üí ReLU ‚Üí Dropout(0.5) ‚Üí Hidden (64) ‚Üí ReLU ‚Üí Dropout(0.5) ‚Üí Output (2)\n",
    "```\n",
    "\n",
    "We use `p=0.5` (50% dropout rate), which is a common choice for hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNWithDropout(nn.Module):\n",
    "    \"\"\"Neural network WITH dropout - better generalization.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(60, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),  # 50% dropout after first hidden layer\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),  # 50% dropout after second hidden layer\n",
    "            nn.Linear(64, 2)    # No dropout before output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with dropout\n",
    "model_with_dropout = SimpleNNWithDropout()\n",
    "optimizer = optim.Adam(model_with_dropout.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nTraining WITH Dropout (p=0.5)\")\n",
    "print(\"=\" * 60)\n",
    "train_losses_with_dropout, val_losses_with_dropout, val_accuracies_with_dropout = train_model(\n",
    "    model_with_dropout, train_loader, test_loader, criterion, optimizer, epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress with dropout\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs = range(1, 21)\n",
    "\n",
    "plt.plot(epochs, train_losses_with_dropout, 'b-', label=\"Train Loss\", linewidth=2)\n",
    "plt.plot(epochs, val_losses_with_dropout, 'r-', label=\"Validation Loss\", linewidth=2)\n",
    "plt.plot(epochs, val_accuracies_with_dropout, 'g--', label=\"Validation Accuracy\", linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss / Accuracy', fontsize=12)\n",
    "plt.title('Training WITH Dropout (Better Generalization)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Comparing Both Models\n",
    "\n",
    "Let's compare the validation performance of both models side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "epochs = range(1, 21)\n",
    "\n",
    "# Validation Loss Comparison\n",
    "axes[0].plot(epochs, val_losses_no_dropout, 'r-o', label=\"Without Dropout\", linewidth=2)\n",
    "axes[0].plot(epochs, val_losses_with_dropout, 'b-o', label=\"With Dropout\", linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Validation Loss', fontsize=12)\n",
    "axes[0].set_title('Validation Loss Comparison', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy Comparison\n",
    "axes[1].plot(epochs, val_accuracies_no_dropout, 'r-o', label=\"Without Dropout\", linewidth=2)\n",
    "axes[1].plot(epochs, val_accuracies_with_dropout, 'b-o', label=\"With Dropout\", linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Validation Accuracy', fontsize=12)\n",
    "axes[1].set_title('Validation Accuracy Comparison', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nWithout Dropout:\")\n",
    "print(f\"  Best Val Accuracy: {max(val_accuracies_no_dropout):.4f}\")\n",
    "print(f\"  Final Val Accuracy: {val_accuracies_no_dropout[-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {val_losses_no_dropout[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nWith Dropout:\")\n",
    "print(f\"  Best Val Accuracy: {max(val_accuracies_with_dropout):.4f}\")\n",
    "print(f\"  Final Val Accuracy: {val_accuracies_with_dropout[-1]:.4f}\")\n",
    "print(f\"  Final Val Loss: {val_losses_with_dropout[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Key Takeaways\n",
    "\n",
    "### What We Observed\n",
    "\n",
    "| Aspect | Without Dropout | With Dropout |\n",
    "|--------|-----------------|---------------|\n",
    "| **Training loss** | Decreases rapidly | Decreases more slowly |\n",
    "| **Validation loss** | May increase (overfit) | More stable |\n",
    "| **Generalization** | Poor on small datasets | Better |\n",
    "\n",
    "### When to Use Dropout\n",
    "\n",
    "- Small datasets (like Sonar)\n",
    "- Deep networks with many parameters\n",
    "- When you see overfitting (train loss << val loss)\n",
    "- Fully connected layers (use Dropout2d for CNNs)\n",
    "\n",
    "### Dropout Best Practices\n",
    "\n",
    "1. **Start with p=0.5** for hidden layers\n",
    "2. **Use lower rates (0.1-0.2)** for input layers\n",
    "3. **Never use dropout on output layer**\n",
    "4. **Always call `model.eval()`** before inference\n",
    "5. **Combine with other regularization** (L2, early stopping) for best results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
