{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# üß† Activation Functions in Neural Networks\n",
    "\n",
    "This notebook demonstrates the implementation and behavior of common activation functions used in deep learning. Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How each activation function transforms input values\n",
    "- The mathematical formula behind each function\n",
    "- When to use each activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-explanation",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use NumPy for efficient numerical computations. NumPy's vectorized operations allow us to apply activation functions to entire arrays at once."
   ]
  },
  {
   "cell_type": "code",
   "id": "0d4cbab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T17:21:30.132463Z",
     "start_time": "2025-12-18T17:21:30.062681Z"
    }
   },
   "source": "import numpy as np",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "4c2d0af2e547a952",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Sigmoid Function\n",
    "\n",
    "**Formula:** œÉ(x) = 1 / (1 + e‚ÅªÀ£)\n",
    "\n",
    "**Output Range:** 0 to 1\n",
    "\n",
    "The sigmoid function squashes any input value into a range between 0 and 1, making it ideal for:\n",
    "- **Binary classification** output layers (predicting probabilities)\n",
    "- Interpreting outputs as probabilities\n",
    "\n",
    "**Characteristics:**\n",
    "- Smooth, S-shaped curve\n",
    "- Output of 0.5 when input is 0\n",
    "- Saturates (flattens) for very large or very small inputs, which can cause vanishing gradients\n",
    "\n",
    "**‚ö†Ô∏è Numerical Stability Note:** The simple implementation below works well for typical input ranges. However, for extreme inputs (e.g., x < -700 or x > 700), `np.exp(-x)` can overflow or underflow. Production code often uses numerically-stable variants like conditional formulations or input clipping. See the stable version in the cell below the basic implementation."
   ]
  },
  {
   "cell_type": "code",
   "id": "317fb8f27888876a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T17:22:18.812102Z",
     "start_time": "2025-12-18T17:22:18.809606Z"
    }
   },
   "source": [
    "# Basic sigmoid implementation (works for typical input ranges)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "values = np.array([-2, -1, 0, 1, 2])\n",
    "sigmoid_values = sigmoid(values)\n",
    "print(\"Sigmoid Function Results:\")\n",
    "print(sigmoid_values)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Function Results:\n",
      "[0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "sigmoid-stable",
   "metadata": {},
   "source": [
    "# Numerically stable sigmoid using np.where\n",
    "# This avoids overflow for large negative values and underflow for large positive values\n",
    "def sigmoid_stable(x):\n",
    "    return np.where(\n",
    "        x >= 0,\n",
    "        1 / (1 + np.exp(-x)),      # For positive x: standard formula\n",
    "        np.exp(x) / (1 + np.exp(x)) # For negative x: equivalent but stable\n",
    "    )\n",
    "\n",
    "# Test with extreme values\n",
    "extreme_values = np.array([-1000, -100, 0, 100, 1000])\n",
    "print(\"Stable Sigmoid with extreme inputs:\")\n",
    "print(sigmoid_stable(extreme_values))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "sigmoid-interpretation",
   "metadata": {},
   "source": [
    "**Interpreting the results:**\n",
    "- Input `-2` ‚Üí Output `0.12` (close to 0, low probability)\n",
    "- Input `0` ‚Üí Output `0.5` (exactly in the middle)\n",
    "- Input `2` ‚Üí Output `0.88` (close to 1, high probability)\n",
    "\n",
    "Notice how negative inputs map to values below 0.5, and positive inputs map to values above 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115dbc3fa1ae7663",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Softmax Function\n",
    "\n",
    "**Formula:** softmax(x·µ¢) = eÀ£‚Å± / Œ£eÀ£ ≤\n",
    "\n",
    "**Output Range:** 0 to 1 (all outputs sum to 1)\n",
    "\n",
    "Softmax converts a vector of raw scores (logits) into a probability distribution. It's the go-to choice for:\n",
    "- **Multi-class classification** output layers\n",
    "- When you need outputs to represent mutually exclusive class probabilities\n",
    "\n",
    "**Characteristics:**\n",
    "- All outputs are positive and sum to exactly 1\n",
    "- Larger inputs get exponentially larger probabilities\n",
    "- We subtract `max(x)` for numerical stability to prevent overflow\n",
    "\n",
    "**Implementation Note:** The version below handles both 1D vectors and batched 2D+ inputs by using `axis=-1` and `keepdims=True` for proper broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "id": "9fd11ebf12fb2b2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T17:23:05.735484Z",
     "start_time": "2025-12-18T17:23:05.732831Z"
    }
   },
   "source": [
    "def softmax(x):\n",
    "    # Subtract max for numerical stability (prevents overflow)\n",
    "    # axis=-1 and keepdims=True ensure this works for both 1D and batched inputs\n",
    "    x_max = np.max(x, axis=-1, keepdims=True)\n",
    "    e_x = np.exp(x - x_max)\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# Single vector example\n",
    "values = np.array([2.0, 1.0, 0.1])\n",
    "softmax_values = softmax(values)\n",
    "print(\"Softmax Function Results (1D):\")\n",
    "print(softmax_values)\n",
    "print(f\"Sum: {softmax_values.sum():.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Softmax Function Results:\n",
      "[0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "softmax-batched",
   "metadata": {},
   "source": [
    "# Batched example: multiple samples at once\n",
    "batch_values = np.array([\n",
    "    [2.0, 1.0, 0.1],   # Sample 1\n",
    "    [1.0, 2.0, 3.0],   # Sample 2\n",
    "    [0.5, 0.5, 0.5]    # Sample 3 (equal logits)\n",
    "])\n",
    "batch_softmax = softmax(batch_values)\n",
    "print(\"\\nSoftmax Function Results (Batched 2D):\")\n",
    "print(batch_softmax)\n",
    "print(f\"\\nRow sums (should all be 1.0): {batch_softmax.sum(axis=-1)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "softmax-interpretation",
   "metadata": {},
   "source": [
    "**Interpreting the results:**\n",
    "- Input `[2.0, 1.0, 0.1]` represents raw scores for 3 classes\n",
    "- Class 0 (score 2.0) ‚Üí 65.9% probability (highest score = highest probability)\n",
    "- Class 1 (score 1.0) ‚Üí 24.2% probability\n",
    "- Class 2 (score 0.1) ‚Üí 9.9% probability\n",
    "- **Sum: 0.659 + 0.242 + 0.099 = 1.0** ‚úì\n",
    "\n",
    "This is perfect for tasks like image classification where an image belongs to exactly one category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c1e1d7eb0751a4",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Tanh (Hyperbolic Tangent) Function\n",
    "\n",
    "**Formula:** tanh(x) = (eÀ£ - e‚ÅªÀ£) / (eÀ£ + e‚ÅªÀ£)\n",
    "\n",
    "**Output Range:** -1 to 1\n",
    "\n",
    "Tanh is similar to sigmoid but outputs values centered around zero. This makes it useful for:\n",
    "- **Hidden layers** where zero-centered outputs improve training\n",
    "- RNNs and LSTMs where values need to flow in both directions\n",
    "\n",
    "**Characteristics:**\n",
    "- Zero-centered (output is 0 when input is 0)\n",
    "- Stronger gradients than sigmoid (steeper curve)\n",
    "- Still suffers from vanishing gradients at extreme values"
   ]
  },
  {
   "cell_type": "code",
   "id": "f082dcbd172caa7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T17:23:27.021770Z",
     "start_time": "2025-12-18T17:23:27.019683Z"
    }
   },
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "values = np.array([-2, -1, 0, 1, 2])\n",
    "tanh_values = tanh(values)\n",
    "print(\"\\nTanh Function Results:\")\n",
    "print(tanh_values)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tanh Function Results:\n",
      "[-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "tanh-interpretation",
   "metadata": {},
   "source": [
    "**Interpreting the results:**\n",
    "- Input `-2` ‚Üí Output `-0.96` (close to -1)\n",
    "- Input `0` ‚Üí Output `0` (exactly zero-centered)\n",
    "- Input `2` ‚Üí Output `0.96` (close to 1)\n",
    "\n",
    "**Comparison with Sigmoid:**\n",
    "- Tanh outputs are symmetric around 0 (-1 to 1)\n",
    "- Sigmoid outputs are always positive (0 to 1)\n",
    "- Tanh is essentially a scaled and shifted sigmoid: tanh(x) = 2 √ó sigmoid(2x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0c33c0a4f588",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ReLU (Rectified Linear Unit) Function\n",
    "\n",
    "**Formula:** ReLU(x) = max(0, x)\n",
    "\n",
    "**Output Range:** 0 to ‚àû\n",
    "\n",
    "ReLU is the most widely used activation function in modern deep learning. It's the **default choice for hidden layers** because:\n",
    "- Computationally efficient (simple comparison operation)\n",
    "- Reduces vanishing gradient problem (gradient is 1 for positive inputs)\n",
    "- Promotes sparsity (many neurons output exactly 0)\n",
    "\n",
    "**Characteristics:**\n",
    "- Outputs 0 for all negative inputs\n",
    "- Outputs the input unchanged for positive values\n",
    "- Can suffer from \"dying ReLU\" where neurons get stuck outputting 0"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4bac17845cbc1d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T17:26:09.437459Z",
     "start_time": "2025-12-18T17:26:09.434917Z"
    }
   },
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "values = np.array([-2, -1, 0, 1, 2])\n",
    "relu_values = relu(values)\n",
    "print(\"\\nReLU Function Results:\")\n",
    "print(relu_values)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ReLU Function Results:\n",
      "[0 0 0 1 2]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "relu-interpretation",
   "metadata": {},
   "source": [
    "**Interpreting the results:**\n",
    "- Input `-2` ‚Üí Output `0` (negative values become 0)\n",
    "- Input `-1` ‚Üí Output `0` (negative values become 0)\n",
    "- Input `0` ‚Üí Output `0` (boundary case)\n",
    "- Input `1` ‚Üí Output `1` (positive values pass through unchanged)\n",
    "- Input `2` ‚Üí Output `2` (positive values pass through unchanged)\n",
    "\n",
    "**Why ReLU is so popular:**\n",
    "1. **Speed**: Just a comparison, no exponentials to compute\n",
    "2. **Gradient flow**: Gradient is 1 for positive inputs, preventing vanishing gradients\n",
    "3. **Sparsity**: Many neurons output 0, making the network more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Choosing the Right Activation Function\n",
    "\n",
    "| Layer Type | Recommended Activation | Reason |\n",
    "|------------|----------------------|--------|\n",
    "| Hidden layers | **ReLU** | Fast, reduces vanishing gradients |\n",
    "| Binary classification output | **Sigmoid** | Outputs probability (0-1) |\n",
    "| Multi-class classification output | **Softmax** | Outputs probability distribution |\n",
    "| RNN/LSTM hidden layers | **Tanh** | Zero-centered, works well with sequences |\n",
    "| Regression output | **Linear (none)** | Allows any output value |\n",
    "\n",
    "**Pro tip:** When in doubt, start with ReLU for hidden layers. Only switch to alternatives like Leaky ReLU or ELU if you encounter training issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
