{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Hyperparameter Tuning with Optuna\n",
    "\n",
    "This notebook demonstrates **Bayesian Optimization** using Optuna.\n",
    "\n",
    "## Why Optuna?\n",
    "\n",
    "| Method | Efficiency |\n",
    "|--------|------------|\n",
    "| Grid Search | Low |\n",
    "| Random Search | Medium |\n",
    "| **Optuna** | High |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training: {X_train.shape[0]} samples, Validation: {X_val.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample input:\", X_train[0][:5], \"...\")\n",
    "print(f\"Label: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
    "print(f\"Shapes: X={X_train_t.shape}, y={y_train_t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "test_model = SimpleNN(20, 64)\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Define the Optuna Objective Function\n",
    "\n",
    "| Parameter | Type | Range |\n",
    "|-----------|------|-------|\n",
    "| learning_rate | Log-uniform | 1e-4 to 1e-1 |\n",
    "| hidden_dim | Integer | 16 to 128 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Objective function for Optuna optimization.\n",
    "    \n",
    "    Note: Training for 20 epochs during search for speed.\n",
    "    Final model will train for 50 epochs (see section 8).\n",
    "    \"\"\"\n",
    "    lr = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 16, 128)\n",
    "    \n",
    "    model = SimpleNN(20, hidden_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train for 20 epochs (fast evaluation during search)\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        for bx, by in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(bx), by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_val_t).argmax(1)\n",
    "        acc = (preds == y_val_t).float().mean().item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Run Optuna Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best accuracy: {study.best_value:.4f}\")\n",
    "print(f\"Best params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in study.trials:\n",
    "    best = '*' if t.number == study.best_trial.number else ''\n",
    "    print(f\"Trial {t.number}: lr={t.params['learning_rate']:.6f}, hidden={t.params['hidden_dim']}, acc={t.value:.4f} {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    optuna.visualization.plot_optimization_history(study).show()\n",
    "except: print('Install plotly: pip install plotly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Train Final Model with Extended Training and Validation Monitoring\n",
    "\n",
    "### Important: Epoch Mismatch and Mitigation\n",
    "\n",
    "**The Issue:**\n",
    "- Hyperparameter search: 20 epochs (fast evaluation)\n",
    "- Final training: 50 epochs (better convergence)\n",
    "\n",
    "**Why This Mismatch?**\n",
    "- Searching with 20 epochs is faster; final model gets more training for better performance\n",
    "- Hyperparameters tuned for 20 epochs might not be optimal for 50 epochs\n",
    "\n",
    "**How We Mitigate This Risk:**\n",
    "1. **Periodic validation** every 10 epochs to monitor generalization\n",
    "2. **Track improvement** - if validation accuracy stops improving, model may be overfitting\n",
    "3. **Early stopping indicators** - watch for increasing \"No improve\" count\n",
    "\n",
    "**What to Look For:**\n",
    "- If validation accuracy plateaus or decreases ‚Üí overfitting detected\n",
    "- If \"No improve\" count is high ‚Üí consider reducing epochs or adding regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = SimpleNN(20, study.best_params['hidden_dim'])\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=study.best_params['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=32)\n",
    "\n",
    "print(f\"Training final model with best hyperparameters:\")\n",
    "print(f\"  Learning rate: {study.best_params['learning_rate']:.6f}\")\n",
    "print(f\"  Hidden dim: {study.best_params['hidden_dim']}\")\n",
    "print(f\"\\n{'Epoch':<8} {'Train Loss':<12} {'Val Accuracy':<15} {'Status':<20}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "best_val_acc = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Training phase\n",
    "    final_model.train()\n",
    "    train_loss = 0\n",
    "    for bx, by in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(final_model(bx), by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation every 10 epochs (or at final epoch)\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 49:\n",
    "        final_model.eval()\n",
    "        val_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for bx, by in val_loader:\n",
    "                val_acc += (final_model(bx).argmax(1) == by).float().sum().item()\n",
    "        val_acc /= len(y_val_t)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            status = \"‚úì Improved\"\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            status = f\"No improve ({epochs_no_improve})\"\n",
    "        \n",
    "        print(f\"{epoch+1:<8} {train_loss:<12.4f} {val_acc:<15.4f} {status:<20}\")\n",
    "\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"Final validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if epochs_no_improve == 0:\n",
    "    print(\"‚úì Model improved throughout training - good generalization\")\n",
    "elif epochs_no_improve == 1:\n",
    "    print(\"‚ö† Model plateaued in last 10 epochs - normal behavior\")\n",
    "else:\n",
    "    print(f\"‚ö† Model hasn't improved for {epochs_no_improve*10} epochs - possible overfitting\")\n",
    "    print(\"  Consider: reducing epochs, adding dropout, or increasing L2 regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Key Takeaways\n",
    "\n",
    "1. **Optuna uses Bayesian optimization** to intelligently search hyperparameter space\n",
    "2. **Use `log=True` for learning rate** to explore all orders of magnitude equally\n",
    "3. **20+ trials** usually finds good hyperparameters\n",
    "4. **Validate during final training** to detect overfitting and ensure hyperparameters generalize\n",
    "5. **Monitor improvement** - if validation accuracy plateaus, consider early stopping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
