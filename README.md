# ğŸš€ Step Into Deep Learning

A hands-on learning repository for understanding deep learning fundamentals from the ground up. This project provides comprehensive documentation and practical code examples to help you master neural networks and modern AI concepts.

---

## ğŸ“ Repository Structure

```
step-into-dl/
â”œâ”€â”€ Getting Started/
â”‚   â””â”€â”€ README.md          # Deep learning foundations & overview
â”œâ”€â”€ Neural Networks: Basics/
â”‚   â”œâ”€â”€ README.md          # Neurons & activation functions theory
â”‚   â””â”€â”€ functions.ipynb    # Activation function implementations
â”œâ”€â”€ Pytorch/
â”‚   â”œâ”€â”€ README.md          # Matrices, tensors & calculus fundamentals
â”‚   â”œâ”€â”€ tensor1.ipynb      # Tensor operations, matrix math, GPU acceleration
â”‚   â”œâ”€â”€ tensor2.ipynb      # Tensor attributes, reshaping, initialization
â”‚   â””â”€â”€ autograd.ipynb     # Automatic differentiation & gradients
â””â”€â”€ README.md              # You are here
```

---

## ğŸ“š What's Covered

### 1. Getting Started
A comprehensive introduction to deep learning covering:
- Neural network architecture (input, hidden, output layers)
- Deep Learning vs. Statistical ML decision matrix
- Popular architectures (FNN, CNN, RNN, Transformers) and their use cases
- Developer toolkit: PyTorch, TensorFlow, GPUs/TPUs
- Training fundamentals: loss functions, backpropagation, optimizers
- Common challenges: overfitting, underfitting, vanishing gradients
- Evaluation metrics for classification and regression
- Best practices and learning resources

### 2. Neural Networks: Basics
Deep dive into the building blocks of neural networks:
- What is a neuron and how it processes information
- Evolution from Perceptrons to Multilayer Perceptrons (MLPs)
- Intuitive examples (insurance prediction model)
- Comprehensive guide to activation functions:
  - Sigmoid, Softmax, Tanh, ReLU, Leaky ReLU
  - When to use each function
  - Mathematical formulas and characteristics

### 3. PyTorch Fundamentals
Introduction to PyTorch and the math behind deep learning:
- Matrix fundamentals and why they matter for AI
- Tensor basics: dimensions, attributes, and operations
- Calculus for learning: derivatives, chain rule, and gradients
- Autograd: automatic differentiation in PyTorch
- PyTorch tensors vs. NumPy arrays
- Common tensor operations reference

### 4. Practical Implementations
Jupyter notebooks with working code and detailed explanations:

| Notebook | Topics |
|----------|--------|
| `functions.ipynb` | Sigmoid, Softmax, Tanh, ReLU implementations with NumPy |
| `tensor1.ipynb` | Tensor creation, arithmetic, matrix multiplication, GPU acceleration |
| `tensor2.ipynb` | Shape, dtype, device attributes, reshaping, initialization |
| `autograd.ipynb` | Gradient tracking, backward(), chain rule, torch.no_grad() |

---

## ğŸ› ï¸ Prerequisites

- Python 3.8+
- NumPy
- PyTorch
- Jupyter Notebook (for running `.ipynb` files)

```bash
pip install numpy torch jupyter
```

---

## ğŸ¯ Learning Path

1. **Start here** â†’ `Getting Started/README.md` for foundational concepts
2. **Go deeper** â†’ `Neural Networks: Basics/README.md` for neuron mechanics
3. **Practice activations** â†’ `Neural Networks: Basics/functions.ipynb`
4. **Learn PyTorch** â†’ `Pytorch/README.md` for tensors and calculus
5. **Tensor operations** â†’ `Pytorch/tensor1.ipynb` and `tensor2.ipynb`
6. **Master autograd** â†’ `Pytorch/autograd.ipynb` for automatic differentiation

---

## ğŸ“– Recommended Resources

- **Courses**: fast.ai, Coursera Deep Learning Specialization, CodeBasics Deep Learning
- **Books**: *Deep Learning* by Goodfellow et al., *Hands-On Machine Learning* by GÃ©ron
- **Practice**: Kaggle, Google Colab, Hugging Face

---

## ğŸ¤ Contributing

Feel free to open issues or submit PRs to improve the documentation or add new topics.

---

*Happy learning! Start small, experiment often, and don't be afraid to break things.* ğŸ§ 
