# ğŸ§  Step Into Deep Learning

A structured, hands-on learning repository for mastering deep learning fundamentals. From neurons to optimizers, this project provides comprehensive documentation and practical PyTorch implementations.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ¯ What You'll Learn

| Module | Topics | Difficulty |
|--------|--------|------------|
| Getting Started | DL foundations, architectures, toolkit | â­ Beginner |
| Neural Networks: Basics | Neurons, perceptrons, activation functions | â­ Beginner |
| PyTorch Fundamentals | Tensors, autograd, GPU computing | â­â­ Intermediate |
| Neural Network Training | Backprop, gradient descent, optimizers | â­â­ Intermediate |
| Neural Networks in PyTorch | nn.Module, DataLoaders, loss functions | â­â­ Intermediate |
| Model Optimization: Training Algorithms | Momentum, RMSProp, Adam | â­â­â­ Advanced |
| Model Optimization: Regularization | Dropout, L1/L2, BatchNorm, Early Stopping | â­â­â­ Advanced |

---

## ğŸ“ Repository Structure

```
step-into-dl/
â”‚
â”œâ”€â”€ ğŸ“˜ Getting Started/
â”‚   â”œâ”€â”€ README.md                      # Module index
â”‚   â”œâ”€â”€ 01-Neural-Networks-Foundation.md
â”‚   â”œâ”€â”€ 02-DL-vs-Statistical-ML.md
â”‚   â”œâ”€â”€ 03-NN-Architectures.md
â”‚   â”œâ”€â”€ 04-Developer-Toolkit.md
â”‚   â”œâ”€â”€ 05-Training-Fundamentals.md
â”‚   â”œâ”€â”€ 06-Common-Challenges.md
â”‚   â”œâ”€â”€ 07-Evaluation-Metrics.md
â”‚   â”œâ”€â”€ 08-Best-Practices.md
â”‚   â”œâ”€â”€ 09-Learning-Resources.md
â”‚   â””â”€â”€ 10-Glossary.md
â”‚
â”œâ”€â”€ ğŸ”¬ Neural Networks: Basics/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01-What-is-a-Neuron.md
â”‚   â”œâ”€â”€ 02-Perceptrons-to-MLPs.md
â”‚   â”œâ”€â”€ 03-Insurance-Prediction-Intuition.md
â”‚   â”œâ”€â”€ 04-Role-of-Activation-Functions.md
â”‚   â”œâ”€â”€ 05-Activation-Functions-Guide.md
â”‚   â”œâ”€â”€ 06-Practical-Tips.md
â”‚   â””â”€â”€ functions.ipynb                # ğŸ““ Activation implementations
â”‚
â”œâ”€â”€ ğŸ”¥ Pytorch/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01-Matrix-Fundamentals.md
â”‚   â”œâ”€â”€ 02-Tensor-Basics.md
â”‚   â”œâ”€â”€ 03-Calculus-for-Learning.md
â”‚   â”œâ”€â”€ 04-Autograd-Explained.md
â”‚   â”œâ”€â”€ 05-Tensors-vs-NumPy.md
â”‚   â”œâ”€â”€ 06-Common-Operations.md
â”‚   â”œâ”€â”€ 07-Best-Practices.md
â”‚   â”œâ”€â”€ tensor1.ipynb                  # ğŸ““ Tensor operations & GPU
â”‚   â”œâ”€â”€ tensor2.ipynb                  # ğŸ““ Reshaping & initialization
â”‚   â””â”€â”€ autograd.ipynb                 # ğŸ““ Automatic differentiation
â”‚
â”œâ”€â”€ ğŸ“ˆ Neural Network: Training/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01-Backpropagation.md
â”‚   â”œâ”€â”€ 02-Gradient-Descent.md
â”‚   â”œâ”€â”€ 03-GD-Variants.md
â”‚   â”œâ”€â”€ 04-Optimizers.md
â”‚   â”œâ”€â”€ 05-Monitoring-Training.md
â”‚   â”œâ”€â”€ data_generation.ipynb          # ğŸ““ Synthetic data creation
â”‚   â”œâ”€â”€ gradient_descent.ipynb         # ğŸ““ GD from scratch
â”‚   â””â”€â”€ gd_vs_mini_gd_vs_sgd.ipynb     # ğŸ““ GD variants comparison
â”‚
â”œâ”€â”€ âš¡ Neural Networks: Pytorch/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01-nn-Module.md
â”‚   â”œâ”€â”€ 02-Datasets-DataLoaders.md
â”‚   â”œâ”€â”€ 03-Binary-Cross-Entropy.md
â”‚   â”œâ”€â”€ 04-Categorical-Cross-Entropy.md
â”‚   â”œâ”€â”€ 05-Training-Loop.md
â”‚   â”œâ”€â”€ log_loss.ipynb                 # ğŸ““ MSE vs BCE
â”‚   â”œâ”€â”€ cross_entropy_loss.ipynb       # ğŸ““ Multi-class loss
â”‚   â”œâ”€â”€ dataset_dataloader.ipynb       # ğŸ““ Data pipelines
â”‚   â””â”€â”€ handwritten_digits.ipynb       # ğŸ““ MNIST classifier
â”‚
â”œâ”€â”€ ğŸš€ Model Optimization: Training Algorithms/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01-What-is-Model-Optimization.md
â”‚   â”œâ”€â”€ 02-EWMA-Foundation.md
â”‚   â”œâ”€â”€ 03-Momentum.md
â”‚   â”œâ”€â”€ 04-RMSProp.md
â”‚   â”œâ”€â”€ 05-Adam.md
â”‚   â”œâ”€â”€ 06-Optimizer-Comparison.md
â”‚   â””â”€â”€ optimizers.ipynb               # ğŸ““ Optimizer comparison
â”‚
â”œâ”€â”€ ğŸ›¡ï¸ Model Optimization: Regularization Techniques/
â”‚   â”œâ”€â”€ README.md                      # Module overview and learning path
â”‚   â”œâ”€â”€ 01-Understanding-Regularization.md  # Overfitting and bias-variance
â”‚   â”œâ”€â”€ 02-Dropout.md                  # Dropout regularization
â”‚   â”œâ”€â”€ 03-L1-L2-Regularization.md     # Weight penalties and decay
â”‚   â”œâ”€â”€ 04-Batch-Normalization.md      # Normalizing layer inputs
â”‚   â”œâ”€â”€ 05-Early-Stopping.md           # Optimal stopping point
â”‚   â”œâ”€â”€ 06-Data-Augmentation.md        # Expanding training data
â”‚   â”œâ”€â”€ dropout_regularization.ipynb   # ğŸ““ Dropout comparison
â”‚   â”œâ”€â”€ l2_regularization.ipynb        # ğŸ““ Weight decay demo
â”‚   â”œâ”€â”€ batch_norm.ipynb               # ğŸ““ BatchNorm on MNIST
â”‚   â””â”€â”€ early_stopping.ipynb           # ğŸ““ Early stopping implementation
â”‚
â””â”€â”€ README.md                          # You are here
```

---

## ğŸ›¤ï¸ Learning Path

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Getting        â”‚     â”‚  Neural Nets    â”‚     â”‚    PyTorch      â”‚
â”‚  Started        â”‚ â”€â”€â–º â”‚  Basics         â”‚ â”€â”€â–º â”‚  Fundamentals   â”‚
â”‚  (Theory)       â”‚     â”‚  (Neurons)      â”‚     â”‚  (Tensors)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NN Training    â”‚     â”‚  NNs in         â”‚     â”‚    Model        â”‚
â”‚  (Backprop,     â”‚ â”€â”€â–º â”‚  PyTorch        â”‚ â”€â”€â–º â”‚  Optimization   â”‚
â”‚   GD)           â”‚     â”‚  (nn.Module)    â”‚     â”‚  (Adam, etc.)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quick Start

1. **New to Deep Learning?** â†’ Start with `Getting Started/01-Neural-Networks-Foundation.md`
2. **Know the basics?** â†’ Jump to `Neural Network: Training/` for hands-on practice
3. **Ready to build?** â†’ Go to `Neural Networks: Pytorch/handwritten_digits.ipynb`

---

## ğŸ““ Notebooks Overview

| Notebook | Module | What You'll Build |
|----------|--------|-------------------|
| `functions.ipynb` | Basics | Sigmoid, Softmax, Tanh, ReLU from scratch |
| `tensor1.ipynb` | PyTorch | Tensor ops, matrix multiplication, GPU usage |
| `tensor2.ipynb` | PyTorch | Reshaping, broadcasting, initialization |
| `autograd.ipynb` | PyTorch | Gradient computation, computational graphs |
| `gradient_descent.ipynb` | Training | GD optimizer from scratch |
| `gd_vs_mini_gd_vs_sgd.ipynb` | Training | Compare Batch/Mini-Batch/SGD |
| `log_loss.ipynb` | PyTorch NN | MSE vs BCE for classification |
| `cross_entropy_loss.ipynb` | PyTorch NN | Multi-class classification loss |
| `dataset_dataloader.ipynb` | PyTorch NN | FashionMNIST data pipeline |
| `handwritten_digits.ipynb` | PyTorch NN | Complete MNIST classifier |
| `optimizers.ipynb` | Optimization | SGD vs Momentum vs Adam |
| `dropout_regularization.ipynb` | Regularization | Dropout effect on Sonar dataset |
| `l2_regularization.ipynb` | Regularization | Weight decay and weight distributions |
| `batch_norm.ipynb` | Regularization | BatchNorm impact on MNIST training |
| `early_stopping.ipynb` | Regularization | Patience-based stopping with checkpoints |

---

## ğŸ› ï¸ Setup

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (optional, for faster training)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/step-into-dl.git
cd step-into-dl

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install torch torchvision numpy pandas matplotlib jupyter scikit-learn
```

### Verify Installation

```python
import torch
print(f"PyTorch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
```

---

## ğŸ“š Module Details

### 1. Getting Started
Foundational concepts for understanding deep learning:
- Neural network architecture and information flow
- When to use DL vs. traditional ML
- Popular architectures: FNN, CNN, RNN, Transformers
- Developer toolkit: PyTorch, TensorFlow, hardware options
- Common challenges: overfitting, vanishing gradients

### 2. Neural Networks: Basics
The building blocks of neural networks:
- Biological inspiration and artificial neurons
- From Perceptrons to Multi-Layer Perceptrons
- Activation functions: why non-linearity matters
- Comprehensive guide with formulas and use cases

### 3. PyTorch Fundamentals
Essential PyTorch skills:
- Tensors: creation, operations, GPU acceleration
- Autograd: automatic differentiation explained
- NumPy interoperability and best practices

### 4. Neural Network Training
How networks learn:
- Backpropagation: the chain rule in action
- Gradient Descent variants: Batch, Mini-Batch, SGD
- Monitoring training: loss curves, debugging tips

### 5. Neural Networks in PyTorch
Building real models:
- `nn.Module`: the foundation of PyTorch models
- Datasets and DataLoaders for efficient training
- Loss functions: BCE, Cross Entropy, when to use each

### 6. Model Optimization: Training Algorithms
Advanced training techniques:
- EWMA: the math behind modern optimizers
- Momentum: accelerating convergence
- RMSProp: adaptive learning rates
- Adam: the gold standard optimizer

### 7. Model Optimization: Regularization Techniques
Preventing overfitting:
- Dropout: randomly deactivating neurons
- L1/L2 regularization: weight penalties
- Batch Normalization: stabilizing training
- Early Stopping: knowing when to stop
- Data Augmentation: expanding training data

---

## ğŸ“– Recommended Resources

### Courses
- [fast.ai](https://www.fast.ai/) â€” Practical deep learning
- [Coursera Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) â€” Andrew Ng
- [CodeBasics Deep Learning](https://www.youtube.com/playlist?list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO) â€” YouTube series

### Books
- *Deep Learning* by Goodfellow, Bengio, Courville
- *Hands-On Machine Learning* by AurÃ©lien GÃ©ron
- *PyTorch Documentation* â€” [pytorch.org/docs](https://pytorch.org/docs)

### Practice
- [Kaggle](https://www.kaggle.com/) â€” Competitions and datasets
- [Google Colab](https://colab.research.google.com/) â€” Free GPU notebooks
- [Hugging Face](https://huggingface.co/) â€” Pre-trained models

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Open issues for bugs or suggestions
- Submit PRs to improve documentation
- Add new topics or notebooks

---

## ğŸ“„ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

---

<p align="center">
  <i>Start small, experiment often, and don't be afraid to break things.</i> ğŸš€
</p>
