# ðŸ“ Word Embeddings: The Semantic Foundation

Word embeddings are the fundamental building blocks that allow neural networks to understand and process human language. Before a Transformer or any NLP model can work with text, it must convert words into a numerical format that captures their semantic meaning.

---

## What Are Word Embeddings?

Word embeddings are **dense vector representations** of words where semantically similar words are mapped to nearby points in a high-dimensional vector space. Unlike simple one-hot encoding (where each word is independent), embeddings capture the _meaning_ and _relationships_ between words.

### The Problem with One-Hot Encoding

```text
Traditional One-Hot Encoding:

Vocabulary: [cat, dog, king, queen, man, woman]

cat   = [1, 0, 0, 0, 0, 0]
dog   = [0, 1, 0, 0, 0, 0]
king  = [0, 0, 1, 0, 0, 0]
queen = [0, 0, 0, 1, 0, 0]
man   = [0, 0, 0, 0, 1, 0]
woman = [0, 0, 0, 0, 0, 1]

Problems:
âŒ Every word is equally distant from every other word
âŒ cosine_similarity(cat, dog) = 0  (but they're both animals!)
âŒ cosine_similarity(king, queen) = 0  (but they're both royalty!)
âŒ Vectors are HUGE (vocabulary size can be 50,000+)
âŒ No semantic information captured
```

### The Embedding Solution

```text
Word Embeddings (dense vectors):

cat   = [0.2, -0.4, 0.7, 0.1]     â”
dog   = [0.3, -0.3, 0.6, 0.2]     â”œâ”€ Close in vector space (animals)
                                  â”˜
king  = [0.9, 0.1, -0.2, 0.8]    â”
queen = [0.8, 0.2, -0.1, 0.9]    â”œâ”€ Close in vector space (royalty)
                                  â”˜

Benefits:
âœ… Similar words have similar vectors
âœ… Compact representation (100-1024 dimensions vs 50,000+)
âœ… Semantic relationships are encoded
âœ… Enable mathematical operations on meaning
```

---

## Key Properties of Word Embeddings

### 1. Dimensionality

Embeddings typically have 100-1024 dimensions. Each dimension captures some abstract feature of meaning.

```text
Example: 300-dimensional vector for "king"

Dimension 1:  0.42  â†’ might relate to "royalty"
Dimension 2: -0.15  â†’ might relate to "gender"
Dimension 3:  0.88  â†’ might relate to "power"
...
Dimension 300: 0.23 â†’ some other abstract feature

Note: Dimensions are NOT explicitly labeledâ€”the network learns
      what each dimension represents during training.
```

### 2. Semantic Similarity

Words with similar meanings have vectors that are close together, measured by **cosine similarity**:

```text
Cosine Similarity Formula:

              A Â· B           Î£(aáµ¢ Ã— báµ¢)
cos(Î¸) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          ||A|| ||B||   âˆšÎ£(aáµ¢Â²) Ã— âˆšÎ£(báµ¢Â²)

Range: -1 to 1 (higher = more similar)

Example Similarities:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Word Pair          â”‚ Similarity    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ king, queen        â”‚ 0.85          â”‚
â”‚ cat, dog           â”‚ 0.76          â”‚
â”‚ happy, joyful      â”‚ 0.89          â”‚
â”‚ king, banana       â”‚ 0.12          â”‚
â”‚ running, walking   â”‚ 0.72          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Vector Arithmetic (Compositionality)

One of the most fascinating properties: semantic relationships are encoded as vector directions!

```text
The Famous Analogy:

    King - Man + Woman â‰ˆ Queen

Visualized:
                    Queen
                     â†‘
                     â”‚ Woman
                     â”‚
    King â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ?
         Man         â”‚
                     â”‚
                     â†“

The vector from "Man" to "King" represents "royalty + male"
Subtracting "Man" removes the male component
Adding "Woman" adds the female component
Result: "royalty + female" = Queen
```

### More Vector Arithmetic Examples

```text
Geographic Relationships:
    Russia - Moscow + Delhi â‰ˆ India
    (Country - Capital + Capital = Country)

    Japan - Tokyo + Paris â‰ˆ France

Verb Tenses:
    walking - walk + swim â‰ˆ swimming
    ran - run + fly â‰ˆ flew

Comparatives:
    bigger - big + small â‰ˆ smaller

Plurals:
    cats - cat + dog â‰ˆ dogs
```

---

## Static Embedding Techniques

### Word2Vec (Google, 2013)

Word2Vec introduced two revolutionary training architectures:

#### Skip-gram: Predict Context from Word

```text
Skip-gram Architecture:

Given center word â†’ Predict surrounding words

Sentence: "The quick brown fox jumps"
Window size: 2

Training pairs (center â†’ context):
  quick â†’ The
  quick â†’ brown
  brown â†’ quick
  brown â†’ fox
  fox â†’ brown
  fox â†’ jumps

Network:
         Center Word
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Input Layer   â”‚  (one-hot)
    â”‚    (V dims)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Hidden Layer   â”‚  (embedding)
    â”‚    (N dims)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Output Layer   â”‚  (softmax)
    â”‚    (V dims)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
      Context Word Probabilities
```

#### CBOW: Predict Word from Context

```text
CBOW (Continuous Bag of Words):

Given surrounding words â†’ Predict center word

Sentence: "The quick brown fox jumps"
Window size: 2

Training example:
  Context: [The, brown] â†’ Center: quick
  Context: [quick, fox] â†’ Center: brown

Network:
    Context Word 1    Context Word 2
          â†“                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Embed   â”‚     â”‚  Embed   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Average  â”‚
            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                 â†“
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Output  â”‚
           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                â†“
           Center Word Probabilities
```

### GloVe (Stanford, 2014)

**Global Vectors for Word Representation** combines the benefits of matrix factorization and local context methods.

```text
GloVe Key Insight:

Word relationships can be captured through co-occurrence ratios.

Example: Analyzing words related to "ice" vs "steam"

             Co-occurrence with:
             ice    steam
solid       high    low      â†’ ratio >> 1
gas         low     high     â†’ ratio << 1
water       high    high     â†’ ratio â‰ˆ 1
fashion     low     low      â†’ ratio â‰ˆ 1

GloVe learns embeddings such that:
  wáµ¢áµ€wâ±¼ + báµ¢ + bâ±¼ = log(Xáµ¢â±¼)

Where Xáµ¢â±¼ = co-occurrence count of word i and j
```

### Comparison: Word2Vec vs GloVe

| Aspect          | Word2Vec                    | GloVe                              |
| :-------------- | :-------------------------- | :--------------------------------- |
| **Approach**    | Predictive (neural network) | Count-based (matrix factorization) |
| **Context**     | Local (sliding window)      | Global (entire corpus statistics)  |
| **Training**    | Stochastic (online)         | Batch (full matrix)                |
| **Speed**       | Faster for small data       | Faster for large data              |
| **Performance** | Similar                     | Similar                            |

---

## The Limitation: Context Blindness

Static embeddings have a critical flaw: **polysemy** (words with multiple meanings).

```text
The Problem:

In static embeddings, "bank" has ONE vector, but:

Sentence 1: "I deposited money at the bank"
Sentence 2: "We had a picnic on the river bank"
Sentence 3: "Don't bank on it happening"

   bank (financial) â‰  bank (river) â‰  bank (rely on)
        â†“                 â†“               â†“
   Same vector!       Same vector!    Same vector!

The embedding averages all meanings, accurately representing NONE.
```

### Other Polysemy Examples

```text
"Apple":
  - "I ate an apple" (fruit)
  - "I bought an Apple" (company)

"Play":
  - "Let's play a game" (activity)
  - "I watched a play" (theater)
  - "Press play to start" (button)

"Spring":
  - "Spring is my favorite season" (time)
  - "The spring in the mattress broke" (coil)
  - "Water from the spring was cold" (water source)
```

---

## From Static to Contextual Embeddings

The limitations of static embeddings led to the development of **contextual embeddings** in Transformers:

```text
Static Embeddings (Word2Vec, GloVe):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "bank" â†’ [0.2, -0.3, 0.5, ...]     â”‚
â”‚                                     â”‚
â”‚ Same vector regardless of context   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Contextual Embeddings (BERT, GPT):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "I visited the bank to deposit money"               â”‚
â”‚ "bank" â†’ [0.8, 0.2, -0.1, ...]  (financial)        â”‚
â”‚                                                     â”‚
â”‚ "We walked along the river bank"                    â”‚
â”‚ "bank" â†’ [-0.2, 0.7, 0.3, ...]  (geographical)     â”‚
â”‚                                                     â”‚
â”‚ Different vectors based on context!                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How Transformers Create Contextual Embeddings

```text
Processing: "The bank was near the river"

Step 1: Start with static token embeddings
        bank â†’ [initial embedding]

Step 2: Self-attention reads the full context
        bank â† attends to â†’ [The, was, near, the, river]
                                              â†‘
                                         HIGH attention
                                         (context clue!)

Step 3: Update embedding based on attention
        bank â†’ [river-influenced embedding]

Result: "bank" representation is now river-specific!
```

---

## Embedding Dimensions in Practice

### Common Configurations

| Model               | Embedding Dim     | Vocabulary Size | Total Parameters       |
| :------------------ | :---------------- | :-------------- | :--------------------- |
| Word2Vec (original) | 300               | ~3M words       | 900M                   |
| GloVe (6B)          | 50, 100, 200, 300 | 400K            | 20M-120M               |
| BERT-Base           | 768               | 30,522          | 23M (embeddings only)  |
| BERT-Large          | 1024              | 30,522          | 31M (embeddings only)  |
| GPT-2               | 768               | 50,257          | 38M (embeddings only)  |
| GPT-3               | 12,288            | 50,257          | 617M (embeddings only) |

### Choosing Embedding Dimensions

```text
Guidelines:

Small (50-100 dims):
  âœ“ Fast training and inference
  âœ“ Good for small vocabularies
  âœ“ Simple tasks

Medium (200-300 dims):
  âœ“ Good balance of speed and quality
  âœ“ Standard for Word2Vec/GloVe
  âœ“ Most NLP tasks

Large (512-1024 dims):
  âœ“ Better semantic capture
  âœ“ Modern Transformers (BERT, GPT)
  âœ“ Complex tasks

Very Large (2048+ dims):
  âœ“ State-of-the-art LLMs
  âœ“ Maximum expressiveness
  âœ— Expensive to compute/store
```

---

## Using Pre-trained Embeddings in PyTorch

### Loading GloVe Embeddings

```python
import torch
import torch.nn as nn

def load_glove_embeddings(glove_path, word_to_idx, embedding_dim=300):
    """Load GloVe embeddings for your vocabulary."""

    # Initialize random embeddings
    vocab_size = len(word_to_idx)
    embeddings = torch.randn(vocab_size, embedding_dim)

    # Load GloVe vectors
    with open(glove_path, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            if word in word_to_idx:
                idx = word_to_idx[word]
                vector = torch.tensor([float(v) for v in values[1:]])
                embeddings[idx] = vector

    return embeddings

# Create embedding layer with pre-trained weights
pretrained_embeddings = load_glove_embeddings(
    'glove.6B.300d.txt',
    word_to_idx,
    embedding_dim=300
)

embedding_layer = nn.Embedding.from_pretrained(
    pretrained_embeddings,
    freeze=False  # Set True to keep embeddings fixed
)
```

### Simple Embedding Example

```python
import torch
import torch.nn as nn

# Create embedding layer
vocab_size = 10000
embedding_dim = 128
embedding = nn.Embedding(vocab_size, embedding_dim)

# Convert word indices to embeddings
word_indices = torch.tensor([42, 256, 1024])  # 3 word indices
word_vectors = embedding(word_indices)

print(f"Input shape: {word_indices.shape}")      # (3,)
print(f"Output shape: {word_vectors.shape}")     # (3, 128)

# Each word now has a 128-dimensional representation!
```

---

## Visualizing Embeddings with t-SNE

```python
import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

def visualize_embeddings(words, embeddings, word_to_idx):
    """Visualize word embeddings in 2D using t-SNE."""

    # Get embeddings for selected words
    indices = [word_to_idx[w] for w in words]
    vectors = embeddings[indices].detach().numpy()

    # Reduce to 2D with t-SNE
    tsne = TSNE(n_components=2, random_state=42, perplexity=5)
    vectors_2d = tsne.fit_transform(vectors)

    # Plot
    plt.figure(figsize=(12, 8))
    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.7)

    for i, word in enumerate(words):
        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]))

    plt.title("Word Embeddings Visualization")
    plt.show()

# Example usage
words = ['king', 'queen', 'man', 'woman', 'prince', 'princess',
         'dog', 'cat', 'puppy', 'kitten', 'car', 'truck', 'bus']
visualize_embeddings(words, embeddings, word_to_idx)
```

**Expected Result:**

```text
t-SNE Plot:

                    royalty cluster
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         queen â€¢    â”‚             â”‚    â€¢ princess
              king â€¢ â”‚             â”‚ â€¢ prince
                    â”‚    man â€¢    â”‚
                    â”‚  woman â€¢    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    dog â€¢  â€¢ cat                         â€¢ car
                                    â€¢ truck
  puppy â€¢    â€¢ kitten                    â€¢ bus
         â†‘                               â†‘
    animal cluster              vehicle cluster
```

---

## Summary

| Concept                  | Description                                                  |
| :----------------------- | :----------------------------------------------------------- |
| **Word Embeddings**      | Dense vector representations that capture semantic meaning   |
| **One-Hot Encoding**     | Sparse, high-dimensional, no semantic information            |
| **Dimensionality**       | Typically 100-1024 dimensions per word                       |
| **Semantic Similarity**  | Similar words have similar vectors (cosine similarity)       |
| **Vector Arithmetic**    | Relationships encoded as vector operations                   |
| **Word2Vec**             | Neural predictive model (Skip-gram, CBOW)                    |
| **GloVe**                | Count-based global co-occurrence statistics                  |
| **Static vs Contextual** | Static = one vector per word; Contextual = context-dependent |

---

## What's Next?

Now that you understand how words are represented numerically, let's explore the architecture that processes these embeddings:

âž¡ï¸ **Next:** [Architecture Overview: Encoder and Decoder](02-Architecture-Overview.md)

---

_Word embeddings were a breakthrough that enabled modern NLP. Understanding how meaning is encoded in vectors is essential for grasping how Transformers and LLMs work at a fundamental level._
