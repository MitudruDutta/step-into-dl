{
 "cells": [
  {"cell_type": "markdown", "id": "intro", "metadata": {}, "source": ["# ⚡ PyTorch Autograd: Automatic Differentiation\n\nAutograd is PyTorch's automatic differentiation engine that powers neural network training. It computes gradients automatically, eliminating manual calculus.\n\n**Topics:** Gradient tracking, backward pass, chain rule, torch.no_grad()"]},
  {"cell_type": "markdown", "id": "setup-md", "metadata": {}, "source": ["---\n## Setup"]},
  {"cell_type": "code", "id": "setup", "metadata": {}, "source": ["import torch"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "basics-md", "metadata": {}, "source": ["---\n## 1. Gradient Basics\n\nTo track gradients, set `requires_grad=True` when creating tensors. PyTorch builds a computation graph as you perform operations.\n\n**Business Example:** Model a cost function where:\n- Total Cost = 3 × (material_cost)² + 5 × (labor_cost) + 100\n\nWe want to know: *How does total cost change if we adjust material or labor costs?*"]},
  {"cell_type": "code", "id": "cost-fn", "metadata": {}, "source": ["# Create tensors with gradient tracking enabled\nmaterial_cost = torch.tensor(10.0, requires_grad=True)\nlabor_cost = torch.tensor(5.0, requires_grad=True)\n\n# Define the cost function\ntotal_cost = 3 * material_cost**2 + 5 * labor_cost + 100\ntotal_cost"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "backward-md", "metadata": {}, "source": ["### Computing Gradients with .backward()\n\nCalling `.backward()` computes gradients for all tensors with `requires_grad=True`.\n\n**Mathematical derivation:**\n- ∂(total_cost)/∂(material_cost) = 6 × material_cost = 6 × 10 = **60**\n- ∂(total_cost)/∂(labor_cost) = 5 (constant coefficient) = **5**"]},
  {"cell_type": "code", "id": "backward", "metadata": {}, "source": ["# Compute gradients via backpropagation\ntotal_cost.backward()\n\n# Access gradients via .grad attribute\nprint(f\"∂cost/∂material = {material_cost.grad}\")  # Should be 60\nprint(f\"∂cost/∂labor = {labor_cost.grad}\")        # Should be 5"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "interpret-md", "metadata": {}, "source": ["**Interpretation:**\n- Material cost gradient (60): A $1 increase in material cost raises total cost by $60\n- Labor cost gradient (5): A $1 increase in labor cost raises total cost by $5\n\nThis tells us material cost has 12x more impact on total cost than labor cost!"]},
  {"cell_type": "markdown", "id": "no-grad-md", "metadata": {}, "source": ["---\n## 2. Disabling Gradient Tracking\n\nUse `torch.no_grad()` when you don't need gradients (inference, evaluation). This:\n- Saves memory (no computation graph stored)\n- Speeds up computation\n- Is essential during model evaluation"]},
  {"cell_type": "code", "id": "no-grad", "metadata": {}, "source": ["x = torch.tensor(4.0, requires_grad=True)\n\n# Inside no_grad context, operations don't track gradients\nwith torch.no_grad():\n    y = x**2 + 5\n    print(f\"Inside no_grad - requires_grad: {y.requires_grad}\")\n\n# Outside no_grad, gradients are tracked again\nz = x**2 + 5\nprint(f\"Outside no_grad - requires_grad: {z.requires_grad}\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "matrix-md", "metadata": {}, "source": ["---\n## 3. Gradients with Matrices\n\nAutograd works seamlessly with multi-dimensional tensors. For non-scalar outputs, you must reduce to a scalar before calling `.backward()`.\n\n**Function:** y = 2x³ + 7\n\n**Derivative:** dy/dx = 6x²"]},
  {"cell_type": "code", "id": "matrix-grad", "metadata": {}, "source": ["x = torch.tensor([[1.0, 2.0], \n                  [3.0, 4.0]], requires_grad=True)\n\ny = 2 * x**3 + 7\nprint(\"y =\")\nprint(y)"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "sum-md", "metadata": {}, "source": ["### Reducing to Scalar\n\nSince `y` is a matrix, we sum it to get a scalar before backpropagation. This is equivalent to computing gradients for each element and summing them."]},
  {"cell_type": "code", "id": "sum-backward", "metadata": {}, "source": ["result = y.sum()\nresult.backward()"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "verify-md", "metadata": {}, "source": ["### Verify the Gradients\n\nFor y = 2x³ + 7, the derivative is dy/dx = 6x²\n\n| x | Expected gradient (6x²) |\n|---|------------------------|\n| 1 | 6 × 1² = 6 |\n| 2 | 6 × 2² = 24 |\n| 3 | 6 × 3² = 54 |\n| 4 | 6 × 4² = 96 |"]},
  {"cell_type": "code", "id": "verify", "metadata": {}, "source": ["print(\"Computed gradients:\")\nprint(x.grad)\nprint(\"\\nExpected: [[6, 24], [54, 96]]\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "chain-md", "metadata": {}, "source": ["---\n## 4. Chain Rule in Action\n\nAutograd automatically applies the chain rule for composite functions. This is the foundation of backpropagation in neural networks.\n\n**Example:** z = (x + y)² where x=2, y=3\n- Let u = x + y = 5\n- z = u² = 25\n- ∂z/∂x = ∂z/∂u × ∂u/∂x = 2u × 1 = 2(5) = 10"]},
  {"cell_type": "code", "id": "chain", "metadata": {}, "source": ["x = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(3.0, requires_grad=True)\n\nu = x + y\nz = u ** 2\n\nz.backward()\n\nprint(f\"z = {z.item()}\")\nprint(f\"∂z/∂x = {x.grad.item()}\")  # Should be 10\nprint(f\"∂z/∂y = {y.grad.item()}\")  # Should be 10"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "accumulate-md", "metadata": {}, "source": ["---\n## 5. Gradient Accumulation\n\n**Important:** Gradients accumulate by default! You must zero them before each backward pass in training loops."]},
  {"cell_type": "code", "id": "accumulate", "metadata": {}, "source": ["w = torch.tensor(1.0, requires_grad=True)\n\n# First backward\nloss1 = w * 2\nloss1.backward()\nprint(f\"After 1st backward: grad = {w.grad}\")\n\n# Second backward - gradients ACCUMULATE!\nloss2 = w * 3\nloss2.backward()\nprint(f\"After 2nd backward: grad = {w.grad} (accumulated: 2 + 3 = 5)\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "zero-md", "metadata": {}, "source": ["### Zeroing Gradients\n\nIn training loops, always zero gradients before each iteration:"]},
  {"cell_type": "code", "id": "zero", "metadata": {}, "source": ["w = torch.tensor(1.0, requires_grad=True)\n\nfor i in range(3):\n    # Zero gradients first!\n    if w.grad is not None:\n        w.grad.zero_()\n    \n    loss = w * (i + 1)\n    loss.backward()\n    print(f\"Iteration {i}: loss = {loss.item()}, grad = {w.grad.item()}\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "detach-md", "metadata": {}, "source": ["---\n## 6. Detaching Tensors\n\nUse `.detach()` to create a tensor that shares data but doesn't track gradients. Useful for:\n- Freezing parts of a model\n- Using tensor values without affecting the computation graph"]},
  {"cell_type": "code", "id": "detach", "metadata": {}, "source": ["x = torch.tensor(3.0, requires_grad=True)\ny = x ** 2\n\n# Detach y from the graph\ny_detached = y.detach()\n\nprint(f\"y requires_grad: {y.requires_grad}\")\nprint(f\"y_detached requires_grad: {y_detached.requires_grad}\")\nprint(f\"Same values: {y.item()} == {y_detached.item()}\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "summary", "metadata": {}, "source": ["---\n## Summary\n\n| Concept | Code | Purpose |\n|---------|------|--------|\n| Enable gradients | `requires_grad=True` | Track operations for backprop |\n| Compute gradients | `.backward()` | Run backpropagation |\n| Access gradients | `.grad` | Get computed derivatives |\n| Disable tracking | `torch.no_grad()` | Inference/evaluation mode |\n| Zero gradients | `.grad.zero_()` | Reset before each training step |\n| Detach tensor | `.detach()` | Remove from computation graph |\n\n**Key insight:** Autograd transforms the tedious manual calculus of backpropagation into a single `.backward()` call!"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 5
}