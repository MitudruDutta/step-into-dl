{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9ea830",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Cross Entropy Loss for Multi-Class Classification\n",
    "\n",
    "This notebook demonstrates how **Cross Entropy Loss** works for multi-class classification problems.\n",
    "\n",
    "## Key Concepts\n",
    "- Cross Entropy measures the difference between predicted probability distribution and true labels\n",
    "- Lower loss = better prediction\n",
    "- The loss is `-log(probability of true class)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a28826",
   "metadata": {},
   "source": [
    "## Example 1: Good Prediction\n",
    "\n",
    "We have 3 classes (0, 1, 2). The model outputs raw scores (logits):\n",
    "- Class 0: 0.1\n",
    "- Class 1: 0.2  \n",
    "- Class 2: 0.7 (highest)\n",
    "\n",
    "The true label is **class 2**.\n",
    "\n",
    "Since the model gives the highest score to the correct class, the loss should be relatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "y_pred = torch.tensor([[0.1, 0.2, 0.7]])  # Raw logits for 3 classes\n",
    "y_true = torch.tensor([2])  # True class is 2\n",
    "\n",
    "loss = loss_func(y_pred, y_true)\n",
    "print(f\"Cross Entropy Loss: {loss.item(): .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f4b96",
   "metadata": {},
   "source": [
    "## Example 2: Bad Prediction\n",
    "\n",
    "Same 3 classes, but now the model outputs:\n",
    "- Class 0: 0.7 (highest - WRONG!)\n",
    "- Class 1: 0.2\n",
    "- Class 2: 0.1 (lowest)\n",
    "\n",
    "The true label is still **class 2**.\n",
    "\n",
    "The model is most confident about the wrong class, so the loss will be much higher!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "y_pred = torch.tensor([[0.7, 0.2, 0.1]])  # Highest score for wrong class\n",
    "y_true = torch.tensor([2])  # True class is still 2\n",
    "\n",
    "loss = loss_func(y_pred, y_true)\n",
    "print(f\"Cross Entropy Loss: {loss.item(): .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a76861",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "| Scenario | Loss |\n",
    "|----------|------|\n",
    "| Good prediction (highest score for correct class) | ~0.77 |\n",
    "| Bad prediction (highest score for wrong class) | ~1.37 |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **CrossEntropyLoss** combines LogSoftmax + NLLLoss in one step\n",
    "2. **Don't apply softmax** to your model output - it's built into the loss\n",
    "3. **Targets are integer class indices**, not one-hot encoded\n",
    "4. Higher confidence in wrong class = higher loss\n",
    "5. Use `CrossEntropyLoss` for multi-class classification (one class per sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
