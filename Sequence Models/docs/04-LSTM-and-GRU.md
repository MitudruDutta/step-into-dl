# ğŸ§  LSTM & GRU Architectures

**Long Short-Term Memory (LSTM)** and **Gated Recurrent Units (GRU)** are advanced recurrent architectures designed to solve the vanishing gradient problem and capture long-term dependencies in sequential data.

---

## The Problem They Solve

Standard RNNs suffer from short-term memory â€” they struggle to maintain information across many time steps due to vanishing gradients.

```
RNN Memory Decay:

Step 1: "The" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Strong signal
Step 5: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Weakened
Step 20: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Almost gone
Step 50: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Forgotten

LSTM/GRU Solution: Create a "highway" for information to flow!
```

---

## LSTM Architecture

### The Key Innovation: Cell State

LSTM introduces a **cell state** â€” a separate memory track that runs through the entire sequence with minimal modifications:

```
           Cell State (Long-term memory highway)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
            Ã—           +            Ã—
            â†‘           â†‘            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              LSTM Cell                 â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚ Forget  â”‚ â”‚  Input  â”‚ â”‚ Output  â”‚   â”‚
    â”‚  â”‚  Gate   â”‚ â”‚  Gate   â”‚ â”‚  Gate   â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚           â”‚           â”‚
    â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€
            Hidden State (Short-term memory)
```

### The Three Gates

| Gate            | Symbol | Purpose                         | Output Range |
| :-------------- | :----: | :------------------------------ | :----------: |
| **Forget Gate** |   fâ‚œ   | What to discard from cell state |    [0, 1]    |
| **Input Gate**  |   iâ‚œ   | What new info to add            |    [0, 1]    |
| **Output Gate** |   oâ‚œ   | What to output from cell state  |    [0, 1]    |

### LSTM Equations

```
1. FORGET GATE: What to throw away
   fâ‚œ = Ïƒ(Wf Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bf)

2. INPUT GATE: What to update
   iâ‚œ = Ïƒ(Wi Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bi)

3. CANDIDATE VALUES: New potential values
   CÌƒâ‚œ = tanh(Wc Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bc)

4. UPDATE CELL STATE: Forget old + add new
   Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ

5. OUTPUT GATE: What to output
   oâ‚œ = Ïƒ(Wo Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bo)

6. HIDDEN STATE: Filtered cell state
   hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ)
```

### Step-by-Step Gate Operations

```
STEP 1: FORGET GATE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"Should I forget this information?"

Câ‚œâ‚‹â‚ = [0.8, -0.5, 0.3]  (previous cell state)
fâ‚œ   = [0.1, 0.9, 0.5]   (forget gate output)

fâ‚œ âŠ™ Câ‚œâ‚‹â‚ = [0.08, -0.45, 0.15]

â†’ First element mostly forgotten (Ã—0.1)
â†’ Second element kept (Ã—0.9)


STEP 2: INPUT GATE + CANDIDATE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"What new information should I add?"

iâ‚œ   = [0.9, 0.2, 0.7]   (input gate)
CÌƒâ‚œ   = [0.5, 0.3, -0.8]  (candidate values)

iâ‚œ âŠ™ CÌƒâ‚œ = [0.45, 0.06, -0.56]

â†’ Scales how much of each candidate to add


STEP 3: UPDATE CELL STATE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ
   = [0.08, -0.45, 0.15] + [0.45, 0.06, -0.56]
   = [0.53, -0.39, -0.41]


STEP 4: OUTPUT GATE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"What should I output?"

oâ‚œ = [0.8, 0.3, 0.6]
hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ)
   = [0.8, 0.3, 0.6] âŠ™ tanh([0.53, -0.39, -0.41])
   = [0.38, -0.11, -0.23]
```

---

## Why LSTM Solves Vanishing Gradients

### The Cell State Highway

```
In standard RNN:
  hâ‚œ = tanh(Wh Â· hâ‚œâ‚‹â‚ + Wx Â· xâ‚œ)

  Gradient: âˆ‚hâ‚œ/âˆ‚hâ‚œâ‚‹â‚ involves tanh' (can shrink)

In LSTM:
  Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ

  Gradient: âˆ‚Câ‚œ/âˆ‚Câ‚œâ‚‹â‚ = fâ‚œ (just multiplication by gate!)

If fâ‚œ â‰ˆ 1: Gradient flows through unchanged!
```

### Gradient Flow Comparison

```
Standard RNN:
  âˆ‚hâ‚â‚€â‚€/âˆ‚hâ‚ = âˆáµ¢ (tanh'(záµ¢) Â· Wâ‚•â‚•)  â†’  Vanishes!

LSTM:
  âˆ‚Câ‚â‚€â‚€/âˆ‚Câ‚ = âˆáµ¢ fáµ¢  â†’  If f â‰ˆ 1, gradient preserved!

The forget gate literally controls gradient flow:
  f = 1: "Remember everything, gradient flows"
  f = 0: "Forget completely"
```

---

## GRU Architecture

GRU is a simplified version of LSTM, combining the forget and input gates into a single **update gate**.

### GRU Structure

```
GRU has only TWO gates:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              GRU Cell               â”‚
    â”‚                                     â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚   â”‚   Reset   â”‚   â”‚  Update   â”‚    â”‚
    â”‚   â”‚   Gate    â”‚   â”‚   Gate    â”‚    â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚         â”‚               â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚               â”‚
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              Hidden State only
              (No separate cell state)
```

### GRU Equations

```
1. RESET GATE: How much past to use in candidate
   râ‚œ = Ïƒ(Wr Â· [hâ‚œâ‚‹â‚, xâ‚œ] + br)

2. UPDATE GATE: Balance between old and new
   zâ‚œ = Ïƒ(Wz Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bz)

3. CANDIDATE STATE: Potential new hidden state
   hÌƒâ‚œ = tanh(Wh Â· [râ‚œ âŠ™ hâ‚œâ‚‹â‚, xâ‚œ] + bh)

4. NEW HIDDEN STATE: Interpolate old and new
   hâ‚œ = (1 - zâ‚œ) âŠ™ hâ‚œâ‚‹â‚ + zâ‚œ âŠ™ hÌƒâ‚œ
```

### Understanding GRU Gates

```
UPDATE GATE (zâ‚œ):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Controls the balance between keeping old state and accepting new

z = 1: "Use new candidate completely"
z = 0: "Keep old hidden state completely"

hâ‚œ = (1 - z) Â· hâ‚œâ‚‹â‚ + z Â· hÌƒâ‚œ
     â†‘              â†‘
     Old           New


RESET GATE (râ‚œ):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Controls how much of the past to "forget" when creating candidate

r = 1: "Use all of previous hidden state"
r = 0: "Ignore previous hidden state completely"

hÌƒâ‚œ = tanh(W Â· [r âŠ™ hâ‚œâ‚‹â‚, xâ‚œ])
            â†‘
            Reset filters what past info to consider
```

---

## LSTM vs GRU Comparison

| Aspect             | LSTM                         | GRU                             |
| :----------------- | :--------------------------- | :------------------------------ |
| **Gates**          | 3 (forget, input, output)    | 2 (reset, update)               |
| **States**         | Cell state + Hidden state    | Hidden state only               |
| **Parameters**     | More                         | ~25% fewer                      |
| **Training Speed** | Slower                       | Faster                          |
| **Memory**         | Excellent long-term          | Good long-term                  |
| **Best For**       | Complex, very long sequences | Simpler sequences, speed needed |

### When to Choose

```
Choose LSTM when:
  âœ“ Very long sequences (100+ steps)
  âœ“ Complex dependencies
  âœ“ Maximum accuracy is priority
  âœ“ Memory/compute is not limited

Choose GRU when:
  âœ“ Moderate sequence lengths
  âœ“ Faster training needed
  âœ“ Limited computational resources
  âœ“ Fewer training examples
```

---

## PyTorch Implementation

### Basic LSTM

```python
import torch
import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes, num_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(
            input_size=embed_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.3,
            bidirectional=True
        )
        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional

    def forward(self, x):
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)
        lstm_out, (h_n, c_n) = self.lstm(embedded)
        # Concatenate final forward and backward hidden states
        hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)
        return self.fc(hidden)
```

### Basic GRU

```python
class GRUClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes, num_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.gru = nn.GRU(
            input_size=embed_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.3,
            bidirectional=True
        )
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
        embedded = self.embedding(x)
        gru_out, h_n = self.gru(embedded)
        hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)
        return self.fc(hidden)
```

### Sequence-to-Sequence (Many-to-Many)

```python
class Seq2SeqLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.decoder = nn.LSTM(output_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, src, tgt):
        # Encode source sequence
        _, (h, c) = self.encoder(src)

        # Decode target sequence
        dec_out, _ = self.decoder(tgt, (h, c))

        # Project to output space
        return self.fc(dec_out)
```

---

## Practical Tips

### 1. Layer Stacking

```python
# Stack multiple LSTM layers
self.lstm = nn.LSTM(
    input_size=128,
    hidden_size=256,
    num_layers=3,      # 3 stacked layers
    dropout=0.3        # Dropout between layers
)
```

### 2. Bidirectional Processing

```python
# Bidirectional LSTM
self.lstm = nn.LSTM(
    hidden_size=256,
    bidirectional=True  # Forward + backward
)
# Output size becomes hidden_size * 2
```

### 3. Gradient Clipping During Training

```python
optimizer.zero_grad()
loss.backward()
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
optimizer.step()
```

### 4. Attention Mechanism (Modern Enhancement)

```python
class AttentionLSTM(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.attention = nn.Linear(hidden_size, 1)

    def forward(self, lstm_outputs):
        # lstm_outputs: (batch, seq_len, hidden)
        attn_weights = torch.softmax(self.attention(lstm_outputs), dim=1)
        context = torch.sum(attn_weights * lstm_outputs, dim=1)
        return context
```

---

## Summary

| Concept            | LSTM                    | GRU                                |
| :----------------- | :---------------------- | :--------------------------------- |
| **Key Innovation** | Cell state highway      | Simplified gating                  |
| **Gates**          | Forget, Input, Output   | Reset, Update                      |
| **Gradient Flow**  | Via cell state          | Via update gate                    |
| **Parameters**     | More                    | Fewer                              |
| **Use Case**       | Complex, long sequences | Faster training, shorter sequences |

---

## What You've Learned

âœ… How LSTM's gates control information flow  
âœ… How the cell state solves vanishing gradients  
âœ… GRU's simplified two-gate architecture  
âœ… When to choose LSTM vs GRU  
âœ… PyTorch implementations for both

---

_LSTM and GRU revolutionized sequence modeling. While Transformers have become dominant for many NLP tasks, these gated RNNs remain excellent choices for time series, embedded systems, and scenarios requiring lower computational cost._
