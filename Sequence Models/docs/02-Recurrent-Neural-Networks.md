# ğŸ”„ Recurrent Neural Networks (RNN)

Recurrent Neural Networks are the foundation of sequential processing in deep learning. They are designed to maintain a **hidden state** that acts as memory, allowing information to persist and influence future predictions as the network processes a sequence step by step.

---

## The Core Idea

The fundamental innovation of RNNs is the **recurrent connection** â€” a loop that allows information to flow from one step to the next:

```
Traditional Feedforward:                 Recurrent Neural Network:

  x â†’ [Layer] â†’ y                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚                â†“
  No memory between inputs               â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚    â”‚      RNN      â”‚
                                         â”‚    â”‚     Cell      â”‚
                                         â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚            â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                                         Hidden state loops back!
```

---

## RNN Architecture

### Unrolled View

When we "unroll" an RNN across time, we can see how information flows through the network:

```
Unrolled RNN across 4 time steps:

        xâ‚          xâ‚‚          xâ‚ƒ          xâ‚„
         â†“           â†“           â†“           â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”
hâ‚€ â†’  â”‚ RNN  â”‚â†’hâ‚â”‚ RNN  â”‚â†’hâ‚‚â”‚ RNN  â”‚â†’hâ‚ƒâ”‚ RNN  â”‚â†’ hâ‚„
      â”‚ Cell â”‚   â”‚ Cell â”‚   â”‚ Cell â”‚   â”‚ Cell â”‚
      â””â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”€â”˜
         â†“           â†“           â†“           â†“
        yâ‚          yâ‚‚          yâ‚ƒ          yâ‚„

Key: Same RNN Cell (weights) is used at every step!
```

### Inside the RNN Cell

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RNN Cell                    â”‚
â”‚                                                â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚    â”‚   xâ‚œ    â”‚ Input at time t                 â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                 â”‚
â”‚         â”‚                                      â”‚
â”‚         â†“                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚  Wâ‚“â‚• Â· xâ‚œ + Wâ‚•â‚• Â· hâ‚œâ‚‹â‚ + b  â”‚              â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                   â”‚                             â”‚
â”‚                   â†“                             â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚             â”‚   tanh   â”‚ Activation             â”‚
â”‚             â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                  â”‚                              â”‚
â”‚                  â†“                              â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚             â”‚   hâ‚œ    â”‚ New hidden state        â”‚
â”‚             â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                         â”‚
â”‚                  â”‚                              â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚         â†“                 â†“                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    â”‚   yâ‚œ    â”‚      â”‚ To next  â”‚               â”‚
â”‚    â”‚ (output)â”‚      â”‚   step   â”‚               â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Mathematical Formulation

### Forward Pass Equations

The forward pass of an RNN consists of two main computations:

```
1. Hidden State Update:
   hâ‚œ = tanh(Wâ‚“â‚• Â· xâ‚œ + Wâ‚•â‚• Â· hâ‚œâ‚‹â‚ + bâ‚•)

2. Output Computation:
   yâ‚œ = Wâ‚•áµ§ Â· hâ‚œ + báµ§

Where:
   xâ‚œ   âˆˆ â„áµˆ      Input vector at time t (d = input dimension)
   hâ‚œ   âˆˆ â„Ê°      Hidden state at time t (h = hidden dimension)
   yâ‚œ   âˆˆ â„áµ’      Output at time t (o = output dimension)

   Wâ‚“â‚•  âˆˆ â„Ê°Ë£áµˆ    Input-to-hidden weights
   Wâ‚•â‚•  âˆˆ â„Ê°Ë£Ê°    Hidden-to-hidden weights (recurrent weights)
   Wâ‚•áµ§  âˆˆ â„áµ’Ë£Ê°    Hidden-to-output weights
   bâ‚•   âˆˆ â„Ê°      Hidden bias
   báµ§   âˆˆ â„áµ’      Output bias
```

### Step-by-Step Example

```
Let's trace through a sequence of length 3:

Initial state: hâ‚€ = [0, 0] (zeros)

Step 1 (t=1):
  Input: xâ‚ = "The" (embedded as vector)
  hâ‚ = tanh(Wâ‚“â‚• Â· xâ‚ + Wâ‚•â‚• Â· hâ‚€ + bâ‚•)
       = tanh(Wâ‚“â‚• Â· xâ‚ + Wâ‚•â‚• Â· [0,0] + bâ‚•)
       = tanh(Wâ‚“â‚• Â· xâ‚ + bâ‚•)  # hâ‚€ is zeros
  yâ‚ = Wâ‚•áµ§ Â· hâ‚ + báµ§

Step 2 (t=2):
  Input: xâ‚‚ = "cat"
  hâ‚‚ = tanh(Wâ‚“â‚• Â· xâ‚‚ + Wâ‚•â‚• Â· hâ‚ + bâ‚•)
       # Now hâ‚ contains information about "The"
  yâ‚‚ = Wâ‚•áµ§ Â· hâ‚‚ + báµ§

Step 3 (t=3):
  Input: xâ‚ƒ = "sat"
  hâ‚ƒ = tanh(Wâ‚“â‚• Â· xâ‚ƒ + Wâ‚•â‚• Â· hâ‚‚ + bâ‚•)
       # hâ‚‚ contains information about "The cat"
  yâ‚ƒ = Wâ‚•áµ§ Â· hâ‚ƒ + báµ§

Final hidden state hâ‚ƒ encodes the entire sequence!
```

---

## Backpropagation Through Time (BPTT)

Training RNNs requires a special form of backpropagation called **Backpropagation Through Time (BPTT)**:

### The Challenge

```
To compute gradients, we must trace back through ALL time steps:

Loss at t=4:  Lâ‚„ = loss(yâ‚„, targetâ‚„)

To update Wâ‚•â‚•, we need:
  âˆ‚Lâ‚„/âˆ‚Wâ‚•â‚• = âˆ‚Lâ‚„/âˆ‚yâ‚„ Â· âˆ‚yâ‚„/âˆ‚hâ‚„ Â· âˆ‚hâ‚„/âˆ‚Wâ‚•â‚•

But hâ‚„ depends on hâ‚ƒ, which depends on hâ‚‚, which depends on hâ‚...

  hâ‚„ â†’ hâ‚ƒ â†’ hâ‚‚ â†’ hâ‚ â†’ hâ‚€

We must sum gradients across ALL these dependencies!
```

### BPTT Algorithm

```
BPTT Process:

1. FORWARD PASS: Compute all hidden states and outputs
   hâ‚ â†’ hâ‚‚ â†’ hâ‚ƒ â†’ ... â†’ hâ‚œ
   yâ‚    yâ‚‚    yâ‚ƒ    ...   yâ‚œ

2. COMPUTE LOSS: Sum losses across all time steps
   L = Lâ‚ + Lâ‚‚ + Lâ‚ƒ + ... + Lâ‚œ

3. BACKWARD PASS: Propagate gradients back through time
   âˆ‚L/âˆ‚hâ‚œ â†’ âˆ‚L/âˆ‚hâ‚œâ‚‹â‚ â†’ ... â†’ âˆ‚L/âˆ‚hâ‚

4. ACCUMULATE GRADIENTS: Sum gradients for shared weights
   âˆ‚L/âˆ‚Wâ‚•â‚• = Î£â‚œ (âˆ‚Lâ‚œ/âˆ‚Wâ‚•â‚•)
```

### Gradient Flow Visualization

```
Forward:           Backward (BPTT):

xâ‚ â†’ hâ‚ â†’ yâ‚       Î´â‚ â† Î´â‚ â† âˆ‚Lâ‚
      â†“                 â†‘
xâ‚‚ â†’ hâ‚‚ â†’ yâ‚‚       Î´â‚‚ â† Î´â‚‚ â† âˆ‚Lâ‚‚
      â†“                 â†‘
xâ‚ƒ â†’ hâ‚ƒ â†’ yâ‚ƒ       Î´â‚ƒ â† Î´â‚ƒ â† âˆ‚Lâ‚ƒ
      â†“                 â†‘
xâ‚„ â†’ hâ‚„ â†’ yâ‚„       Î´â‚„ â† â”€â”€â”€â”€ âˆ‚Lâ‚„

Gradients flow backward and ACCUMULATE through each step
```

---

## RNN Architectural Variants

### One-to-Many

```
Used for: Image captioning, music generation

Architecture:
         x (single input, e.g., image features)
         â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”
      â”‚ RNN  â”‚â†’ hâ‚ â†’ yâ‚ ("A")
      â””â”€â”€â”€â”€â”€â”€â”˜    â†“
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚     RNN      â”‚â†’ hâ‚‚ â†’ yâ‚‚ ("cat")
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â†“
                          â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚         RNN          â”‚â†’ hâ‚ƒ â†’ yâ‚ƒ ("sitting")
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output: Sequence of words describing the image
```

### Many-to-One

```
Used for: Sentiment analysis, document classification

Architecture:
    xâ‚        xâ‚‚        xâ‚ƒ        xâ‚„
("This")  ("movie") ("is")   ("great")
     â†“         â†“         â†“         â†“
  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ RNN  â”‚â†’â”‚ RNN  â”‚â†’â”‚ RNN  â”‚â†’â”‚ RNN  â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜
                                 â†“
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Dense  â”‚
                            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                                 â†“
                               y = ğŸ˜Š Positive

Only the FINAL hidden state produces output
```

### Many-to-Many (Synchronized)

```
Used for: Part-of-speech tagging, named entity recognition

Architecture:
    xâ‚         xâ‚‚         xâ‚ƒ
  ("The")   ("cat")    ("sat")
     â†“          â†“          â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ RNN  â”‚â†’ â”‚ RNN  â”‚â†’ â”‚ RNN  â”‚
  â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”˜
     â†“          â†“          â†“
   yâ‚=DET    yâ‚‚=NOUN   yâ‚ƒ=VERB

Output at EVERY time step (same length as input)
```

### Many-to-Many (Encoder-Decoder)

```
Used for: Machine translation, text summarization

Architecture:
         ENCODER                    DECODER
    xâ‚      xâ‚‚      xâ‚ƒ         yâ‚      yâ‚‚      yâ‚ƒ
 ("Hello")("world")(<EOS>)   (<SOS>)("Bonjour")("monde")
     â†“       â†“       â†“           â†“       â†“       â†“
  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
  â”‚ RNN â”‚â†’â”‚ RNN â”‚â†’â”‚ RNN â”‚â†’â†’â†’â†’â”‚ RNN â”‚â†’â”‚ RNN â”‚â†’â”‚ RNN â”‚
  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜
                                â†“       â†“       â†“
                            "Bonjour" "monde" <EOS>

Encoder processes input â†’ Context vector â†’ Decoder generates output
```

---

## Bidirectional RNN

Sometimes, context from **both directions** is important:

```
Example: "The _____ barked loudly"
  - Forward context: "The" (could be many things)
  - Backward context: "barked" (must be a dog!)

Bidirectional RNN Architecture:

Forward:  xâ‚ â”€â”€â†’ hâ‚â†’ â”€â”€â†’ hâ‚‚â†’ â”€â”€â†’ hâ‚ƒâ†’

Backward: xâ‚ â†â”€â”€ hâ‚â† â†â”€â”€ hâ‚‚â† â†â”€â”€ hâ‚ƒâ†

Combined: [hâ‚â†’; hâ‚â†]  [hâ‚‚â†’; hâ‚‚â†]  [hâ‚ƒâ†’; hâ‚ƒâ†]
               â†“           â†“           â†“
              yâ‚          yâ‚‚          yâ‚ƒ

Each output has access to BOTH past and future context
```

---

## PyTorch Implementation

### Basic RNN

```python
import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # RNN layer
        self.rnn = nn.RNN(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True  # Input shape: (batch, seq_len, features)
        )

        # Output layer
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, h0=None):
        # x shape: (batch_size, seq_length, input_size)

        # Initialize hidden state if not provided
        if h0 is None:
            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)

        # Forward through RNN
        # out: (batch, seq_len, hidden_size) - all hidden states
        # hn: (num_layers, batch, hidden_size) - final hidden state
        out, hn = self.rnn(x, h0)

        # Take output from last time step for classification
        out = self.fc(out[:, -1, :])  # Many-to-one

        return out, hn

# Example usage
model = SimpleRNN(input_size=10, hidden_size=64, output_size=5)
x = torch.randn(32, 20, 10)  # batch=32, seq_len=20, features=10
output, hidden = model(x)
print(f"Output shape: {output.shape}")  # (32, 5)
```

### RNN for Text Classification

```python
class TextRNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):
        super().__init__()

        # Embedding layer: converts word indices to vectors
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # RNN layer
        self.rnn = nn.RNN(
            embedding_dim,
            hidden_size,
            batch_first=True,
            bidirectional=True  # Use both directions
        )

        # Output layer (hidden_size * 2 for bidirectional)
        self.fc = nn.Linear(hidden_size * 2, num_classes)

    def forward(self, x):
        # x shape: (batch, seq_len) - word indices

        # Convert indices to embeddings
        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)

        # Forward through RNN
        output, hidden = self.rnn(embedded)

        # For bidirectional: concatenate final hidden states
        # hidden shape: (2, batch, hidden_size)
        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)

        # Classification
        out = self.fc(hidden)
        return out

# Example
model = TextRNN(vocab_size=10000, embedding_dim=128, hidden_size=256, num_classes=2)
x = torch.randint(0, 10000, (32, 50))  # batch=32, seq_len=50
output = model(x)
print(f"Output shape: {output.shape}")  # (32, 2)
```

---

## Limitations of Basic RNNs

### 1. Vanishing Gradients

```
In long sequences, gradients become exponentially small:

âˆ‚hâ‚â‚€â‚€/âˆ‚hâ‚ = âˆ‚hâ‚â‚€â‚€/âˆ‚hâ‚‰â‚‰ Â· âˆ‚hâ‚‰â‚‰/âˆ‚hâ‚‰â‚ˆ Â· ... Â· âˆ‚hâ‚‚/âˆ‚hâ‚

If each term < 1:
  0.9 Ã— 0.9 Ã— ... Ã— 0.9 (100 times) â‰ˆ 0.000027

Gradient virtually disappears! Early layers barely update.
```

### 2. Short-Term Memory

```
RNNs struggle to remember information from many steps ago:

"The cat, which had been sitting on the windowsill watching
 birds fly by for the past hour, finally ___"

By the time we reach "finally ___", the RNN may have
"forgotten" that the subject is "cat" (many steps ago).
```

### 3. Difficulty with Long-Range Dependencies

```
Example: Language Modeling

"I grew up in France ... [100 words later] ... I speak fluent ___"

The model should predict "French" based on "France"
But that context is 100+ steps in the past
Basic RNNs cannot maintain this information
```

---

## Summary

| Concept          | Description                                                  |
| :--------------- | :----------------------------------------------------------- |
| **Hidden State** | Internal memory that carries information through time        |
| **Recurrence**   | Same weights applied at every time step                      |
| **BPTT**         | Backpropagation that traces gradients through all time steps |
| **Variants**     | One-to-Many, Many-to-One, Many-to-Many, Bidirectional        |
| **Limitations**  | Vanishing gradients, short-term memory                       |

---

## What's Next?

The limitations of basic RNNs led to a critical problem that needed solving:

â¡ï¸ **Next:** [The Vanishing Gradient Problem](03-Vanishing-Gradient-Problem.md)

---

_RNNs introduced the revolutionary concept of neural memory. While they have limitations, understanding them is essential before learning how LSTM and GRU architectures solve these problems._
