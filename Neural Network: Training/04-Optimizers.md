# ğŸš€ Advanced Optimizers: Beyond Vanilla Gradient Descent

While basic gradient descent works, modern deep learning uses enhanced optimizers that converge faster and handle complex loss landscapes better.

---

## Why Advanced Optimizers?

Vanilla SGD has limitations:
- Same learning rate for all parameters
- Can oscillate in ravines
- Slow convergence on flat surfaces
- Gets stuck at saddle points

Advanced optimizers address these issues with:
- **Momentum**: Accumulate velocity to smooth updates
- **Adaptive Learning Rates**: Different LR per parameter
- **Second-Order Information**: Use curvature estimates

---

## SGD with Momentum

Adds a "velocity" term that accumulates past gradients, helping to:
- Smooth out oscillations
- Accelerate through flat regions
- Escape shallow local minima

### Algorithm
```
velocity = momentum Ã— velocity + gradient
weights = weights - learning_rate Ã— velocity
```

### Intuition
Like a ball rolling downhillâ€”it builds up speed and can roll through small bumps.

### PyTorch
```python
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9  # Typical value
)
```

### When to Use
- CNNs (often outperforms Adam)
- When you want more control
- Large-scale training

---

## Adam (Adaptive Moment Estimation)

The most popular optimizer. Combines momentum with adaptive learning rates.

### Key Features
- Maintains running averages of gradients (momentum)
- Maintains running averages of squared gradients (adaptive LR)
- Bias correction for initial steps

### Algorithm
```
m = Î²â‚ Ã— m + (1 - Î²â‚) Ã— gradient           # First moment (momentum)
v = Î²â‚‚ Ã— v + (1 - Î²â‚‚) Ã— gradientÂ²          # Second moment (adaptive LR)
m_hat = m / (1 - Î²â‚áµ—)                       # Bias correction
v_hat = v / (1 - Î²â‚‚áµ—)                       # Bias correction
weights = weights - lr Ã— m_hat / (âˆšv_hat + Îµ)
```

### PyTorch
```python
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.001,        # Default, often works well
    betas=(0.9, 0.999),  # Momentum coefficients
    eps=1e-8         # Numerical stability
)
```

### When to Use
- Default choice for most tasks
- Transformers and NLP
- When you want "it just works"

---

## AdamW (Adam with Weight Decay)

Adam with proper weight decay (L2 regularization) decoupled from gradient updates.

### Why AdamW?
Original Adam applies weight decay incorrectly. AdamW fixes this, leading to better generalization.

### PyTorch
```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01  # L2 regularization
)
```

### When to Use
- Transformers (BERT, GPT, etc.)
- When using weight decay
- Modern best practice over Adam

---

## RMSprop

Adapts learning rate based on recent gradient magnitudes. Predecessor to Adam.

### Algorithm
```
v = decay Ã— v + (1 - decay) Ã— gradientÂ²
weights = weights - lr Ã— gradient / (âˆšv + Îµ)
```

### PyTorch
```python
optimizer = torch.optim.RMSprop(
    model.parameters(),
    lr=0.01,
    alpha=0.99  # Decay rate
)
```

### When to Use
- RNNs and LSTMs
- Non-stationary problems
- When Adam doesn't work well

---

## AdaGrad

Adapts learning rate based on historical gradient accumulation. Good for sparse data.

### Behavior
- Parameters with large gradients get smaller LR
- Parameters with small gradients get larger LR
- LR monotonically decreases (can become too small)

### PyTorch
```python
optimizer = torch.optim.Adagrad(
    model.parameters(),
    lr=0.01
)
```

### When to Use
- Sparse features (NLP, recommendations)
- When different features have very different frequencies

---

## Comparison Table

| Optimizer | Adaptive LR | Momentum | Memory | Best For |
|-----------|-------------|----------|--------|----------|
| **SGD** | âŒ | âŒ | Low | Baseline |
| **SGD+Momentum** | âŒ | âœ… | Low | CNNs, large-scale |
| **AdaGrad** | âœ… | âŒ | Medium | Sparse data |
| **RMSprop** | âœ… | âŒ | Medium | RNNs |
| **Adam** | âœ… | âœ… | High | General purpose |
| **AdamW** | âœ… | âœ… | High | Transformers |

---

## Optimizer Selection Guide

```
Start Here
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ What type of model?                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€ Transformer/NLP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º AdamW
    â”‚
    â”œâ”€â”€ CNN (image) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º SGD+Momentum or Adam
    â”‚
    â”œâ”€â”€ RNN/LSTM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Adam or RMSprop
    â”‚
    â”œâ”€â”€ Sparse data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º AdaGrad or Adam
    â”‚
    â””â”€â”€ Not sure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Adam (safe default)
```

---

## Hyperparameter Defaults

| Optimizer | Learning Rate | Other |
|-----------|---------------|-------|
| **SGD** | 0.01 - 0.1 | momentum=0.9 |
| **Adam** | 0.001 | betas=(0.9, 0.999) |
| **AdamW** | 0.001 | weight_decay=0.01 |
| **RMSprop** | 0.01 | alpha=0.99 |

---

## Common Patterns

### Learning Rate Warmup
Gradually increase LR at the start of training:
```python
# Linear warmup for first 1000 steps
if step < 1000:
    lr = base_lr * (step / 1000)
```

### Learning Rate Scheduling with Adam
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=100
)

for epoch in range(100):
    train_one_epoch()
    scheduler.step()
```

### Gradient Clipping
Prevent exploding gradients:
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
optimizer.step()
```

---

## Debugging Optimizer Issues

| Symptom | Possible Cause | Solution |
|---------|----------------|----------|
| Loss not decreasing | LR too low | Increase LR |
| Loss exploding | LR too high | Decrease LR |
| Loss oscillating | LR too high | Decrease LR, add momentum |
| Slow convergence | Wrong optimizer | Try Adam |
| Poor generalization | No weight decay | Use AdamW with decay |

---

*Adam/AdamW is the safe default for most tasks. SGD+Momentum often works better for CNNs with proper tuning. Always start simple and add complexity only when needed.*
