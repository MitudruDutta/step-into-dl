{
 "cells": [
  {"cell_type": "markdown", "id": "intro", "metadata": {}, "source": ["# üéØ Gradient Descent from Scratch in PyTorch\n\nThis notebook implements gradient descent to find optimal weights for predicting employee bonuses. We'll see how the algorithm iteratively adjusts weights to minimize prediction error.\n\n**Problem:** Find weights w‚ÇÅ, w‚ÇÇ, w‚ÇÉ and bias b such that:\n```\nbonus = w‚ÇÅ√óperformance + w‚ÇÇ√óexperience + w‚ÇÉ√óprojects + b\n```\n\n**Ground Truth:** w‚ÇÅ=12, w‚ÇÇ=6, w‚ÇÉ=2, bias=20"]},
  {"cell_type": "markdown", "id": "setup-md", "metadata": {}, "source": ["---\n## Setup & Data Loading"]},
  {"cell_type": "code", "id": "imports", "metadata": {}, "source": ["import pandas as pd\nimport torch"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "load-md", "metadata": {}, "source": ["### Load the Employee Dataset\n\nThe dataset was generated with known weights, so we can verify if gradient descent recovers them."]},
  {"cell_type": "code", "id": "load", "metadata": {}, "source": ["df = pd.read_csv('emp_bonus.csv')\nprint(f\"Dataset shape: {df.shape}\")\ndf.head()"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "tensors-md", "metadata": {}, "source": ["---\n## Convert Data to PyTorch Tensors\n\nPyTorch requires tensors for gradient computation. We use `float32` for numerical stability."]},
  {"cell_type": "code", "id": "tensors", "metadata": {}, "source": ["# Convert features and target to tensors\nperformance = torch.tensor(df['performance'].values, dtype=torch.float32)\nyears_of_experience = torch.tensor(df['years_of_experience'].values, dtype=torch.float32)\nprojects_completed = torch.tensor(df['projects_completed'].values, dtype=torch.float32)\nbonus = torch.tensor(df['bonus'].values, dtype=torch.float32)\n\nprint(f\"Number of samples: {len(bonus)}\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "init-md", "metadata": {}, "source": ["---\n## Initialize Weights Randomly\n\nWe start with random weights and let gradient descent find the optimal values.\n\n**Key:** `requires_grad=True` tells PyTorch to track operations for automatic differentiation."]},
  {"cell_type": "code", "id": "init", "metadata": {}, "source": ["# Initialize weights randomly (gradient descent will optimize these)\nw1 = torch.rand(1, requires_grad=True)   # Weight for performance\nw2 = torch.rand(1, requires_grad=True)   # Weight for experience\nw3 = torch.rand(1, requires_grad=True)   # Weight for projects\nbias = torch.rand(1, requires_grad=True) # Bias term\n\nprint(f\"Initial weights: w1={w1.item():.4f}, w2={w2.item():.4f}, w3={w3.item():.4f}, bias={bias.item():.4f}\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "hyperparams-md", "metadata": {}, "source": ["---\n## Set Hyperparameters\n\n| Hyperparameter | Value | Purpose |\n|----------------|-------|--------|\n| Learning Rate | 0.006 | Step size for weight updates |\n| Epochs | 5000 | Number of complete passes through data |"]},
  {"cell_type": "code", "id": "hyperparams", "metadata": {}, "source": ["epochs = 5000\nlearning_rate = 0.006"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "loop-md", "metadata": {}, "source": ["---\n## Training Loop: Gradient Descent\n\nEach iteration:\n1. **Forward pass:** Compute predictions using current weights\n2. **Compute loss:** Mean Squared Error between predictions and actual values\n3. **Backward pass:** Compute gradients via backpropagation\n4. **Update weights:** Adjust weights in the direction that reduces loss\n5. **Zero gradients:** Clear gradients for next iteration"]},
  {"cell_type": "code", "id": "training", "metadata": {}, "source": ["for epoch in range(epochs):\n    # Forward pass: compute predicted bonus\n    predicted_bonus = w1 * performance + w2 * years_of_experience + w3 * projects_completed + bias\n    \n    # Compute Mean Squared Error loss\n    loss = ((predicted_bonus - bonus) ** 2).mean()\n    \n    # Backward pass: compute gradients\n    loss.backward()\n    \n    # Update weights using gradient descent\n    with torch.no_grad():\n        w1 -= learning_rate * w1.grad\n        w2 -= learning_rate * w2.grad\n        w3 -= learning_rate * w3.grad\n        bias -= learning_rate * bias.grad\n    \n    # Zero gradients for next iteration (IMPORTANT!)\n    w1.grad.zero_()\n    w2.grad.zero_()\n    w3.grad.zero_()\n    bias.grad.zero_()\n    \n    # Print progress every 100 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}: Loss = {loss.item():.2f}\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "results-md", "metadata": {}, "source": ["---\n## Results: Learned Weights\n\nCompare learned weights to ground truth:\n\n| Parameter | True Value | Learned Value |\n|-----------|------------|---------------|\n| w‚ÇÅ (performance) | 12 | ? |\n| w‚ÇÇ (experience) | 6 | ? |\n| w‚ÇÉ (projects) | 2 | ? |\n| bias | 20 | ? |"]},
  {"cell_type": "code", "id": "results", "metadata": {}, "source": ["print(\"Learned weights:\")\nprint(f\"  w1 (performance): {w1.item():.2f} (true: 12)\")\nprint(f\"  w2 (experience):  {w2.item():.2f} (true: 6)\")\nprint(f\"  w3 (projects):    {w3.item():.2f} (true: 2)\")\nprint(f\"  bias:             {bias.item():.2f} (true: 20)\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "predict-md", "metadata": {}, "source": ["---\n## Make a Prediction\n\nUse the learned weights to predict bonus for a new employee:\n- Performance: 7\n- Years of experience: 4\n- Projects completed: 7\n\n**Expected:** 12√ó7 + 6√ó4 + 2√ó7 + 20 = 84 + 24 + 14 + 20 = **142**"]},
  {"cell_type": "code", "id": "predict", "metadata": {}, "source": ["# Predict bonus for new employee\nnew_performance = 7\nnew_experience = 4\nnew_projects = 7\n\npredicted = w1 * new_performance + w2 * new_experience + w3 * new_projects + bias\nprint(f\"Predicted bonus: ${predicted.item():.2f}\")\nprint(f\"Expected bonus:  $142.00\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "summary", "metadata": {}, "source": ["---\n## Summary\n\n**What we learned:**\n1. Gradient descent iteratively adjusts weights to minimize loss\n2. `requires_grad=True` enables automatic gradient computation\n3. Always zero gradients before each backward pass\n4. Learning rate controls convergence speed vs stability\n\n**Next:** See `gd_vs_mini_gd_vs_sgd.ipynb` to compare Batch GD, Mini-Batch GD, and SGD."]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 5
}