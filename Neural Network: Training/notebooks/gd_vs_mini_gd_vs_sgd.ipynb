{
 "cells": [
  {"cell_type": "markdown", "id": "intro", "metadata": {}, "source": ["# ⚡ Comparing Gradient Descent Variants\n\nThis notebook compares three gradient descent approaches on the same problem:\n\n| Variant | Data per Update | Updates per Epoch |\n|---------|-----------------|-------------------|\n| **Batch GD** | All 100 samples | 1 |\n| **Mini-Batch GD** | 16 samples | 7 (100/16) |\n| **SGD** | 1 sample | 100 |\n\n**Problem:** Find weights for `bonus = w₁×performance + w₂×experience + w₃×projects + bias`"]},
  {"cell_type": "markdown", "id": "setup-md", "metadata": {}, "source": ["---\n## Setup & Data Loading"]},
  {"cell_type": "code", "id": "imports", "metadata": {}, "source": ["import pandas as pd\nimport torch\nfrom matplotlib import pyplot as plt"], "outputs": [], "execution_count": null},
  {"cell_type": "code", "id": "load", "metadata": {}, "source": ["# Load the employee bonus dataset\ndf = pd.read_csv('emp_bonus.csv')\nprint(f\"Dataset shape: {df.shape}\")\ndf.head()"], "outputs": [], "execution_count": null},
  {"cell_type": "code", "id": "tensors", "metadata": {}, "source": ["# Convert to PyTorch tensors\nperformance = torch.tensor(df['performance'].values, dtype=torch.float32)\nyears_of_experience = torch.tensor(df['years_of_experience'].values, dtype=torch.float32)\nprojects_completed = torch.tensor(df['projects_completed'].values, dtype=torch.float32)\nbonus = torch.tensor(df['bonus'].values, dtype=torch.float32)"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "plot-fn-md", "metadata": {}, "source": ["### Helper Functions"]},
  {"cell_type": "code", "id": "plot-fn", "metadata": {}, "source": ["def plot_loss(x_range, loss_history, title):\n    \"\"\"Plot loss over iterations.\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(x_range, loss_history, color='blue', linewidth=2)\n    plt.title(title)\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Mean Squared Error\")\n    plt.grid(True)\n    plt.show()\n\ndef safe_plot_loss(loss_history, start, end, title):\n    \"\"\"Plot loss with bounds checking to avoid IndexError.\"\"\"\n    n = len(loss_history)\n    if n == 0:\n        print(f\"Warning: No loss history to plot for '{title}'\")\n        return\n    \n    # Clamp indices to valid range\n    safe_start = max(0, min(start, n - 1))\n    safe_end = min(end, n)\n    \n    if safe_start >= safe_end:\n        print(f\"Warning: Requested range [{start}:{end}] out of bounds (history length: {n})\")\n        print(f\"Plotting available range [0:{n}] instead.\")\n        safe_start, safe_end = 0, n\n    elif safe_start != start or safe_end != end:\n        print(f\"Warning: Adjusted range from [{start}:{end}] to [{safe_start}:{safe_end}]\")\n    \n    plot_loss(range(safe_start, safe_end), loss_history[safe_start:safe_end], title)"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "batch-md", "metadata": {}, "source": ["---\n## 1. Batch Gradient Descent\n\n**Characteristics:**\n- Uses **entire dataset** for each gradient computation\n- **1 update per epoch**\n- Smooth, stable convergence\n- High memory usage\n- Slow for large datasets"]},
  {"cell_type": "code", "id": "batch-init", "metadata": {}, "source": ["# Initialize weights randomly\nw1 = torch.rand(1, requires_grad=True)\nw2 = torch.rand(1, requires_grad=True)\nw3 = torch.rand(1, requires_grad=True)\nbias = torch.rand(1, requires_grad=True)\n\n# Hyperparameters\nlearning_rate = 0.006\nepochs = 5000\n\n# Track loss history\nbatch_loss_history = []"], "outputs": [], "execution_count": null},
  {"cell_type": "code", "id": "batch-train", "metadata": {}, "source": ["# Training loop - Batch Gradient Descent\nfor epoch in range(epochs):\n    # Forward pass: use ALL data\n    predicted_bonus = w1 * performance + w2 * years_of_experience + w3 * projects_completed + bias\n    \n    # Compute MSE loss over entire dataset\n    loss = ((predicted_bonus - bonus) ** 2).mean()\n    batch_loss_history.append(loss.item())\n    \n    # Backward pass\n    loss.backward()\n    \n    # Update weights\n    with torch.no_grad():\n        w1 -= learning_rate * w1.grad\n        w2 -= learning_rate * w2.grad\n        w3 -= learning_rate * w3.grad\n        bias -= learning_rate * bias.grad\n    \n    # Zero gradients\n    w1.grad.zero_()\n    w2.grad.zero_()\n    w3.grad.zero_()\n    bias.grad.zero_()\n    \n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\nprint(f\"\\nLearned: w1={w1.item():.2f}, w2={w2.item():.2f}, w3={w3.item():.2f}, bias={bias.item():.2f}\")\nprint(f\"True:    w1=12, w2=6, w3=2, bias=20\")"], "outputs": [], "execution_count": null},
  {"cell_type": "code", "id": "batch-plot", "metadata": {}, "source": ["# Plot loss curve with safe bounds checking\nsafe_plot_loss(batch_loss_history, 1000, 2000, \"Batch GD: Smooth Convergence\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "mini-md", "metadata": {}, "source": ["---\n## 2. Mini-Batch Gradient Descent\n\n**Characteristics:**\n- Uses **small batches** (16 samples) for each update\n- **Multiple updates per epoch** (100/16 ≈ 7)\n- **Shuffles data each epoch** to prevent learning order-dependent patterns\n- Balances speed and stability\n- **Industry standard** for deep learning"]},
  {"cell_type": "code", "id": "mini-init", "metadata": {}, "source": ["# Re-initialize weights\nw1 = torch.rand(1, requires_grad=True)\nw2 = torch.rand(1, requires_grad=True)\nw3 = torch.rand(1, requires_grad=True)\nbias = torch.rand(1, requires_grad=True)\n\n# Hyperparameters\nlearning_rate = 0.001  # Lower LR for more frequent updates\nepochs = 5000\nbatch_size = 16\nn_samples = len(performance)\n\nmini_loss_history = []"], "outputs": [], "execution_count": null},
  {"cell_type": "code", "id": "mini-train", "metadata": {}, "source": ["# Training loop - Mini-Batch Gradient Descent\nfor epoch in range(epochs):\n    # Shuffle data at the start of each epoch (important for generalization!)\n    indices = torch.randperm(n_samples)\n    perf_shuffled = performance[indices]\n    exp_shuffled = years_of_experience[indices]\n    proj_shuffled = projects_completed[indices]\n    bonus_shuffled = bonus[indices]\n    \n    # Process data in batches\n    for i in range(0, n_samples, batch_size):\n        # Select mini-batch from shuffled data\n        batch_perf = perf_shuffled[i:i + batch_size]\n        batch_exp = exp_shuffled[i:i + batch_size]\n        batch_proj = proj_shuffled[i:i + batch_size]\n        batch_bonus = bonus_shuffled[i:i + batch_size]\n        \n        # Forward pass on batch only\n        predicted = w1 * batch_perf + w2 * batch_exp + w3 * batch_proj + bias\n        \n        # Loss on batch\n        loss = ((predicted - batch_bonus) ** 2).mean()\n        mini_loss_history.append(loss.item())\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        with torch.no_grad():\n            w1 -= learning_rate * w1.grad\n            w2 -= learning_rate * w2.grad\n            w3 -= learning_rate * w3.grad\n            bias -= learning_rate * bias.grad\n        \n        # Zero gradients\n        w1.grad.zero_()\n        w2.grad.zero_()\n        w3.grad.zero_()\n        bias.grad.zero_()\n    \n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\nprint(f\"\\nLearned: w1={w1.item():.2f}, w2={w2.item():.2f}, w3={w3.item():.2f}, bias={bias.item():.2f}\")\nprint(f\"Total iterations: {len(mini_loss_history)}\")"], "outputs": [], "execution_count": null},
  {"cell_type": "code", "id": "mini-plot", "metadata": {}, "source": ["# Plot loss curve with safe bounds checking\nsafe_plot_loss(mini_loss_history, 10000, 10300, \"Mini-Batch GD: Moderate Noise\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "sgd-md", "metadata": {}, "source": ["---\n## 3. Stochastic Gradient Descent (SGD)\n\n**Characteristics:**\n- Uses **single sample** for each update\n- **100 updates per epoch** (one per sample)\n- **Shuffles data each epoch**\n- Very noisy but fast updates\n- Low memory usage\n- Noise can help escape local minima"]},
  {"cell_type": "code", "id": "sgd-init", "metadata": {}, "source": ["# Re-initialize weights\nw1 = torch.randn(1, requires_grad=True)\nw2 = torch.randn(1, requires_grad=True)\nw3 = torch.randn(1, requires_grad=True)\nbias = torch.randn(1, requires_grad=True)\n\n# Hyperparameters\nlearning_rate = 0.001\nepochs = 500  # Fewer epochs needed (more updates per epoch)\nn_samples = len(performance)\n\nsgd_loss_history = []"], "outputs": [], "execution_count": null},
  {"cell_type": "code", "id": "sgd-train", "metadata": {}, "source": ["# Training loop - Stochastic Gradient Descent\nfor epoch in range(epochs):\n    # Shuffle data at the start of each epoch\n    indices = torch.randperm(n_samples)\n    \n    for i in range(n_samples):\n        # Use single data point (from shuffled indices)\n        idx = indices[i]\n        single_perf = performance[idx]\n        single_exp = years_of_experience[idx]\n        single_proj = projects_completed[idx]\n        single_bonus = bonus[idx]\n        \n        # Forward pass on single sample\n        predicted = w1 * single_perf + w2 * single_exp + w3 * single_proj + bias\n        \n        # Loss on single sample (squared error, not mean)\n        loss = (predicted - single_bonus) ** 2\n        \n        # Record every 10th iteration to reduce noise in plot\n        if i % 10 == 0:\n            sgd_loss_history.append(loss.item())\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        with torch.no_grad():\n            w1 -= learning_rate * w1.grad\n            w2 -= learning_rate * w2.grad\n            w3 -= learning_rate * w3.grad\n            bias -= learning_rate * bias.grad\n        \n        # Zero gradients\n        w1.grad.zero_()\n        w2.grad.zero_()\n        w3.grad.zero_()\n        bias.grad.zero_()\n    \n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n\nprint(f\"\\nLearned: w1={w1.item():.2f}, w2={w2.item():.2f}, w3={w3.item():.2f}, bias={bias.item():.2f}\")\nprint(f\"Total recorded iterations: {len(sgd_loss_history)}\")"], "outputs": [], "execution_count": null},
  {"cell_type": "code", "id": "sgd-plot", "metadata": {}, "source": ["# Plot loss curve with safe bounds checking\nsafe_plot_loss(sgd_loss_history, 1000, 1300, \"SGD: High Variance (Noisy)\")"], "outputs": [], "execution_count": null},
  {"cell_type": "markdown", "id": "comparison-md", "metadata": {}, "source": ["---\n## Comparison Summary\n\n| Aspect | Batch GD | Mini-Batch GD | SGD |\n|--------|----------|---------------|-----|\n| **Data per update** | 100 (all) | 16 | 1 |\n| **Updates per epoch** | 1 | ~7 | 100 |\n| **Shuffling** | Not needed | Each epoch | Each epoch |\n| **Convergence** | Smooth | Moderate noise | Very noisy |\n| **Speed** | Slow | Balanced | Fast updates |\n| **Memory** | High | Moderate | Low |\n| **Best for** | Small data | Most tasks | Large data, online learning |\n\n### Key Observations:\n1. **Batch GD** has the smoothest loss curve but slowest convergence\n2. **Mini-Batch GD** balances speed and stability (recommended default)\n3. **SGD** converges fastest but with high variance in loss\n4. **Shuffling** is essential for Mini-Batch and SGD to prevent learning patterns from data order"]},
  {"cell_type": "markdown", "id": "takeaway", "metadata": {}, "source": ["---\n## Key Takeaways\n\n1. **All three methods converge** to the same optimal weights\n2. **Mini-Batch is the industry standard** for deep learning\n3. **Always shuffle data** at the start of each epoch (except for Batch GD)\n4. **Learning rate must be adjusted** based on batch size:\n   - Larger batches → can use higher learning rate\n   - Smaller batches → need lower learning rate\n5. **Noise in SGD can be beneficial** for escaping local minima"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 5
}