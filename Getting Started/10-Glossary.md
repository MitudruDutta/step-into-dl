# ðŸ“– Glossary

Quick reference for common deep learning terminology.

---

## Core Terms

| Term | Definition |
|------|------------|
| **Activation Function** | Non-linear function applied to neuron outputs (ReLU, Sigmoid, Tanh) |
| **Backpropagation** | Algorithm for computing gradients by propagating errors backward |
| **Batch** | Subset of training data processed together before weight update |
| **Bias** | Learnable parameter added to weighted sum, allows shifting activation |
| **Epoch** | One complete pass through the entire training dataset |
| **Gradient** | Derivative of the loss with respect to model parameters |
| **Inference** | Using a trained model to make predictions on new data |
| **Loss Function** | Measures how far predictions are from actual values |
| **Optimizer** | Algorithm that updates weights based on gradients |
| **Tensor** | Multi-dimensional array; the fundamental data structure in deep learning |
| **Weight** | Learnable parameter determining connection strength between neurons |


---

## Architecture Terms

| Term | Definition |
|------|------------|
| **CNN** | Convolutional Neural Network; optimized for spatial data like images |
| **RNN** | Recurrent Neural Network; designed for sequential data |
| **LSTM** | Long Short-Term Memory; RNN variant that handles long sequences |
| **Transformer** | Architecture using self-attention; state-of-the-art for NLP |
| **MLP** | Multilayer Perceptron; basic feedforward neural network |

---

## Training Terms

| Term | Definition |
|------|------------|
| **Learning Rate** | Step size for weight updates during training |
| **Overfitting** | Model memorizes training data, fails on new data |
| **Underfitting** | Model too simple to capture patterns |
| **Regularization** | Techniques to prevent overfitting (dropout, L1/L2) |
| **Early Stopping** | Stop training when validation loss stops improving |

---

## Hardware Terms

| Term | Definition |
|------|------------|
| **GPU** | Graphics Processing Unit; accelerates parallel computations |
| **TPU** | Tensor Processing Unit; Google's custom AI accelerator |
| **CUDA** | NVIDIA's parallel computing platform for GPUs |
| **VRAM** | Video RAM; GPU memory for storing tensors |

---

*Refer back to this glossary as you encounter new terms in your learning journey.*
