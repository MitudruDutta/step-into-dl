{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# üöÄ Optimizer Comparison: SGD vs Momentum vs Adam\n",
    "\n",
    "This notebook provides a hands-on comparison of different optimization algorithms using the **FashionMNIST** dataset. We'll train the same neural network architecture with three different optimizers and observe how they affect training speed and convergence.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How to set up a classification task with PyTorch\n",
    "2. The practical differences between SGD, SGD+Momentum, and Adam\n",
    "3. How optimizer choice affects training dynamics\n",
    "4. How to evaluate model performance with classification metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We start by importing the necessary libraries:\n",
    "- `torch`: Core PyTorch library\n",
    "- `nn`: Neural network modules\n",
    "- `optim`: Optimization algorithms (SGD, Adam, etc.)\n",
    "- `datasets`: Pre-built datasets like FashionMNIST\n",
    "- `DataLoader`: Utility for batching and shuffling data"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dataset-intro-markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the FashionMNIST Dataset\n",
    "\n",
    "**FashionMNIST** is a dataset of Zalando's article images, designed as a drop-in replacement for MNIST. It contains:\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "- 10 clothing categories\n",
    "- 28√ó28 grayscale images\n",
    "\n",
    "The `ToTensor()` transform converts PIL images to PyTorch tensors and normalizes pixel values to [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "id": "e0eda5fc4750ae40",
   "metadata": {},
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dataloader-markdown",
   "metadata": {},
   "source": [
    "## 3. Creating DataLoaders\n",
    "\n",
    "DataLoaders wrap our datasets and provide:\n",
    "- **Batching**: Groups samples into mini-batches (64 images per batch)\n",
    "- **Shuffling**: Randomizes data order each epoch (reduces overfitting)\n",
    "- **Parallel loading**: Efficiently loads data in the background\n",
    "\n",
    "A batch size of 64 is a common choice that balances:\n",
    "- Memory usage (smaller batches use less GPU memory)\n",
    "- Gradient stability (larger batches give more stable gradients)\n",
    "- Training speed (larger batches are more efficient on GPUs)"
   ]
  },
  {
   "cell_type": "code",
   "id": "d16cf313355e8640",
   "metadata": {},
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "explore-data-markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring the Data\n",
    "\n",
    "Let's examine the shape of our data:\n",
    "- **Images**: `[batch_size, channels, height, width]` = `[64, 1, 28, 28]`\n",
    "- **Labels**: `[batch_size]` = `[64]` (one label per image)\n",
    "\n",
    "The images have 1 channel (grayscale) and are 28√ó28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "id": "5651e0dea516dbde",
   "metadata": {},
   "source": [
    "for images, labels in train_loader:\n",
    "    print(f\"Image batch shape: {images.size()}\")\n",
    "    print(f\"Label batch shape: {labels.size()}\")\n",
    "    break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "visualize-markdown",
   "metadata": {},
   "source": [
    "### Visualizing a Sample Image\n",
    "\n",
    "Let's look at one of the training images. The `.squeeze()` removes the channel dimension (from `[1, 28, 28]` to `[28, 28]`) for visualization."
   ]
  },
  {
   "cell_type": "code",
   "id": "a14eafca658612ef",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(images[0].squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb15128495fcb954",
   "metadata": {},
   "source": [
    "labels[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "classes-markdown",
   "metadata": {},
   "source": [
    "### Class Labels\n",
    "\n",
    "FashionMNIST has 10 classes, each representing a type of clothing item. The labels are integers 0-9, which we map to human-readable names."
   ]
  },
  {
   "cell_type": "code",
   "id": "f7784b3d6521ed2e",
   "metadata": {},
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\"\n",
    "]\n",
    "classes[labels[9].item()]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "device-markdown",
   "metadata": {},
   "source": [
    "## 5. Device Configuration\n",
    "\n",
    "We check if a GPU (CUDA) is available. Training on GPU is significantly faster than CPU for neural networks. If no GPU is available, we fall back to CPU."
   ]
  },
  {
   "cell_type": "code",
   "id": "6ef0197a1b474f9",
   "metadata": {},
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "model-markdown",
   "metadata": {},
   "source": [
    "## 6. Defining the Neural Network\n",
    "\n",
    "We create a simple **feedforward neural network** (Multi-Layer Perceptron) for classification:\n",
    "\n",
    "```\n",
    "Input (28√ó28 = 784) ‚Üí Hidden (128) ‚Üí Hidden (64) ‚Üí Output (10)\n",
    "```\n",
    "\n",
    "**Architecture breakdown:**\n",
    "1. `Flatten()`: Converts 28√ó28 image to 784-element vector\n",
    "2. `Linear(784, 128)`: First hidden layer with 128 neurons\n",
    "3. `ReLU()`: Activation function (introduces non-linearity)\n",
    "4. `Linear(128, 64)`: Second hidden layer with 64 neurons\n",
    "5. `ReLU()`: Another activation\n",
    "6. `Linear(64, 10)`: Output layer (10 classes)\n",
    "\n",
    "**Note:** We don't apply Softmax at the end because `CrossEntropyLoss` expects raw logits."
   ]
  },
  {
   "cell_type": "code",
   "id": "198db94a131f5858",
   "metadata": {},
   "source": [
    "class ClothsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "sgd-intro-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training with Vanilla SGD\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** is the simplest optimizer:\n",
    "\n",
    "```\n",
    "weights = weights - learning_rate √ó gradient\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- Simple and well-understood\n",
    "- Can be slow to converge\n",
    "- May oscillate in narrow valleys\n",
    "- Often generalizes well (with proper tuning)\n",
    "\n",
    "We use `lr=0.01` as our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "id": "73650b1bd9c92c3f",
   "metadata": {},
   "source": [
    "model = ClothsClassifier().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "training-loop-markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "The training loop follows this pattern for each batch:\n",
    "\n",
    "1. **Forward pass**: Compute predictions (`model(images)`)\n",
    "2. **Compute loss**: Compare predictions to true labels\n",
    "3. **Zero gradients**: Clear old gradients (`optimizer.zero_grad()`)\n",
    "4. **Backward pass**: Compute gradients (`loss.backward()`)\n",
    "5. **Update weights**: Apply gradients (`optimizer.step()`)\n",
    "\n",
    "We print the loss every 100 batches to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "id": "b31421d464567783",
   "metadata": {},
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "sgd-results-markdown",
   "metadata": {},
   "source": [
    "### SGD Results Analysis\n",
    "\n",
    "Notice how the loss decreases over time, but the progress can be slow. Vanilla SGD often:\n",
    "- Starts with high loss (around 2.0+)\n",
    "- Gradually decreases but may plateau\n",
    "- Shows some oscillation in loss values\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "momentum-intro-markdown",
   "metadata": {},
   "source": [
    "## 8. Training with SGD + Momentum\n",
    "\n",
    "**Momentum** adds \"velocity\" to gradient descent, helping it:\n",
    "- Accelerate in consistent gradient directions\n",
    "- Dampen oscillations in narrow valleys\n",
    "- Escape small local minima\n",
    "\n",
    "```\n",
    "velocity = Œ≤ √ó velocity + gradient\n",
    "weights = weights - learning_rate √ó velocity\n",
    "```\n",
    "\n",
    "We use `momentum=0.9`, which means 90% of the previous velocity is retained. This is the most common setting.\n",
    "\n",
    "**Expected improvement:** Faster convergence and lower final loss compared to vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "id": "bf9727f5ae555b3",
   "metadata": {},
   "source": [
    "model = ClothsClassifier().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4ce8a622d987186",
   "metadata": {},
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "momentum-results-markdown",
   "metadata": {},
   "source": [
    "### Momentum Results Analysis\n",
    "\n",
    "Compare these results to vanilla SGD:\n",
    "- **Faster initial progress**: Loss drops more quickly in early iterations\n",
    "- **Lower loss values**: Reaches better minima in the same number of epochs\n",
    "- **Smoother convergence**: Less oscillation in loss values\n",
    "\n",
    "Momentum is especially helpful when the loss landscape has narrow valleys or noisy gradients.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adam-intro-markdown",
   "metadata": {},
   "source": [
    "## 9. Training with Adam\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)** combines the best of:\n",
    "- **Momentum**: Tracks gradient direction (first moment)\n",
    "- **RMSProp**: Adapts learning rate per-parameter (second moment)\n",
    "\n",
    "```\n",
    "m = Œ≤‚ÇÅ √ó m + (1 - Œ≤‚ÇÅ) √ó gradient        # Momentum\n",
    "v = Œ≤‚ÇÇ √ó v + (1 - Œ≤‚ÇÇ) √ó gradient¬≤       # RMSProp\n",
    "weights = weights - lr √ó mÃÇ / (‚àövÃÇ + Œµ)   # Adaptive update\n",
    "```\n",
    "\n",
    "**Key advantages:**\n",
    "- Works well out-of-the-box with default parameters\n",
    "- Adapts learning rate for each parameter individually\n",
    "- Handles sparse gradients effectively\n",
    "- Fast convergence on most problems\n",
    "\n",
    "**Note:** We use `lr=0.01` here, but Adam's default is `lr=0.001`. Higher learning rates can work but may cause instability."
   ]
  },
  {
   "cell_type": "code",
   "id": "1cb7934da8443005",
   "metadata": {},
   "source": [
    "model = ClothsClassifier().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4341b4d865788187",
   "metadata": {},
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "adam-results-markdown",
   "metadata": {},
   "source": [
    "### Adam Results Analysis\n",
    "\n",
    "Adam typically shows:\n",
    "- **Fastest initial convergence**: Loss drops very quickly in early iterations\n",
    "- **Adaptive behavior**: Automatically adjusts step sizes for different parameters\n",
    "- **Stable training**: Less sensitive to learning rate choice\n",
    "\n",
    "Adam is often the default choice for deep learning because it \"just works\" without much tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-intro-markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation\n",
    "\n",
    "Now let's evaluate our Adam-trained model on the test set. We:\n",
    "1. Set the model to evaluation mode (`model.eval()`)\n",
    "2. Disable gradient computation (`torch.no_grad()`) for efficiency\n",
    "3. Collect predictions and true labels for all test samples\n",
    "\n",
    "**Why `model.eval()`?**\n",
    "- Disables dropout (if any)\n",
    "- Uses running statistics for batch normalization (if any)\n",
    "- Ensures consistent inference behavior"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c145aaeba6405e9",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "predictions-markdown",
   "metadata": {},
   "source": [
    "### Checking Predictions\n",
    "\n",
    "Let's verify our predictions by looking at the first few samples. The labels and predictions should match for correctly classified images."
   ]
  },
  {
   "cell_type": "code",
   "id": "49ba2eb28560720f",
   "metadata": {},
   "source": [
    "all_labels[:5]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f1a7b27f1f4e7aa",
   "metadata": {},
   "source": [
    "all_predicted[:5]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "report-intro-markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "The classification report provides detailed metrics for each class:\n",
    "\n",
    "- **Precision**: Of all items predicted as class X, what fraction were actually X?\n",
    "- **Recall**: Of all actual class X items, what fraction did we correctly identify?\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balanced metric)\n",
    "- **Support**: Number of samples in each class\n",
    "\n",
    "**Accuracy** is the overall percentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "id": "e33fd18c8406e8ff",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(all_labels, all_predicted, target_names=classes)\n",
    "print(report)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary: Optimizer Comparison\n",
    "\n",
    "| Optimizer | Convergence Speed | Ease of Use | Notes |\n",
    "|-----------|-------------------|-------------|-------|\n",
    "| **SGD** | Slow | Requires tuning | Good generalization with proper LR |\n",
    "| **SGD + Momentum** | Medium | Moderate | Faster than SGD, handles valleys well |\n",
    "| **Adam** | Fast | Easy (works out-of-box) | Default choice for most tasks |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Vanilla SGD** is simple but can be slow and oscillate\n",
    "2. **Momentum** accelerates training by building velocity in consistent directions\n",
    "3. **Adam** combines momentum with adaptive learning rates for fast, stable training\n",
    "4. **Start with Adam** for quick experiments, try **SGD+Momentum** for potentially better generalization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different learning rates for each optimizer\n",
    "- Add learning rate scheduling (e.g., `StepLR`, `CosineAnnealingLR`)\n",
    "- Experiment with `AdamW` (Adam with proper weight decay)\n",
    "- Compare final test accuracy across optimizers with more epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
